{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Try chatgpt for your task before fine-tuning. If chatgpt is not enough, then fine-tune it. Do not forget that chat-gpt may be sufficient for most simple tasks.\n",
        "\n",
        "To fine-tune the GPT-3.5 model, you need to convert each observation in your data to the following message template (Jsonline Format):"
      ],
      "metadata": {
        "id": "wBRzu-Yk4Btb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"{'messages': [{'role': 'system', 'content': 'Detect sentiment in the text.'},\n",
        "#                {'role': 'user', 'content': 'It is a very nice pair of pants. I recommend it to everyone.'},\n",
        "#                {'role': 'assistant', 'content': 'possitive'}]}\"\n",
        "\n",
        "# system: the instructions you give to the model.\n",
        "# user: the feature/independent variable/text of each observation in your data (X)\n",
        "# assistant: the dependent variable/target/label of each observation in your data (y)."
      ],
      "metadata": {
        "id": "P_X9Ptrr5HoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b7oLIrkJnHm",
        "outputId": "e561c663-8492-45a0-d6cd-fc004b8919a2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.35.15-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.6/328.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.35.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5ZLFxvoohMT",
        "outputId": "822af691-6f93-4653-8db1-74b356551f3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOqXTmLNk47Q",
        "outputId": "693916a9-dd9a-46b7-8003-59ce331be052"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "yR7fCAUpkrJY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/GENAI-LLM/Fıne Tuning and Function Calling/clothing_reviews.csv\")\n",
        "df.head()\n",
        "\n",
        "# We will fine-tune a data set with 300 observations."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0D2xCVuUk2Ax",
        "outputId": "671d9de3-54af-4867-fa5c-93a3493b599e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text     label\n",
              "0  I love this shirt! great fabric, nice quality,...  positive\n",
              "1  I recently purchased this tunic and love it! i...  positive\n",
              "2  I love this top, it's design is very pretty an...  positive\n",
              "3  Love this sweater! it's really flattering, lon...  positive\n",
              "4  I wanted to like this so much. it's a great go...  negative"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4dd0b600-80ba-4a7e-901e-bf5471582e99\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I love this shirt! great fabric, nice quality,...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I recently purchased this tunic and love it! i...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I love this top, it's design is very pretty an...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Love this sweater! it's really flattering, lon...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I wanted to like this so much. it's a great go...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4dd0b600-80ba-4a7e-901e-bf5471582e99')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4dd0b600-80ba-4a7e-901e-bf5471582e99 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4dd0b600-80ba-4a7e-901e-bf5471582e99');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-060f0abf-a179-4bf6-a421-9a382efbec1d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-060f0abf-a179-4bf6-a421-9a382efbec1d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-060f0abf-a179-4bf6-a421-9a382efbec1d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"# We will fine-tune a data set with 300 observations\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"I recently purchased this tunic and love it! its not to big, hangs just right, arm holes are not to big and the material is very comfortable! looks great with white jeans, shorts, or just jeans. i will get a lot of wear all year round! fits true to size. oh, slimming too!\",\n          \"I wanted to like this so much. it's a great good girl dress, church dress. however, it makes no sense. it's a summer dress with massively thick lining that snags i'm petite and busty and it made me look and feel very wide.\",\n          \"I love this top, it's design is very pretty and like nothing that i have. i think this top runs true to size, i'm usually an xs/s in tops and i went with the s for this one and it fits great. i will say that the top layer of material will snag very easily, so while wearing it you have to be very careful!!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffdfINE6EcHV",
        "outputId": "7131d9a1-3d5c-4ede-9b31-0b3e5f5d7bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.fig = plt.figure(figsize = (7,5))\n",
        "ax = sns.countplot(x=\"label\",\n",
        "                   data=df)\n",
        "ax.bar_label(ax.containers[0]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "KxG1fd_UlXE3",
        "outputId": "ef726486-045e-4a66-e4ef-2aca3f95eec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVUoaIGhlkGY",
        "outputId": "8ad314d8-b6f7-4aee-971c-fae56fdec9db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text     0\n",
              "label    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[\"text\"]\n",
        "y = df[\"label\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=101)"
      ],
      "metadata": {
        "id": "_6dYx0KAu5Z4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train))\n",
        "print(len(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spR4NNO81iWr",
        "outputId": "1fe481eb-fdff-4c2e-9ecf-ac86961655e6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "225\n",
            "75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiJz3_qFoXLt",
        "outputId": "b2b11eaa-a917-4021-c3c5-ee2375d2c7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "283    The skirt is comfortable and as pictured. it a...\n",
              "184    Saw this online and ran over to the store to t...\n",
              "237    For me, this top ran true to size. i'm so glad...\n",
              "230    All the colors are beautiful, the white is a t...\n",
              "224    What a fun dress to wear. it feels great, and ...\n",
              "                             ...                        \n",
              "287    I am pleased with this skirt. the jean materia...\n",
              "43     Love this top! i receive so many compliments w...\n",
              "88     Received this dress yesterday, love it!\\r\\nmy ...\n",
              "32     These jeans are very comfortable. the distress...\n",
              "284    I usually wear a size xs or s in retailer tops...\n",
              "Name: text, Length: 225, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def convert_df_to_dict(X, y):\n",
        "  \"\"\"Converts X and y to dictionary format.\n",
        "\n",
        "  Args:\n",
        "    X: text\n",
        "    y: label\n",
        "\n",
        "  Returns:\n",
        "    A dataframe(as X and y) in dictionary format.\n",
        "  \"\"\"\n",
        "\n",
        "  dictionary = []\n",
        "  for i, j in zip(X, y):\n",
        "    dictionary.append({\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Detect sentiment in the text.\" # system content'i tüm gözlemler için aynı olacaktır.\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": str(i)\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": str(j)\n",
        "            }\n",
        "        ]\n",
        "    })\n",
        "\n",
        "  return dictionary\n",
        "\n",
        "# Gözlemleri JSONLine formatına dönüştürmek için öncelikle onları dict formatına dönüştürmemiz gerekiyor."
      ],
      "metadata": {
        "id": "RY6NHIN0lzEf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = convert_df_to_dict(X_train, y_train)\n",
        "test = convert_df_to_dict(X_test, y_test)\n",
        "\n",
        "# Train ve test verilerini ayrı ayrı dictionary formatına dönüştürüyoruz."
      ],
      "metadata": {
        "id": "A3kUGrxlraUQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV6TOTzsA9rM",
        "outputId": "36f97c8c-7bbd-4bd8-8d00-528897742b52"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'messages': [{'role': 'system', 'content': 'Detect sentiment in the text.'},\n",
              "   {'role': 'user',\n",
              "    'content': 'The skirt is comfortable and as pictured. it also ran tts. i\\'m 5\\'5\" tall and weigh 140 lbs. i had the medium and didn\\'t need to size up to a large. when you put the skirt on a normal body, it looks like you are wearing an easter egg... and not in a good way.'},\n",
              "   {'role': 'assistant', 'content': 'negative'}]},\n",
              " {'messages': [{'role': 'system', 'content': 'Detect sentiment in the text.'},\n",
              "   {'role': 'user',\n",
              "    'content': \"Saw this online and ran over to the store to try it on. so lovely! i thought i would want it in the cream color but the blue was so lovely in person. it's like a muted blue that makes it so versatile. the detail in the back is gorgeous and so flattering. the safety pin is such a unique touch. it's nice and warm, just not super soft, but not scratchy. i do think i will be extra careful with it because it seems like it could get caught on things easily.\"},\n",
              "   {'role': 'assistant', 'content': 'positive'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(train[0]) # the type of observations is dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCEHj56gOTTE",
        "outputId": "63accf67-646c-464d-8221-e7fe3a263e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"train.jsonl\", \"w\") as f:\n",
        "    for item in train:\n",
        "        f.write(json.dumps(item) + \"\\n\") # Python'da bir dictionary'i JSON formatına dönüştürmek için json.dumps() fonksiyonu kullanılır.\n",
        "                                         # Bu fonksiyon dictionary'i bir JSON string'e dönüştürür.\n",
        "\n",
        "# İnce ayar için modele besleyeceğimiz train ve test datası JSON line formatında olmalıdır. Bunun için dictionary formatındaki tüm gözlemleri\n",
        "# JSON line formatına dönüştürüyoruz.\n",
        "\n",
        "# JSON formatı dictionary formatının string'e dönüştürülmesidir.\n",
        "\n",
        "# dict format: {'name': 'johnson', 'age':44}\n",
        "# json format: \"{'name': 'johnson', 'age':'44'}\"\n",
        "\n",
        "\n",
        "# Python uses the json.dumps() function to convert a dictionary to JSON format.\n",
        " # This function converts the dictionary into a JSON string.\n",
        "\n",
        "# For fine tuning, the train and test data that we will feed to the model should be in JSON line format. For this, all observations in dictionary format\n",
        "# We convert to JSON line format.\n",
        "\n",
        "# JSON format is the conversion of dictionary format to string.\n",
        "\n",
        "# dict format: {'name': 'johnson', 'age':44}\n",
        "# json format: \"{'name': 'johnson', 'age':'44'}\""
      ],
      "metadata": {
        "id": "i7M9n1ozKjDN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"test.jsonl\", \"w\") as f:\n",
        "    for item in test:\n",
        "        f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "# Train datasına yaptığımız JSON line dönüşümünün aynısını test datasına da uyguluyoruz."
      ],
      "metadata": {
        "id": "8NRNwOh9KvhR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import codecs\n",
        "\n",
        "with codecs.open('train.jsonl', 'r', encoding='utf-8') as f:\n",
        "  a = 0\n",
        "  for line in f:\n",
        "    print(type(line))\n",
        "    print(line)\n",
        "    a+=1\n",
        "\n",
        "    if a==2:\n",
        "      break\n",
        "\n",
        "# JSON line formatına dönüştürülen tüm gözlemlerin string type'da olduğunu görebiliriz."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCL6YmEcR2Bi",
        "outputId": "00d379ff-1bea-4c72-9ccf-d39ec3e80de8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "{\"messages\": [{\"role\": \"system\", \"content\": \"Detect sentiment in the text.\"}, {\"role\": \"user\", \"content\": \"The skirt is comfortable and as pictured. it also ran tts. i'm 5'5\\\" tall and weigh 140 lbs. i had the medium and didn't need to size up to a large. when you put the skirt on a normal body, it looks like you are wearing an easter egg... and not in a good way.\"}, {\"role\": \"assistant\", \"content\": \"negative\"}]}\n",
            "\n",
            "<class 'str'>\n",
            "{\"messages\": [{\"role\": \"system\", \"content\": \"Detect sentiment in the text.\"}, {\"role\": \"user\", \"content\": \"Saw this online and ran over to the store to try it on. so lovely! i thought i would want it in the cream color but the blue was so lovely in person. it's like a muted blue that makes it so versatile. the detail in the back is gorgeous and so flattering. the safety pin is such a unique touch. it's nice and warm, just not super soft, but not scratchy. i do think i will be extra careful with it because it seems like it could get caught on things easily.\"}, {\"role\": \"assistant\", \"content\": \"positive\"}]}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation and analysis for chat model fine-tuning\n",
        "\n",
        "\n",
        "**Data loading**, **Format validation**, **Token Counting Utilities**, **Data Warnings and Token Counts**, ve **Cost Estimation** bölümleri için aşağıdaki kod blokları OpenAI tarafından hazırlanmıştır. Ve ince ayardan önce kullanılması tavsiye edilir.\n",
        "\n",
        "Bu kod blokları, bir chatgpt modeline ince ayar yapmak için kullanılan datayı önceden işlemek ve analiz etmek için bir araç görevi görür. Biçim hatalarını kontrol eder, temel istatistikler sağlar ve ince ayar maliyetini belirlemek için gerekli olan token sayılarını tahmin eder."
      ],
      "metadata": {
        "id": "qH3I4tzySovp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code blocks for the Data loading, Format validation, Token Counting Utilities, Data Warnings and Token Counts, and Cost Estimation sections are provided by OpenAI. And it is recommended to use them before fine tuning.\n",
        "\n",
        "These code blocks serve as a tool to pre-process and analyze the data used to fine-tune a chatgpt model. It checks for format errors, provides basic statistics and estimates the number of tokens needed to determine the cost of fine-tuning."
      ],
      "metadata": {
        "id": "-KkAd363CZuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tiktoken # for token counting\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "2UAjU36rTJpL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading\n",
        "\n",
        "We first load the chat dataset from an example JSONL file."
      ],
      "metadata": {
        "id": "2Ws8lfn7TPzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/train.jsonl\" # data_path değişkeni jsonl dosyasının yolunu belirtir.\n",
        "\n",
        "# Load the dataset\n",
        "with open(data_path, 'r', encoding='utf-8') as f: # with open() dosyayı açar ve json.loads() işlevini\n",
        "                                                  # kullanarak her satırı bir dict nesnesi olarak yükler.\n",
        "\n",
        "    dataset = [json.loads(line) for line in f] # tüm dict nesnelerini dataset listesine atıyoruz.\n",
        "\n",
        "# Initial dataset stats\n",
        "print(\"Num examples:\", len(dataset))\n",
        "print(\"First example:\")\n",
        "for message in dataset[0][\"messages\"]:\n",
        "    print(type(message))\n",
        "    print(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DA0XyMBTLe5",
        "outputId": "9a8b3250-1bc2-42ad-84e1-a1a33fbd40cb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num examples: 225\n",
            "First example:\n",
            "<class 'dict'>\n",
            "{'role': 'system', 'content': 'Detect sentiment in the text.'}\n",
            "<class 'dict'>\n",
            "{'role': 'user', 'content': 'The skirt is comfortable and as pictured. it also ran tts. i\\'m 5\\'5\" tall and weigh 140 lbs. i had the medium and didn\\'t need to size up to a large. when you put the skirt on a normal body, it looks like you are wearing an easter egg... and not in a good way.'}\n",
            "<class 'dict'>\n",
            "{'role': 'assistant', 'content': 'negative'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format validation\n",
        "\n",
        "Veri kümesindeki her bir gözlemin ince ayar API'sinin beklediği formata uygun olduğunu doğrulamak için çeşitli hata kontrolleri gerçekleştirebiliriz. Hatalar, daha kolay hata ayıklama için niteliklerine göre kategorize edilir.\n",
        "\n",
        "1.**Data Type Check(Veri Tipi Kontrolü)**:Veri setindeki her girdinin(input) bir dict olup olmadığını kontrol eder. Error type: data_type.\n",
        "\n",
        "2.**Presence of Message List(Mesaj Listesinin Varlığı)**: Her girdi için mesaj listesinin olup olmadığını kontrol eder. Error type: missing_messages_list.\n",
        "\n",
        "3.**Message Keys Check(Mesaj Anahtarları Kontrolü)**: Mesajların içerisindeki \"role\" ve \"content\" anahtarlarının varlığını kontrol eder. Error type: message_missing_key.\n",
        "\n",
        "4.**Unrecognized Keys in Messages(Mesajlardaki Tanınmayan Anahtarlar)**: Bir mesajın \"role\", \"content\" ve role_name(system, user, assistant) dışında anahtarları olup olmadığını kontrol eder. Error type: message_unrecognized_key.\n",
        "\n",
        "5.**Role Validation(role doğrulaması)**: \"Role\"lerin \"system\", \"user\", veya \"assistant\" olduğunu teyit eder. Error type: unrecognized_role.\n",
        "\n",
        "6.**Content Validation(content doğrulaması)**: contentin string bir ifade olduğunu teyit eder. Error type: missing_content.\n",
        "\n",
        "7.**Assistant Message Presence(Asistan Mesajı Varlığı:)**: mesajın içinde assistant'a ait bir içerik olup olmadığını kontrol eder. Error type: example_missing_assistant_message.\n",
        "\n",
        "Aşağıdaki kod, bu kontrolleri gerçekleştirir ve bulunan her türlü hatanın sayısını yazdırır. Bu, hata ayıklama ve veri kümesinin bir sonraki adımlara hazır olup olmadığını kontrol etme açısından faydalıdır."
      ],
      "metadata": {
        "id": "5Qfu9GeYTjxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can perform various error checks to verify that each observation in the dataset conforms to the format expected by the fine-tuning API. Errors are categorized according to their nature for easier debugging.\n",
        "\n",
        "1.Data Type Check: Checks whether each input in the dataset is a dict. Error type: data_type.\n",
        "\n",
        "2.Presence of Message List: Checks if there is a message list for each input. Error type: missing_messages_list.\n",
        "\n",
        "3.Message Keys Check: Checks for the presence of \"role\" and \"content\" keys in the messages. Error type: message_missing_key.\n",
        "\n",
        "4.Unrecognized Keys in Messages: Checks if a message has keys other than \"role\", \"content\" and role_name(system, user, assistant). Error type: message_unrecognized_key.\n",
        "\n",
        "5.Role Validation: Confirms that the \"roles\" are \"system\", \"user\", or \"assistant\". Error type: unrecognized_role.\n",
        "\n",
        "6.Content Validation: verifies that the content is a string expression. Error type: missing_content.\n",
        "\n",
        "7.Assistant Message Presence: checks if the message contains content belonging to the assistant. Error type: example_missing_assistant_message.\n",
        "\n",
        "\n",
        "The code below performs these checks and prints the number of any errors found. This is useful for debugging and checking that the dataset is ready for the next steps."
      ],
      "metadata": {
        "id": "09y8cwJLC88z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        if not content or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")\n",
        "\n",
        "# bu fonksiyon, verilerin ince ayar için hazır olup olmadığını kontrol eder. Eksik veya geçersiz veriler için hata döndürür.\n",
        "# datada problem yoksa \"Hata bulunamadı\" mesajını döndürür."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaCwPIfrTLmR",
        "outputId": "bc8dedd2-d80f-4dd6-ed2c-870be76d8270"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No errors found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token Counting Utilities\n",
        "\n",
        "Lets define a few helpful utilities to be used in the rest of the notebook."
      ],
      "metadata": {
        "id": "esD69V_LTrwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# not exact!\n",
        "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.05)}, {np.quantile(values, 0.95)}\")"
      ],
      "metadata": {
        "id": "_wxxwwAgTLtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Warnings and Token Counts\n",
        "\n",
        "Aşağıdaki analizlerle, verilerdeki potansiyel sorunları, eksik mesajları belirleyebilir ve mesaj ve token sayıları hakkında istatistiksel bilgiler sağlayabiliriz.\n",
        "\n",
        "1.**Missing System/User Messages**: \"system\" veya \"user\" mesajı eksik olan gözlemleri sayar. Bu mesajlar, assistant'ın davranışını tanımlamak ve konuşmayı başlatmak için kritik öneme sahiptir.\n",
        "\n",
        "2.**Number of Messages Per Example**: Her bir gözlemdeki mesaj (system, user, assistant) sayısının dağılımını özetler ve diyalog karmaşıklığı hakkında bilgi sağlar. Her mesajın 3 ayrı bölümü olup olmadığını kontrol eder: system, user, assistant. Öyleyse, her mesajdaki parça sayısının min, max, mean, median, %5 ve %95 dilimlerdeki değeri her zaman 3 olacaktır. Aksi takdirde, bu değerler 3'ten farklı olacaktır.\n",
        "\n",
        "3.**Total Tokens Per Example**: Her gözlemdeki toplam token sayısının dağılımını hesaplar ve özetler. İnce ayar maliyetlerini anlamak için önemlidir. Bu, system, user ve assistant parçaları da dahil olmak üzere her mesajdaki toplam token sayısı ve dağılımları (mean, median, minimum, maximum, etc.) hakkında bilgi sağlar.\n",
        "\n",
        "4.**Tokens in Assistant's Messages**: her gözlemdeki assistant mesajlarındaki toekn sayısını hesaplar ve bu dağılımı özetler (mean, median, minimum, maximum, etc.). Assistant hakkında bilgi sağlar.\n",
        "\n",
        "5.**Token Limit Warnings**:  Herhangi bir gözlem maksimum toekn sınırını (4096 token) aşarsa kontrol eder, çünkü bu tür gözlemler ince ayar sırasında kırpılacak ve bu da veri kaybına neden olacaktır."
      ],
      "metadata": {
        "id": "NTgrQJlcT6CP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the following analyses, we can identify potential problems in the data, missing messages and provide statistical information about the number of messages and tokens.\n",
        "\n",
        "1.Missing System/User Messages: Counts observations with missing \"system\" or \"user\" messages. These messages are critical for identifying the assistant's behavior and initiating the conversation.\n",
        "\n",
        "2.Number of Messages Per Example: Summarizes the distribution of the number of messages (system, user, assistant) in each observation and provides information about the dialog complexity. It checks if each message has 3 separate parts: system, user, assistant. If so, the min, max, mean, median, 5% and 95% of the number of parts in each message will always be 3. Otherwise, these values will be different from 3.\n",
        "\n",
        "3.Total Tokens Per Example: Calculates and summarizes the distribution of the total number of tokens in each observation. It is important to understand the fine-tuning costs. This provides information about the total number of tokens and their distribution (mean, median, minimum, maximum, etc.) in each message, including the system, user and assistant parts.\n",
        "\n",
        "4.Tokens in Assistant's Messages: calculates the number of tokens in assistant's messages in each observation and summarizes this distribution (mean, median, minimum, maximum, etc.). Provides information about the assistant.\n",
        "\n",
        "5.Token Limit Warnings: Checks if any observation exceeds the maximum token limit (4096 tokens), because such observations will be clipped during fine-tuning, resulting in data loss."
      ],
      "metadata": {
        "id": "R4FPjubmFImi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDmwM6puTLzu",
        "outputId": "efc54809-3541-481e-ead6-25900644fa02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min / max: 3, 3\n",
            "mean / median: 3.0, 3.0\n",
            "p5 / p95: 3.0, 3.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min / max: 34, 166\n",
            "mean / median: 97.36, 94.0\n",
            "p5 / p95: 45.2, 145.79999999999998\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min / max: 1, 1\n",
            "mean / median: 1.0, 1.0\n",
            "p5 / p95: 1.0, 1.0\n",
            "\n",
            "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOT: Test verileri için Veri yükleme, Format doğrulama, Jeton Sayım Yardımcı Programları, Veri Uyarıları ve Jeton Sayımları bölümlerinin de yapılması gerekir. Not defterini kısa tutmak için bunu sadece eğitim verileri için yaptık.\n",
        "\n",
        "\n",
        "NOTE: For test data, the Data loading, Format validation, Token Counting Utilities, Data Alerts and Token Counts sections also need to be done. To keep the notebook short, we have done this only for training data.\n",
        "Ancak maliyet tahmini bölümü yalnızca eğitim verileri için yapılır."
      ],
      "metadata": {
        "id": "Rv-y846P_hZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost Estimation\n",
        "\n",
        "Bu son bölümde, ince ayar için kullanılacak toplam token sayısını tahmin ediyoruz, bu da maliyeti yaklaşık olarak tahmin etmemizi sağlıyor. Token sayısı arttıkça ince ayar işlemlerinin süresinin de artacağını belirtmekte fayda var."
      ],
      "metadata": {
        "id": "TrNGRzLzUMuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this last section, we estimate the total number of tokens that will be used for fine-tuning, which allows us to approximate the cost. It is worth noting that as the number of tokens increases, the duration of the fine-tuning process will also increase."
      ],
      "metadata": {
        "id": "C-IVRB-mFW9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "100//30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm2stEhQ-4_X",
        "outputId": "e372763c-7e09-459d-ce26-f772a7158208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "MIN_DEFAULT_EPOCHS = 1\n",
        "MAX_DEFAULT_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:  # train datasındaki gözlem sayısı ile TARGET_EPOCHS(3) çarpımı MIN_TARGET_EXAMPLES(100)'den küçükse,\n",
        "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples) # epoch olarak MAX_DEFAULT_EPOCHS(25) veya 100//(train datasındaki gözlem sayısı)'den hangisi küçükse\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES: # train datasındaki gözlem sayısı ile TARGET_EPOCHS(3) çarpımı MAX_TARGET_EXAMPLES(25000)'den büyükse,\n",
        "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples) # epoch olarak MIN_DEFAULT_EPOCHS(1) veya MIN_TARGET_EXAMPLES(25000)//(train datasındaki gözlem sayısı)'den hangisi büyükse\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens) # bir yorumun token sayısı max 4096'yı geçmeyecek şekilde tüm yorumların toplam token sayısı\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\") # eğitime tabii tutulan toplam token sayısı\n",
        "\n",
        "current_price=8 # bir milyon token başına fiyat, lütfen güncel fiyat için openai web sayfasını kontrol edin.\n",
        "print(f\"Estimate total costs ~{(n_epochs * n_billing_tokens_in_dataset)/1000000 * current_price}\")\n",
        "\n",
        "# İnce ayar için modele en az 10 gözlem verilmelidir. Aksi takdirde model hata döndürecektir. Ancak ince ayar için modele minimum 100, maksimum 25000\n",
        "# gözlem verilmesi tavsiye edilir.\n",
        "\n",
        "# GPT 3.5-Turbo modeli, ince ayar için varsayılan olarak 3 epoch ile çalışır. Ancak bu kod, verilerinizdeki gözlem sayısına göre minimum 1'den maksimum 25'e\n",
        "# kadar kaç epoch eğitim yapmanız gerektiğini size önerir.\n",
        "\n",
        "# Tüm eğitim boyunca işlenecek token sayısını ve bu token sayısına göre ince ayar maliyetini döndürür.\n",
        "\n",
        "# NOT: Bu bölüm yalnızca eğitim verilerine uygulanacaktır.\n",
        "\n",
        "\n",
        "# At least 10 observations must be given to the model for fine tuning. Otherwise the model will return an error. However, for fine tuning, the model should be given a minimum of 100 and a maximum of 25000\n",
        "# observation is recommended.\n",
        "\n",
        "# GPT 3.5-Turbo model works with 3 epochs by default for fine tuning. However, this code can be adjusted from a minimum of 1 to a maximum of 25, depending on the number of observations in your data.\n",
        "# Suggests how many epochs of training you need to do.\n",
        "\n",
        "# Returns the number of tokens to process over the entire training and the cost of fine-tuning based on this number of tokens.\n",
        "\n",
        "# NOTE: This section will only apply to training data."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6iFu2fCUQLB",
        "outputId": "57084ddf-e985-4895-f524-6d5c68e32566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset has ~21906 tokens that will be charged for during training\n",
            "By default, you'll train for 3 epochs on this dataset\n",
            "By default, you'll be charged for ~65718 tokens\n",
            "Estimate total costs ~0.525744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=userdata.get('openai_key')"
      ],
      "metadata": {
        "id": "6Mo4ro3p83PC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "#api_key=os.environ['OPENAI_API_KEY']"
      ],
      "metadata": {
        "id": "yS3Hvp_gJw8O"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_response = client.files.create(file=open(\"/content/train.jsonl\", \"rb\"),\n",
        "                                        purpose=\"fine-tune\") # Train datasını ince ayar için hazırlıyoruz.\n",
        "\n",
        "training_file_id = training_response.id # Hazır olan train datası için oluşturulan ID’yi çekiyoruz.\n",
        "\n",
        "validation_response = client.files.create(file=open(\"/content/test.jsonl\", \"rb\"),\n",
        "                                          purpose=\"fine-tune\") # Test datasını ince ayar için hazırlıyoruz.\n",
        "\n",
        "validation_file_id = validation_response.id # Hazır olan test datası için oluşturulan ID’yi çekiyoruz.\n",
        "\n",
        "print(\"Training file id:\", training_file_id)\n",
        "print(\"Validation file id:\", validation_file_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qby3xDv2JYC8",
        "outputId": "b7ff8862-11f0-436a-ac14-3529a89f62a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training file id: file-Ff3jrpMZovt0ydTK0Evh1r05\n",
            "Validation file id: file-sbowLb2wia7VFyeS8JNTG2sL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "suffix_name = \"sentiment analys 4\"\n",
        "\n",
        "response = client.fine_tuning.jobs.create(\n",
        "    training_file=training_file_id, # Train datsının ID'si ince ayar modeline veriliyor. Train datasının ID'si mutlaka sağlanmalıdır\n",
        "    validation_file=validation_file_id, #Validation datsının ID'si ince ayar modeline veriliyor. Bu kimliği sağlamak isteğe bağlıdır\n",
        "    model=\"gpt-3.5-turbo\", # şimdilik mevcut tek model GPT-3.5-turbo'dur. GPT-4'ün yakın gelecekte kullanıma sunulmasını bekliyoruz.\n",
        "    suffix=suffix_name, # İnce ayar yapılan modelin ismine kendi seçeceğimiz bir ek ekleyebiliyoruz ancak modelin ismini tam olarak belirleyemiyoruz.\n",
        "    hyperparameters={\"n_epochs\":3}, # Yalnızca Epoch, batchsize ve learning_rate_multiplier parametresini ayarlamamıza izin veriliyor.\n",
        "                                    # Önerilen epoch sayısı 3 olduğundan epoch'u 3 olarak ayarladık.\n",
        ")\n",
        "\n",
        "job_id = response.id # The ID of fine-tune model.\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgw1dpVcVgPZ",
        "outputId": "2dbbb3bd-3194-40a2-f116-7f74eef945b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FineTuningJob(id='ftjob-8QePFrG4ictRCjm2F8kOP3K9', created_at=1720723853, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-2DK8uJ3r4XlwyMWXFl6PyYe2', result_files=[], seed=1718430282, status='validating_files', trained_tokens=None, training_file='file-Ff3jrpMZovt0ydTK0Evh1r05', validation_file='file-sbowLb2wia7VFyeS8JNTG2sL', estimated_finish=None, integrations=[], user_provided_suffix='sentiment analys 4')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job_id =\"ftjob-8QePFrG4ictRCjm2F8kOP3K9\""
      ],
      "metadata": {
        "id": "TjcV7BzfomDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_id = client.fine_tuning.jobs.list().data[1].id # status='validating_files', status=\"running\""
      ],
      "metadata": {
        "id": "zZ0gyFbraEiB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.fine_tuning.jobs.retrieve(job_id) # İnce ayar hakkında genel bilgiler alıyoruz. job_id=ftjob-8QePFrG4ictRCjm2F8kOP3K9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvYDKRtKFQGj",
        "outputId": "3b1c3e5a-a815-40e8-b0ad-e2dd1cd1a925"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FineTuningJob(id='ftjob-8QePFrG4ictRCjm2F8kOP3K9', created_at=1720723853, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWuAt5', finished_at=1720725243, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-2DK8uJ3r4XlwyMWXFl6PyYe2', result_files=['file-tNsZPLZTzUPxCsQmZOPfBM2j'], seed=1718430282, status='succeeded', trained_tokens=64368, training_file='file-Ff3jrpMZovt0ydTK0Evh1r05', validation_file='file-sbowLb2wia7VFyeS8JNTG2sL', estimated_finish=None, integrations=[], user_provided_suffix='sentiment analys 4')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id, limit=50) # Son 10 adımdaki ince ayar işleminin olaylarını görüntüler."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BprXKrBDIvMb",
        "outputId": "3b17d6e5-f33b-4edf-aead-ea134b977c06"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-tsPHriq8kaWhA2NRFyIML9lg', created_at=1720725258, level='info', message='The job has successfully completed', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-MX4kdT35aka7fKKfr9E1eEk0', created_at=1720725254, level='info', message='New fine-tuned model created: ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWuAt5', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-XhwAEHj8vl04lI8nx8TuMD5z', created_at=1720725245, level='info', message='New fine-tuned model created: ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWuAt5', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-fuDZuizk98wkTTp7nLc42W5H', created_at=1720725244, level='info', message='Checkpoint created at step 450 with Snapshot ID: ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWuWP7:ckpt-step-450', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-H4w6OMxUERfyAaTA3pYAOehb', created_at=1720725244, level='info', message='Checkpoint created at step 225 with Snapshot ID: ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWtPhm:ckpt-step-225', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-B5HCaj2fP70lQY72PMpCoiVW', created_at=1720725241, level='info', message='Step 675/675: training loss=0.00, full validation loss=0.32', object='fine_tuning.job.event', data={'step': 675, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'full_valid_loss': 0.3161024390326606, 'train_mean_token_accuracy': 1.0, 'full_valid_mean_token_accuracy': 0.9733333333333334}, type='metrics'), FineTuningJobEvent(id='ftevent-As12gbQZDBvdnggQr7T97s6E', created_at=1720725238, level='info', message='Step 674/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 674, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-cmJIcpOAlZ1ujtc1fgqucOxY', created_at=1720725236, level='info', message='Step 673/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 673, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-fGMG7buILZNdosmnpq3BOMfj', created_at=1720725234, level='info', message='Step 672/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 672, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-vZ7q8fgKSgasfmaRkiTkWgiS', created_at=1720725232, level='info', message='Step 671/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 671, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-d6khCHlzkhbxziqdtP6SrA3K', created_at=1720725232, level='info', message='Step 670/675: training loss=0.00, validation loss=0.00', object='fine_tuning.job.event', data={'step': 670, 'train_loss': 6.35782896551973e-07, 'valid_loss': 6.357828776041666e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0, 'valid_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-iJVSifNl2RPp0jrgq5lra6s9', created_at=1720725230, level='info', message='Step 669/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 669, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-28IuN48A0VLer3bBWzdyf3wX', created_at=1720725228, level='info', message='Step 668/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 668, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-WMqqSzEfAMmg2BL1rh9pm9CQ', created_at=1720725226, level='info', message='Step 667/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 667, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-vSqgE75BjUPbpG0MN9qxhvn6', created_at=1720725224, level='info', message='Step 666/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 666, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-EMYVKlUVzVHrLMxKLHR6YkBT', created_at=1720725222, level='info', message='Step 665/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 665, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-Wly7bPS3gAPjTHpGazd5TjsK', created_at=1720725220, level='info', message='Step 664/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 664, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-L6ADcLhCYQvQBZDZ0Vs5X6ng', created_at=1720725218, level='info', message='Step 663/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 663, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-vI50fDVO4dyv8Oip0xzgKmar', created_at=1720725218, level='info', message='Step 662/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 662, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-K9rTxc4lUPwcQF9lpNqTYLi9', created_at=1720725216, level='info', message='Step 661/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 661, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-ukLp6pnM0vBInZmvMMVBtMzX', created_at=1720725214, level='info', message='Step 660/675: training loss=0.00, validation loss=0.00', object='fine_tuning.job.event', data={'step': 660, 'train_loss': 6.35782896551973e-07, 'valid_loss': 6.357828776041666e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0, 'valid_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-8vGYnFw3N6O5ahnkQP1lb2gm', created_at=1720725212, level='info', message='Step 659/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 659, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-dR69dvtoyKDdcVokqXBZ9oVP', created_at=1720725210, level='info', message='Step 658/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 658, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-5BfgdDjoLSJjlMUSxQvwMByL', created_at=1720725208, level='info', message='Step 657/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 657, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-EPDgaruLeaUwgpBeu4lWrSAg', created_at=1720725206, level='info', message='Step 656/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 656, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-hX8a7AmecnVwaLCppT1mq1SC', created_at=1720725204, level='info', message='Step 655/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 655, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-wQAVitdcfZXlLwSmP5gN8gIz', created_at=1720725204, level='info', message='Step 654/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 654, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-YwwcKBl5mcvNBye9qSQulcAN', created_at=1720725202, level='info', message='Step 653/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 653, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-ETgXAvl2WyQuIcu1KxleZHZd', created_at=1720725200, level='info', message='Step 652/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 652, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-O04JVJ8p3S6dM1piSE5fhNiq', created_at=1720725198, level='info', message='Step 651/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 651, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-41XGOFoZ9mInUk7Qp73DMub2', created_at=1720725196, level='info', message='Step 650/675: training loss=0.00, validation loss=0.00', object='fine_tuning.job.event', data={'step': 650, 'train_loss': 6.35782896551973e-07, 'valid_loss': 6.357828776041666e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0, 'valid_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-XJsI4Qv7QRiqw1VTyUmzsFMF', created_at=1720725194, level='info', message='Step 649/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 649, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-kKdI9xlfZa8x8EJFMFRmE2y9', created_at=1720725192, level='info', message='Step 648/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 648, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-pyzj6sYwTmb7Y3svEugdTflp', created_at=1720725190, level='info', message='Step 647/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 647, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-blk8rl9rgcKiP6CyPifQsxJK', created_at=1720725188, level='info', message='Step 646/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 646, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-sDYZjCDUVyX9xBVdkVMQCLLT', created_at=1720725188, level='info', message='Step 645/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 645, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-jztIfJdci1xtet1Mt9k1k9Mn', created_at=1720725186, level='info', message='Step 644/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 644, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-kpazNPrFmNZqEC6TFU0U3SaT', created_at=1720725184, level='info', message='Step 643/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 643, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-i788VO4jVLmduDpPCWbOCmlx', created_at=1720725182, level='info', message='Step 642/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 642, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-ZSVTV2cpexPhPVYhBuYmSwX2', created_at=1720725180, level='info', message='Step 641/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 641, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-L7PnZ8M9PNp6OT1pJY3frNxm', created_at=1720725178, level='info', message='Step 640/675: training loss=0.00, validation loss=0.00', object='fine_tuning.job.event', data={'step': 640, 'train_loss': 6.35782896551973e-07, 'valid_loss': 6.357828776041666e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0, 'valid_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-dxCH7j6SpABqziTjoH8yoYWO', created_at=1720725178, level='info', message='Step 639/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 639, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-fTFRxwAL6uNxXiaNGuF8HJqM', created_at=1720725176, level='info', message='Step 638/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 638, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-fOyZiRyC1c10AwLQNWzhaELb', created_at=1720725174, level='info', message='Step 637/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 637, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-1UfGpNNgM1olUiEy6cmnDjSd', created_at=1720725172, level='info', message='Step 636/675: training loss=0.12', object='fine_tuning.job.event', data={'step': 636, 'train_loss': 0.12121836096048355, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-KvwRUFgNAYkv1HT2aOUWy3i4', created_at=1720725170, level='info', message='Step 635/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 635, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-Nt6ASY6AqNLUvOV01R3ZKhBh', created_at=1720725168, level='info', message='Step 634/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 634, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-AVWbOPlLH4mUf5C2uvxt0W6F', created_at=1720725166, level='info', message='Step 633/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 633, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-AUIvBHwysEk0jdfDtChk2IEt', created_at=1720725166, level='info', message='Step 632/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 632, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-P0wJ0M2TqP93suQteSSxue76', created_at=1720725164, level='info', message='Step 631/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 631, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics')], object='list', has_more=True)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response= client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id)\n",
        "\n",
        "events=response.data"
      ],
      "metadata": {
        "id": "5rynQ-cT5EOi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "events.reverse()# İnce ayar işleminin ilk adımları başlangıçta olacak şekilde, reverse() fonksiyonunu kullanarak\n",
        "                # ince ayar adımlarının sırasını tersine çeviriyoruz."
      ],
      "metadata": {
        "id": "Voa550aLVpxK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "events"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGxkxCmsNVz-",
        "outputId": "78af1a32-cd1d-4878-a257-403474758852"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[FineTuningJobEvent(id='ftevent-K9rTxc4lUPwcQF9lpNqTYLi9', created_at=1720725216, level='info', message='Step 661/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 661, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
              " FineTuningJobEvent(id='ftevent-vI50fDVO4dyv8Oip0xzgKmar', created_at=1720725218, level='info', message='Step 662/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 662, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
              " FineTuningJobEvent(id='ftevent-L6ADcLhCYQvQBZDZ0Vs5X6ng', created_at=1720725218, level='info', message='Step 663/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 663, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
              " FineTuningJobEvent(id='ftevent-Wly7bPS3gAPjTHpGazd5TjsK', created_at=1720725220, level='info', message='Step 664/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 664, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
              " FineTuningJobEvent(id='ftevent-EMYVKlUVzVHrLMxKLHR6YkBT', created_at=1720725222, level='info', message='Step 665/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 665, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
              " FineTuningJobEvent(id='ftevent-vSqgE75BjUPbpG0MN9qxhvn6', created_at=1720725224, level='info', message='Step 666/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 666, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
              " FineTuningJobEvent(id='ftevent-WMqqSzEfAMmg2BL1rh9pm9CQ', created_at=1720725226, level='info', message='Step 667/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 667, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
              " FineTuningJobEvent(id='ftevent-28IuN48A0VLer3bBWzdyf3wX', created_at=1720725228, level='info', message='Step 668/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 668, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
              " FineTuningJobEvent(id='ftevent-iJVSifNl2RPp0jrgq5lra6s9', created_at=1720725230, level='info', message='Step 669/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 669, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
              " FineTuningJobEvent(id='ftevent-d6khCHlzkhbxziqdtP6SrA3K', created_at=1720725232, level='info', message='Step 670/675: training loss=0.00, validation loss=0.00', object='fine_tuning.job.event', data={'step': 670, 'train_loss': 6.35782896551973e-07, 'valid_loss': 6.357828776041666e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0, 'valid_mean_token_accuracy': 1.0}, type='metrics'),\n",
              " FineTuningJobEvent(id='ftevent-vZ7q8fgKSgasfmaRkiTkWgiS', created_at=1720725232, level='info', message='Step 671/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 671, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
              " FineTuningJobEvent(id='ftevent-fGMG7buILZNdosmnpq3BOMfj', created_at=1720725234, level='info', message='Step 672/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 672, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
              " FineTuningJobEvent(id='ftevent-cmJIcpOAlZ1ujtc1fgqucOxY', created_at=1720725236, level='info', message='Step 673/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 673, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
              " FineTuningJobEvent(id='ftevent-As12gbQZDBvdnggQr7T97s6E', created_at=1720725238, level='info', message='Step 674/675: training loss=0.00', object='fine_tuning.job.event', data={'step': 674, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
              " FineTuningJobEvent(id='ftevent-B5HCaj2fP70lQY72PMpCoiVW', created_at=1720725241, level='info', message='Step 675/675: training loss=0.00, full validation loss=0.32', object='fine_tuning.job.event', data={'step': 675, 'train_loss': 6.35782896551973e-07, 'total_steps': 675, 'full_valid_loss': 0.3161024390326606, 'train_mean_token_accuracy': 1.0, 'full_valid_mean_token_accuracy': 0.9733333333333334}, type='metrics'),\n",
              " FineTuningJobEvent(id='ftevent-H4w6OMxUERfyAaTA3pYAOehb', created_at=1720725244, level='info', message='Checkpoint created at step 225 with Snapshot ID: ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWtPhm:ckpt-step-225', object='fine_tuning.job.event', data={}, type='message'),\n",
              " FineTuningJobEvent(id='ftevent-fuDZuizk98wkTTp7nLc42W5H', created_at=1720725244, level='info', message='Checkpoint created at step 450 with Snapshot ID: ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWuWP7:ckpt-step-450', object='fine_tuning.job.event', data={}, type='message'),\n",
              " FineTuningJobEvent(id='ftevent-XhwAEHj8vl04lI8nx8TuMD5z', created_at=1720725245, level='info', message='New fine-tuned model created: ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWuAt5', object='fine_tuning.job.event', data={}, type='message'),\n",
              " FineTuningJobEvent(id='ftevent-MX4kdT35aka7fKKfr9E1eEk0', created_at=1720725254, level='info', message='New fine-tuned model created: ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWuAt5', object='fine_tuning.job.event', data={}, type='message'),\n",
              " FineTuningJobEvent(id='ftevent-tsPHriq8kaWhA2NRFyIML9lg', created_at=1720725258, level='info', message='The job has successfully completed', object='fine_tuning.job.event', data={}, type='message')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for event in events:\n",
        "    print(event.message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qau4-ZN6Dw-2",
        "outputId": "6a40e425-ba72-4045-cb1a-18859d2643d2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 661/675: training loss=0.00\n",
            "Step 662/675: training loss=0.00\n",
            "Step 663/675: training loss=0.00\n",
            "Step 664/675: training loss=0.00\n",
            "Step 665/675: training loss=0.00\n",
            "Step 666/675: training loss=0.00\n",
            "Step 667/675: training loss=0.00\n",
            "Step 668/675: training loss=0.00\n",
            "Step 669/675: training loss=0.00\n",
            "Step 670/675: training loss=0.00, validation loss=0.00\n",
            "Step 671/675: training loss=0.00\n",
            "Step 672/675: training loss=0.00\n",
            "Step 673/675: training loss=0.00\n",
            "Step 674/675: training loss=0.00\n",
            "Step 675/675: training loss=0.00, full validation loss=0.32\n",
            "Checkpoint created at step 225 with Snapshot ID: ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWtPhm:ckpt-step-225\n",
            "Checkpoint created at step 450 with Snapshot ID: ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWuWP7:ckpt-step-450\n",
            "New fine-tuned model created: ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWuAt5\n",
            "New fine-tuned model created: ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWuAt5\n",
            "The job has successfully completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#client.fine_tuning.jobs.cancel(job_id)\n",
        "\n",
        "# You can cancel the fine-tuning process you have started with this code. However, the fine-tuning process will start in 1-2 minutes,\n",
        "# so you need to cancel it before that. If you don't, you won't be able to cancel it."
      ],
      "metadata": {
        "id": "8Zq0KkGlafoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response= client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id, limit=700)\n",
        "\n",
        "events=response.data\n",
        "events.reverse()\n",
        "for event in events:\n",
        "    print(event.message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mKd2JjXTpFZ",
        "outputId": "850b807f-a797-4665-c354-236caf09d1cd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created fine-tuning job: ftjob-8QePFrG4ictRCjm2F8kOP3K9\n",
            "Validating training file: file-Ff3jrpMZovt0ydTK0Evh1r05 and validation file: file-sbowLb2wia7VFyeS8JNTG2sL\n",
            "Files validated, moving job to queued state\n",
            "Fine-tuning job started\n",
            "Step 1/675: training loss=2.63\n",
            "Step 2/675: training loss=2.27\n",
            "Step 3/675: training loss=2.57\n",
            "Step 4/675: training loss=2.28\n",
            "Step 5/675: training loss=1.87\n",
            "Step 6/675: training loss=1.99\n",
            "Step 7/675: training loss=2.07\n",
            "Step 8/675: training loss=3.42\n",
            "Step 9/675: training loss=0.47\n",
            "Step 10/675: training loss=0.59, validation loss=0.44\n",
            "Step 11/675: training loss=1.41\n",
            "Step 12/675: training loss=0.11\n",
            "Step 13/675: training loss=0.16\n",
            "Step 14/675: training loss=0.95\n",
            "Step 15/675: training loss=0.01\n",
            "Step 16/675: training loss=0.00\n",
            "Step 17/675: training loss=0.01\n",
            "Step 18/675: training loss=0.00\n",
            "Step 19/675: training loss=0.00\n",
            "Step 20/675: training loss=0.00, validation loss=0.00\n",
            "Step 21/675: training loss=0.00\n",
            "Step 22/675: training loss=0.00\n",
            "Step 23/675: training loss=0.00\n",
            "Step 24/675: training loss=0.00\n",
            "Step 25/675: training loss=0.00\n",
            "Step 26/675: training loss=0.00\n",
            "Step 27/675: training loss=0.00\n",
            "Step 28/675: training loss=0.00\n",
            "Step 29/675: training loss=0.00\n",
            "Step 30/675: training loss=0.00, validation loss=0.00\n",
            "Step 31/675: training loss=0.00\n",
            "Step 32/675: training loss=0.00\n",
            "Step 33/675: training loss=0.00\n",
            "Step 34/675: training loss=0.00\n",
            "Step 35/675: training loss=0.00\n",
            "Step 36/675: training loss=0.00\n",
            "Step 37/675: training loss=0.00\n",
            "Step 38/675: training loss=0.00\n",
            "Step 39/675: training loss=0.00\n",
            "Step 40/675: training loss=0.00, validation loss=0.00\n",
            "Step 41/675: training loss=0.00\n",
            "Step 42/675: training loss=0.00\n",
            "Step 43/675: training loss=8.76\n",
            "Step 44/675: training loss=6.53\n",
            "Step 45/675: training loss=0.00\n",
            "Step 46/675: training loss=0.00\n",
            "Step 47/675: training loss=0.00\n",
            "Step 48/675: training loss=0.00\n",
            "Step 49/675: training loss=0.00\n",
            "Step 50/675: training loss=0.00, validation loss=0.00\n",
            "Step 51/675: training loss=0.00\n",
            "Step 52/675: training loss=2.81\n",
            "Step 53/675: training loss=3.57\n",
            "Step 54/675: training loss=0.00\n",
            "Step 55/675: training loss=0.32\n",
            "Step 56/675: training loss=0.00\n",
            "Step 57/675: training loss=0.04\n",
            "Step 58/675: training loss=0.00\n",
            "Step 59/675: training loss=0.00\n",
            "Step 60/675: training loss=0.00, validation loss=0.00\n",
            "Step 61/675: training loss=0.00\n",
            "Step 62/675: training loss=0.00\n",
            "Step 63/675: training loss=0.00\n",
            "Step 64/675: training loss=0.00\n",
            "Step 65/675: training loss=0.00\n",
            "Step 66/675: training loss=0.00\n",
            "Step 67/675: training loss=0.00\n",
            "Step 68/675: training loss=0.00\n",
            "Step 69/675: training loss=0.00\n",
            "Step 70/675: training loss=0.00, validation loss=0.00\n",
            "Step 71/675: training loss=0.00\n",
            "Step 72/675: training loss=0.00\n",
            "Step 73/675: training loss=0.00\n",
            "Step 74/675: training loss=0.00\n",
            "Step 75/675: training loss=0.00\n",
            "Step 76/675: training loss=0.00\n",
            "Step 77/675: training loss=0.00\n",
            "Step 78/675: training loss=0.00\n",
            "Step 79/675: training loss=0.00\n",
            "Step 80/675: training loss=0.00, validation loss=0.00\n",
            "Step 81/675: training loss=0.00\n",
            "Step 82/675: training loss=0.00\n",
            "Step 83/675: training loss=0.00\n",
            "Step 84/675: training loss=0.00\n",
            "Step 85/675: training loss=0.00\n",
            "Step 86/675: training loss=0.00\n",
            "Step 87/675: training loss=0.00\n",
            "Step 88/675: training loss=0.00\n",
            "Step 89/675: training loss=0.00\n",
            "Step 90/675: training loss=0.00, validation loss=0.00\n",
            "Step 91/675: training loss=2.85\n",
            "Step 92/675: training loss=0.00\n",
            "Step 93/675: training loss=0.00\n",
            "Step 94/675: training loss=0.62\n",
            "Step 95/675: training loss=0.00\n",
            "Step 96/675: training loss=0.00\n",
            "Step 97/675: training loss=0.00\n",
            "Step 98/675: training loss=0.00\n",
            "Step 99/675: training loss=0.00\n",
            "Step 100/675: training loss=0.00, validation loss=0.00\n",
            "Step 101/675: training loss=0.00\n",
            "Step 102/675: training loss=5.76\n",
            "Step 103/675: training loss=0.00\n",
            "Step 104/675: training loss=0.00\n",
            "Step 105/675: training loss=0.00\n",
            "Step 106/675: training loss=5.72\n",
            "Step 107/675: training loss=0.00\n",
            "Step 108/675: training loss=0.00\n",
            "Step 109/675: training loss=0.00\n",
            "Step 110/675: training loss=1.16, validation loss=0.00\n",
            "Step 111/675: training loss=0.00\n",
            "Step 112/675: training loss=3.90\n",
            "Step 113/675: training loss=0.00\n",
            "Step 114/675: training loss=0.00\n",
            "Step 115/675: training loss=0.00\n",
            "Step 116/675: training loss=0.00\n",
            "Step 117/675: training loss=0.00\n",
            "Step 118/675: training loss=0.00\n",
            "Step 119/675: training loss=0.00\n",
            "Step 120/675: training loss=0.00, validation loss=0.23\n",
            "Step 121/675: training loss=0.00\n",
            "Step 122/675: training loss=0.37\n",
            "Step 123/675: training loss=0.00\n",
            "Step 124/675: training loss=0.00\n",
            "Step 125/675: training loss=0.18\n",
            "Step 126/675: training loss=0.00\n",
            "Step 127/675: training loss=0.00\n",
            "Step 128/675: training loss=0.00\n",
            "Step 129/675: training loss=6.44\n",
            "Step 130/675: training loss=8.03, validation loss=0.00\n",
            "Step 131/675: training loss=0.00\n",
            "Step 132/675: training loss=0.00\n",
            "Step 133/675: training loss=0.00\n",
            "Step 134/675: training loss=0.00\n",
            "Step 135/675: training loss=0.00\n",
            "Step 136/675: training loss=0.00\n",
            "Step 137/675: training loss=0.00\n",
            "Step 138/675: training loss=0.00\n",
            "Step 139/675: training loss=1.79\n",
            "Step 140/675: training loss=0.00, validation loss=3.01\n",
            "Step 141/675: training loss=0.00\n",
            "Step 142/675: training loss=0.00\n",
            "Step 143/675: training loss=0.00\n",
            "Step 144/675: training loss=0.00\n",
            "Step 145/675: training loss=0.00\n",
            "Step 146/675: training loss=0.00\n",
            "Step 147/675: training loss=0.00\n",
            "Step 148/675: training loss=0.00\n",
            "Step 149/675: training loss=2.90\n",
            "Step 150/675: training loss=1.56, validation loss=0.00\n",
            "Step 151/675: training loss=0.00\n",
            "Step 152/675: training loss=0.00\n",
            "Step 153/675: training loss=0.00\n",
            "Step 154/675: training loss=0.00\n",
            "Step 155/675: training loss=0.00\n",
            "Step 156/675: training loss=0.00\n",
            "Step 157/675: training loss=0.00\n",
            "Step 158/675: training loss=0.00\n",
            "Step 159/675: training loss=0.02\n",
            "Step 160/675: training loss=0.00, validation loss=0.00\n",
            "Step 161/675: training loss=0.00\n",
            "Step 162/675: training loss=0.00\n",
            "Step 163/675: training loss=0.00\n",
            "Step 164/675: training loss=0.00\n",
            "Step 165/675: training loss=0.00\n",
            "Step 166/675: training loss=0.00\n",
            "Step 167/675: training loss=0.00\n",
            "Step 168/675: training loss=0.00\n",
            "Step 169/675: training loss=0.00\n",
            "Step 170/675: training loss=0.00, validation loss=0.00\n",
            "Step 171/675: training loss=0.00\n",
            "Step 172/675: training loss=0.00\n",
            "Step 173/675: training loss=0.00\n",
            "Step 174/675: training loss=0.00\n",
            "Step 175/675: training loss=0.00\n",
            "Step 176/675: training loss=0.00\n",
            "Step 177/675: training loss=0.00\n",
            "Step 178/675: training loss=0.00\n",
            "Step 179/675: training loss=0.00\n",
            "Step 180/675: training loss=5.19, validation loss=0.00\n",
            "Step 181/675: training loss=0.00\n",
            "Step 182/675: training loss=0.00\n",
            "Step 183/675: training loss=0.00\n",
            "Step 184/675: training loss=0.00\n",
            "Step 185/675: training loss=0.00\n",
            "Step 186/675: training loss=0.00\n",
            "Step 187/675: training loss=0.00\n",
            "Step 188/675: training loss=0.00\n",
            "Step 189/675: training loss=0.00\n",
            "Step 190/675: training loss=0.00, validation loss=0.00\n",
            "Step 191/675: training loss=0.00\n",
            "Step 192/675: training loss=0.00\n",
            "Step 193/675: training loss=0.00\n",
            "Step 194/675: training loss=0.00\n",
            "Step 195/675: training loss=0.00\n",
            "Step 196/675: training loss=0.00\n",
            "Step 197/675: training loss=0.00\n",
            "Step 198/675: training loss=0.00\n",
            "Step 199/675: training loss=0.00\n",
            "Step 200/675: training loss=0.00, validation loss=0.00\n",
            "Step 201/675: training loss=0.00\n",
            "Step 202/675: training loss=0.00\n",
            "Step 203/675: training loss=0.00\n",
            "Step 204/675: training loss=0.00\n",
            "Step 205/675: training loss=0.00\n",
            "Step 206/675: training loss=0.00\n",
            "Step 207/675: training loss=0.00\n",
            "Step 208/675: training loss=0.00\n",
            "Step 209/675: training loss=0.00\n",
            "Step 210/675: training loss=0.00, validation loss=0.00\n",
            "Step 211/675: training loss=0.00\n",
            "Step 212/675: training loss=0.65\n",
            "Step 213/675: training loss=0.00\n",
            "Step 214/675: training loss=0.00\n",
            "Step 215/675: training loss=0.00\n",
            "Step 216/675: training loss=0.00\n",
            "Step 217/675: training loss=0.00\n",
            "Step 218/675: training loss=0.00\n",
            "Step 219/675: training loss=0.00\n",
            "Step 220/675: training loss=0.00, validation loss=0.00\n",
            "Step 221/675: training loss=0.00\n",
            "Step 222/675: training loss=0.00\n",
            "Step 223/675: training loss=0.00\n",
            "Step 224/675: training loss=0.00\n",
            "Step 225/675: training loss=0.00, full validation loss=0.13\n",
            "Step 226/675: training loss=0.00\n",
            "Step 227/675: training loss=8.40\n",
            "Step 228/675: training loss=0.00\n",
            "Step 229/675: training loss=0.00\n",
            "Step 230/675: training loss=0.00, validation loss=0.00\n",
            "Step 231/675: training loss=0.00\n",
            "Step 232/675: training loss=0.00\n",
            "Step 233/675: training loss=0.00\n",
            "Step 234/675: training loss=0.00\n",
            "Step 235/675: training loss=0.00\n",
            "Step 236/675: training loss=0.00\n",
            "Step 237/675: training loss=0.52\n",
            "Step 238/675: training loss=0.00\n",
            "Step 239/675: training loss=0.00\n",
            "Step 240/675: training loss=0.00, validation loss=0.00\n",
            "Step 241/675: training loss=0.00\n",
            "Step 242/675: training loss=0.00\n",
            "Step 243/675: training loss=7.29\n",
            "Step 244/675: training loss=0.00\n",
            "Step 245/675: training loss=0.00\n",
            "Step 246/675: training loss=0.00\n",
            "Step 247/675: training loss=0.00\n",
            "Step 248/675: training loss=0.00\n",
            "Step 249/675: training loss=0.00\n",
            "Step 250/675: training loss=0.00, validation loss=0.00\n",
            "Step 251/675: training loss=0.00\n",
            "Step 252/675: training loss=0.00\n",
            "Step 253/675: training loss=0.00\n",
            "Step 254/675: training loss=0.00\n",
            "Step 255/675: training loss=0.00\n",
            "Step 256/675: training loss=0.00\n",
            "Step 257/675: training loss=0.00\n",
            "Step 258/675: training loss=0.00\n",
            "Step 259/675: training loss=4.96\n",
            "Step 260/675: training loss=0.00, validation loss=0.00\n",
            "Step 261/675: training loss=0.00\n",
            "Step 262/675: training loss=0.00\n",
            "Step 263/675: training loss=0.00\n",
            "Step 264/675: training loss=0.00\n",
            "Step 265/675: training loss=0.00\n",
            "Step 266/675: training loss=2.10\n",
            "Step 267/675: training loss=0.00\n",
            "Step 268/675: training loss=0.00\n",
            "Step 269/675: training loss=0.00\n",
            "Step 270/675: training loss=0.00, validation loss=0.00\n",
            "Step 271/675: training loss=0.00\n",
            "Step 272/675: training loss=0.00\n",
            "Step 273/675: training loss=0.00\n",
            "Step 274/675: training loss=0.00\n",
            "Step 275/675: training loss=0.00\n",
            "Step 276/675: training loss=0.00\n",
            "Step 277/675: training loss=0.00\n",
            "Step 278/675: training loss=0.00\n",
            "Step 279/675: training loss=0.00\n",
            "Step 280/675: training loss=0.00, validation loss=0.00\n",
            "Step 281/675: training loss=0.00\n",
            "Step 282/675: training loss=0.00\n",
            "Step 283/675: training loss=0.00\n",
            "Step 284/675: training loss=0.00\n",
            "Step 285/675: training loss=0.00\n",
            "Step 286/675: training loss=0.00\n",
            "Step 287/675: training loss=0.00\n",
            "Step 288/675: training loss=0.00\n",
            "Step 289/675: training loss=0.00\n",
            "Step 290/675: training loss=0.00, validation loss=0.00\n",
            "Step 291/675: training loss=0.00\n",
            "Step 292/675: training loss=0.00\n",
            "Step 293/675: training loss=0.00\n",
            "Step 294/675: training loss=0.00\n",
            "Step 295/675: training loss=0.00\n",
            "Step 296/675: training loss=0.00\n",
            "Step 297/675: training loss=0.00\n",
            "Step 298/675: training loss=0.00\n",
            "Step 299/675: training loss=0.00\n",
            "Step 300/675: training loss=0.00, validation loss=0.00\n",
            "Step 301/675: training loss=0.00\n",
            "Step 302/675: training loss=0.00\n",
            "Step 303/675: training loss=0.00\n",
            "Step 304/675: training loss=0.00\n",
            "Step 305/675: training loss=0.00\n",
            "Step 306/675: training loss=0.00\n",
            "Step 307/675: training loss=0.00\n",
            "Step 308/675: training loss=0.00\n",
            "Step 309/675: training loss=0.00\n",
            "Step 310/675: training loss=0.00, validation loss=0.00\n",
            "Step 311/675: training loss=0.00\n",
            "Step 312/675: training loss=0.00\n",
            "Step 313/675: training loss=0.00\n",
            "Step 314/675: training loss=0.00\n",
            "Step 315/675: training loss=0.00\n",
            "Step 316/675: training loss=0.00\n",
            "Step 317/675: training loss=4.56\n",
            "Step 318/675: training loss=0.00\n",
            "Step 319/675: training loss=0.00\n",
            "Step 320/675: training loss=10.34, validation loss=0.00\n",
            "Step 321/675: training loss=0.00\n",
            "Step 322/675: training loss=0.00\n",
            "Step 323/675: training loss=0.00\n",
            "Step 324/675: training loss=0.00\n",
            "Step 325/675: training loss=0.00\n",
            "Step 326/675: training loss=0.00\n",
            "Step 327/675: training loss=0.00\n",
            "Step 328/675: training loss=0.00\n",
            "Step 329/675: training loss=0.00\n",
            "Step 330/675: training loss=0.00, validation loss=0.00\n",
            "Step 331/675: training loss=0.00\n",
            "Step 332/675: training loss=0.00\n",
            "Step 333/675: training loss=0.00\n",
            "Step 334/675: training loss=0.00\n",
            "Step 335/675: training loss=0.00\n",
            "Step 336/675: training loss=0.00\n",
            "Step 337/675: training loss=0.00\n",
            "Step 338/675: training loss=0.00\n",
            "Step 339/675: training loss=0.00\n",
            "Step 340/675: training loss=0.00, validation loss=0.00\n",
            "Step 341/675: training loss=0.00\n",
            "Step 342/675: training loss=0.00\n",
            "Step 343/675: training loss=0.00\n",
            "Step 344/675: training loss=0.00\n",
            "Step 345/675: training loss=0.00\n",
            "Step 346/675: training loss=0.00\n",
            "Step 347/675: training loss=0.00\n",
            "Step 348/675: training loss=0.00\n",
            "Step 349/675: training loss=0.00\n",
            "Step 350/675: training loss=0.00, validation loss=0.00\n",
            "Step 351/675: training loss=0.00\n",
            "Step 352/675: training loss=0.00\n",
            "Step 353/675: training loss=0.00\n",
            "Step 354/675: training loss=0.00\n",
            "Step 355/675: training loss=0.00\n",
            "Step 356/675: training loss=0.00\n",
            "Step 357/675: training loss=0.00\n",
            "Step 358/675: training loss=0.00\n",
            "Step 359/675: training loss=0.00\n",
            "Step 360/675: training loss=0.00, validation loss=0.00\n",
            "Step 361/675: training loss=0.00\n",
            "Step 362/675: training loss=0.00\n",
            "Step 363/675: training loss=0.00\n",
            "Step 364/675: training loss=0.00\n",
            "Step 365/675: training loss=0.00\n",
            "Step 366/675: training loss=0.00\n",
            "Step 367/675: training loss=0.00\n",
            "Step 368/675: training loss=0.00\n",
            "Step 369/675: training loss=0.00\n",
            "Step 370/675: training loss=0.00, validation loss=0.00\n",
            "Step 371/675: training loss=5.29\n",
            "Step 372/675: training loss=0.00\n",
            "Step 373/675: training loss=0.00\n",
            "Step 374/675: training loss=0.00\n",
            "Step 375/675: training loss=0.00\n",
            "Step 376/675: training loss=0.00\n",
            "Step 377/675: training loss=0.00\n",
            "Step 378/675: training loss=0.00\n",
            "Step 379/675: training loss=0.00\n",
            "Step 380/675: training loss=0.00, validation loss=0.00\n",
            "Step 381/675: training loss=0.00\n",
            "Step 382/675: training loss=0.00\n",
            "Step 383/675: training loss=0.00\n",
            "Step 384/675: training loss=0.00\n",
            "Step 385/675: training loss=0.00\n",
            "Step 386/675: training loss=0.00\n",
            "Step 387/675: training loss=0.00\n",
            "Step 388/675: training loss=0.00\n",
            "Step 389/675: training loss=0.00\n",
            "Step 390/675: training loss=0.00, validation loss=0.00\n",
            "Step 391/675: training loss=0.00\n",
            "Step 392/675: training loss=0.00\n",
            "Step 393/675: training loss=0.00\n",
            "Step 394/675: training loss=0.00\n",
            "Step 395/675: training loss=0.00\n",
            "Step 396/675: training loss=0.00\n",
            "Step 397/675: training loss=0.00\n",
            "Step 398/675: training loss=0.00\n",
            "Step 399/675: training loss=0.00\n",
            "Step 400/675: training loss=0.00, validation loss=0.00\n",
            "Step 401/675: training loss=0.00\n",
            "Step 402/675: training loss=0.00\n",
            "Step 403/675: training loss=0.00\n",
            "Step 404/675: training loss=0.00\n",
            "Step 405/675: training loss=0.00\n",
            "Step 406/675: training loss=0.00\n",
            "Step 407/675: training loss=0.00\n",
            "Step 408/675: training loss=0.00\n",
            "Step 409/675: training loss=0.00\n",
            "Step 410/675: training loss=0.00, validation loss=0.00\n",
            "Step 411/675: training loss=0.00\n",
            "Step 412/675: training loss=0.00\n",
            "Step 413/675: training loss=0.00\n",
            "Step 414/675: training loss=0.00\n",
            "Step 415/675: training loss=0.00\n",
            "Step 416/675: training loss=0.00\n",
            "Step 417/675: training loss=0.00\n",
            "Step 418/675: training loss=0.00\n",
            "Step 419/675: training loss=0.00\n",
            "Step 420/675: training loss=0.00, validation loss=0.00\n",
            "Step 421/675: training loss=0.00\n",
            "Step 422/675: training loss=0.00\n",
            "Step 423/675: training loss=0.00\n",
            "Step 424/675: training loss=0.00\n",
            "Step 425/675: training loss=0.00\n",
            "Step 426/675: training loss=0.00\n",
            "Step 427/675: training loss=0.00\n",
            "Step 428/675: training loss=0.00\n",
            "Step 429/675: training loss=0.00\n",
            "Step 430/675: training loss=7.90, validation loss=0.00\n",
            "Step 431/675: training loss=0.00\n",
            "Step 432/675: training loss=0.00\n",
            "Step 433/675: training loss=0.00\n",
            "Step 434/675: training loss=0.00\n",
            "Step 435/675: training loss=0.00\n",
            "Step 436/675: training loss=0.00\n",
            "Step 437/675: training loss=0.00\n",
            "Step 438/675: training loss=0.00\n",
            "Step 439/675: training loss=0.00\n",
            "Step 440/675: training loss=0.00, validation loss=0.00\n",
            "Step 441/675: training loss=0.00\n",
            "Step 442/675: training loss=0.00\n",
            "Step 443/675: training loss=0.00\n",
            "Step 444/675: training loss=0.00\n",
            "Step 445/675: training loss=0.00\n",
            "Step 446/675: training loss=0.00\n",
            "Step 447/675: training loss=0.00\n",
            "Step 448/675: training loss=0.00\n",
            "Step 449/675: training loss=0.00\n",
            "Step 450/675: training loss=0.00, validation loss=0.00, full validation loss=0.34\n",
            "Step 451/675: training loss=0.00\n",
            "Step 452/675: training loss=0.00\n",
            "Step 453/675: training loss=0.00\n",
            "Step 454/675: training loss=0.00\n",
            "Step 455/675: training loss=0.00\n",
            "Step 456/675: training loss=0.00\n",
            "Step 457/675: training loss=0.00\n",
            "Step 458/675: training loss=0.00\n",
            "Step 459/675: training loss=0.00\n",
            "Step 460/675: training loss=0.00, validation loss=0.00\n",
            "Step 461/675: training loss=0.00\n",
            "Step 462/675: training loss=0.00\n",
            "Step 463/675: training loss=0.00\n",
            "Step 464/675: training loss=0.00\n",
            "Step 465/675: training loss=0.00\n",
            "Step 466/675: training loss=0.00\n",
            "Step 467/675: training loss=0.00\n",
            "Step 468/675: training loss=0.00\n",
            "Step 469/675: training loss=0.00\n",
            "Step 470/675: training loss=0.00, validation loss=0.00\n",
            "Step 471/675: training loss=0.00\n",
            "Step 472/675: training loss=0.00\n",
            "Step 473/675: training loss=0.00\n",
            "Step 474/675: training loss=0.00\n",
            "Step 475/675: training loss=0.00\n",
            "Step 476/675: training loss=0.00\n",
            "Step 477/675: training loss=0.00\n",
            "Step 478/675: training loss=0.00\n",
            "Step 479/675: training loss=0.00\n",
            "Step 480/675: training loss=0.00, validation loss=0.00\n",
            "Step 481/675: training loss=0.00\n",
            "Step 482/675: training loss=0.00\n",
            "Step 483/675: training loss=0.00\n",
            "Step 484/675: training loss=0.00\n",
            "Step 485/675: training loss=0.00\n",
            "Step 486/675: training loss=0.00\n",
            "Step 487/675: training loss=0.00\n",
            "Step 488/675: training loss=0.00\n",
            "Step 489/675: training loss=0.00\n",
            "Step 490/675: training loss=0.00, validation loss=0.00\n",
            "Step 491/675: training loss=0.00\n",
            "Step 492/675: training loss=0.00\n",
            "Step 493/675: training loss=0.00\n",
            "Step 494/675: training loss=0.00\n",
            "Step 495/675: training loss=0.00\n",
            "Step 496/675: training loss=0.00\n",
            "Step 497/675: training loss=0.00\n",
            "Step 498/675: training loss=0.00\n",
            "Step 499/675: training loss=0.00\n",
            "Step 500/675: training loss=0.00, validation loss=4.71\n",
            "Step 501/675: training loss=0.00\n",
            "Step 502/675: training loss=0.00\n",
            "Step 503/675: training loss=0.00\n",
            "Step 504/675: training loss=0.00\n",
            "Step 505/675: training loss=0.00\n",
            "Step 506/675: training loss=0.00\n",
            "Step 507/675: training loss=0.00\n",
            "Step 508/675: training loss=0.00\n",
            "Step 509/675: training loss=0.00\n",
            "Step 510/675: training loss=0.00, validation loss=0.00\n",
            "Step 511/675: training loss=0.00\n",
            "Step 512/675: training loss=0.00\n",
            "Step 513/675: training loss=0.00\n",
            "Step 514/675: training loss=0.00\n",
            "Step 515/675: training loss=0.00\n",
            "Step 516/675: training loss=0.00\n",
            "Step 517/675: training loss=0.00\n",
            "Step 518/675: training loss=0.00\n",
            "Step 519/675: training loss=0.00\n",
            "Step 520/675: training loss=0.00, validation loss=0.00\n",
            "Step 521/675: training loss=0.00\n",
            "Step 522/675: training loss=0.00\n",
            "Step 523/675: training loss=0.00\n",
            "Step 524/675: training loss=0.00\n",
            "Step 525/675: training loss=0.00\n",
            "Step 526/675: training loss=0.00\n",
            "Step 527/675: training loss=0.00\n",
            "Step 528/675: training loss=0.00\n",
            "Step 529/675: training loss=0.00\n",
            "Step 530/675: training loss=0.00, validation loss=0.00\n",
            "Step 531/675: training loss=0.00\n",
            "Step 532/675: training loss=0.00\n",
            "Step 533/675: training loss=0.00\n",
            "Step 534/675: training loss=0.00\n",
            "Step 535/675: training loss=0.00\n",
            "Step 536/675: training loss=0.00\n",
            "Step 537/675: training loss=0.00\n",
            "Step 538/675: training loss=0.00\n",
            "Step 539/675: training loss=0.00\n",
            "Step 540/675: training loss=0.00, validation loss=0.00\n",
            "Step 541/675: training loss=0.00\n",
            "Step 542/675: training loss=0.00\n",
            "Step 543/675: training loss=0.00\n",
            "Step 544/675: training loss=0.00\n",
            "Step 545/675: training loss=0.00\n",
            "Step 546/675: training loss=0.00\n",
            "Step 547/675: training loss=0.00\n",
            "Step 548/675: training loss=0.00\n",
            "Step 549/675: training loss=0.00\n",
            "Step 550/675: training loss=0.00, validation loss=0.00\n",
            "Step 551/675: training loss=0.00\n",
            "Step 552/675: training loss=0.00\n",
            "Step 553/675: training loss=0.00\n",
            "Step 554/675: training loss=0.00\n",
            "Step 555/675: training loss=0.00\n",
            "Step 556/675: training loss=0.00\n",
            "Step 557/675: training loss=0.00\n",
            "Step 558/675: training loss=0.00\n",
            "Step 559/675: training loss=0.00\n",
            "Step 560/675: training loss=0.00, validation loss=0.00\n",
            "Step 561/675: training loss=0.00\n",
            "Step 562/675: training loss=0.00\n",
            "Step 563/675: training loss=0.00\n",
            "Step 564/675: training loss=0.00\n",
            "Step 565/675: training loss=0.00\n",
            "Step 566/675: training loss=0.00\n",
            "Step 567/675: training loss=0.00\n",
            "Step 568/675: training loss=0.00\n",
            "Step 569/675: training loss=0.00\n",
            "Step 570/675: training loss=0.00, validation loss=0.00\n",
            "Step 571/675: training loss=0.00\n",
            "Step 572/675: training loss=0.00\n",
            "Step 573/675: training loss=0.00\n",
            "Step 574/675: training loss=0.00\n",
            "Step 575/675: training loss=0.00\n",
            "Step 576/675: training loss=0.00\n",
            "Step 577/675: training loss=0.00\n",
            "Step 578/675: training loss=0.00\n",
            "Step 579/675: training loss=0.00\n",
            "Step 580/675: training loss=0.00, validation loss=0.00\n",
            "Step 581/675: training loss=0.00\n",
            "Step 582/675: training loss=0.00\n",
            "Step 583/675: training loss=7.08\n",
            "Step 584/675: training loss=0.00\n",
            "Step 585/675: training loss=0.00\n",
            "Step 586/675: training loss=0.00\n",
            "Step 587/675: training loss=0.00\n",
            "Step 588/675: training loss=0.00\n",
            "Step 589/675: training loss=0.00\n",
            "Step 590/675: training loss=0.00, validation loss=7.98\n",
            "Step 591/675: training loss=0.00\n",
            "Step 592/675: training loss=0.00\n",
            "Step 593/675: training loss=4.45\n",
            "Step 594/675: training loss=0.00\n",
            "Step 595/675: training loss=0.00\n",
            "Step 596/675: training loss=0.00\n",
            "Step 597/675: training loss=0.00\n",
            "Step 598/675: training loss=0.00\n",
            "Step 599/675: training loss=0.00\n",
            "Step 600/675: training loss=0.00, validation loss=0.00\n",
            "Step 601/675: training loss=0.00\n",
            "Step 602/675: training loss=0.00\n",
            "Step 603/675: training loss=0.00\n",
            "Step 604/675: training loss=0.00\n",
            "Step 605/675: training loss=0.00\n",
            "Step 606/675: training loss=0.00\n",
            "Step 607/675: training loss=0.00\n",
            "Step 608/675: training loss=0.00\n",
            "Step 609/675: training loss=0.00\n",
            "Step 610/675: training loss=0.00, validation loss=0.00\n",
            "Step 611/675: training loss=0.00\n",
            "Step 612/675: training loss=0.00\n",
            "Step 613/675: training loss=0.00\n",
            "Step 614/675: training loss=6.49\n",
            "Step 615/675: training loss=0.00\n",
            "Step 616/675: training loss=0.00\n",
            "Step 617/675: training loss=0.00\n",
            "Step 618/675: training loss=0.00\n",
            "Step 619/675: training loss=0.00\n",
            "Step 620/675: training loss=0.00, validation loss=0.00\n",
            "Step 621/675: training loss=0.00\n",
            "Step 622/675: training loss=0.00\n",
            "Step 623/675: training loss=0.00\n",
            "Step 624/675: training loss=0.00\n",
            "Step 625/675: training loss=0.00\n",
            "Step 626/675: training loss=0.00\n",
            "Step 627/675: training loss=0.00\n",
            "Step 628/675: training loss=0.00\n",
            "Step 629/675: training loss=0.00\n",
            "Step 630/675: training loss=0.00, validation loss=0.00\n",
            "Step 631/675: training loss=0.00\n",
            "Step 632/675: training loss=0.00\n",
            "Step 633/675: training loss=0.00\n",
            "Step 634/675: training loss=0.00\n",
            "Step 635/675: training loss=0.00\n",
            "Step 636/675: training loss=0.12\n",
            "Step 637/675: training loss=0.00\n",
            "Step 638/675: training loss=0.00\n",
            "Step 639/675: training loss=0.00\n",
            "Step 640/675: training loss=0.00, validation loss=0.00\n",
            "Step 641/675: training loss=0.00\n",
            "Step 642/675: training loss=0.00\n",
            "Step 643/675: training loss=0.00\n",
            "Step 644/675: training loss=0.00\n",
            "Step 645/675: training loss=0.00\n",
            "Step 646/675: training loss=0.00\n",
            "Step 647/675: training loss=0.00\n",
            "Step 648/675: training loss=0.00\n",
            "Step 649/675: training loss=0.00\n",
            "Step 650/675: training loss=0.00, validation loss=0.00\n",
            "Step 651/675: training loss=0.00\n",
            "Step 652/675: training loss=0.00\n",
            "Step 653/675: training loss=0.00\n",
            "Step 654/675: training loss=0.00\n",
            "Step 655/675: training loss=0.00\n",
            "Step 656/675: training loss=0.00\n",
            "Step 657/675: training loss=0.00\n",
            "Step 658/675: training loss=0.00\n",
            "Step 659/675: training loss=0.00\n",
            "Step 660/675: training loss=0.00, validation loss=0.00\n",
            "Step 661/675: training loss=0.00\n",
            "Step 662/675: training loss=0.00\n",
            "Step 663/675: training loss=0.00\n",
            "Step 664/675: training loss=0.00\n",
            "Step 665/675: training loss=0.00\n",
            "Step 666/675: training loss=0.00\n",
            "Step 667/675: training loss=0.00\n",
            "Step 668/675: training loss=0.00\n",
            "Step 669/675: training loss=0.00\n",
            "Step 670/675: training loss=0.00, validation loss=0.00\n",
            "Step 671/675: training loss=0.00\n",
            "Step 672/675: training loss=0.00\n",
            "Step 673/675: training loss=0.00\n",
            "Step 674/675: training loss=0.00\n",
            "Step 675/675: training loss=0.00, full validation loss=0.32\n",
            "Checkpoint created at step 225 with Snapshot ID: ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWtPhm:ckpt-step-225\n",
            "Checkpoint created at step 450 with Snapshot ID: ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWuWP7:ckpt-step-450\n",
            "New fine-tuned model created: ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWuAt5\n",
            "New fine-tuned model created: ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWuAt5\n",
            "The job has successfully completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.exp(-0.13)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RInjeaJrXnsE",
        "outputId": "efa363db-c0c9-47a2-8f3e-7df6aba74bb4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8780954309205613"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.log(0.8780954309205613)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay9qZa3gY6cI",
        "outputId": "233f0d50-ade1-48ac-cac6-460f5a4d69d4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.13000000000000003"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "I1r1xR_W-LIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.fine_tuning.jobs.retrieve(job_id) # İnce ayar hakkında genel bilgiler alıyoruz öncelikle. burdan fine-tune model ismini çekeceğiz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK8s99KO-NJA",
        "outputId": "542037e7-d249-4d03-a9e0-13a5f2929104"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FineTuningJob(id='ftjob-8QePFrG4ictRCjm2F8kOP3K9', created_at=1720723853, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWuAt5', finished_at=1720725243, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-2DK8uJ3r4XlwyMWXFl6PyYe2', result_files=['file-tNsZPLZTzUPxCsQmZOPfBM2j'], seed=1718430282, status='succeeded', trained_tokens=64368, training_file='file-Ff3jrpMZovt0ydTK0Evh1r05', validation_file='file-sbowLb2wia7VFyeS8JNTG2sL', estimated_finish=None, integrations=[], user_provided_suffix='sentiment analys 4')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name= client.fine_tuning.jobs.retrieve(job_id).fine_tuned_model # model ismini çekiyoruz.\n",
        "model_name"
      ],
      "metadata": {
        "id": "VFaIvBdSB0Sw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "28e6203b-c778-4de7-813e-7840d5d02918"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWuAt5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system=\"Detect sentiment in a text.\" # Tahmin için kullanacağımız system içeriği, ince ayar için kullandığımız system içeriğiyle aynıdır."
      ],
      "metadata": {
        "id": "nNF9ZE1cCgTf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test verilerinden birkaç gözlem seçip tahminlerde bulunalım.\n",
        "display(X_test.loc[139])\n",
        "display(y_test.loc[139])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "1_mOvgmbD55H",
        "outputId": "4e682b60-b7ff-4afa-eaa8-c0bd6488e6f2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'I love this dress! it\\'s so comfy and flowy. i wore to an event and got many compliments on the color and design of the dress. i\\'m 5\\'5\" and weigh 150.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'positive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=model_name,\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": system},\n",
        "    {\"role\": \"user\", \"content\": X_test.loc[139]}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyN_Ps9pCRMw",
        "outputId": "0fc6e7ee-08ae-4114-ab0b-de15ba136d12"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(X_test.loc[85])\n",
        "display(y_test.loc[85])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "AFVlULVeEuKB",
        "outputId": "cfada78b-4133-42ca-bffd-2b315fdc0f08"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'When i saw this blouse online i thought it looked like a great combination of casual with the elegance of an intricate print. i was so disappointed when it came. the print is not centered in the front or the back. it looks like they were trying to save on the material and just cut it out wherever with no thought to symmetry. for this price the detail of centering the design was expected, especially because this is how it appeared in the picture. loved the blouse, but i will be returning it becau'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'negative'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=model_name,\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": system},\n",
        "    {\"role\": \"user\", \"content\": X_test.loc[85]}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asuEDLDWEvrw",
        "outputId": "89736fb0-0be8-4a73-8516-1a72535ce5a2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoints"
      ],
      "metadata": {
        "id": "Okg-t7APQgAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "events=client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id, limit=700)\n",
        "\n",
        "for event in events:\n",
        "  if \"full validation loss\" in event.message:\n",
        "    print(event.message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcOjCKRqLfMq",
        "outputId": "fddfef79-c41e-4f7a-c007-51708c061e1a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 675/675: training loss=0.00, full validation loss=0.32\n",
            "Step 450/675: training loss=0.00, validation loss=0.00, full validation loss=0.34\n",
            "Step 225/675: training loss=0.00, full validation loss=0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.fine_tuning.jobs.checkpoints.list(fine_tuning_job_id=job_id).data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbDT5HYiQi8c",
        "outputId": "e5cac578-6c65-4039-87ef-000f90a0b8e7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[FineTuningJobCheckpoint(id='ftckpt_3uDxRlYBEBDHe3lkbGRosEpu', created_at=1720725239, fine_tuned_model_checkpoint='ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWuAt5', fine_tuning_job_id='ftjob-8QePFrG4ictRCjm2F8kOP3K9', metrics=Metrics(full_valid_loss=None, full_valid_mean_token_accuracy=None, step=675.0, train_loss=6.35782896551973e-07, train_mean_token_accuracy=1.0, valid_loss=None, valid_mean_token_accuracy=None), object='fine_tuning.job.checkpoint', step_number=675),\n",
              " FineTuningJobCheckpoint(id='ftckpt_xf9Jtcyf5aLqgtwcWEmQkODk', created_at=1720724831, fine_tuned_model_checkpoint='ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWuWP7:ckpt-step-450', fine_tuning_job_id='ftjob-8QePFrG4ictRCjm2F8kOP3K9', metrics=Metrics(full_valid_loss=None, full_valid_mean_token_accuracy=None, step=450.0, train_loss=6.35782896551973e-07, train_mean_token_accuracy=1.0, valid_loss=6.357828776041666e-07, valid_mean_token_accuracy=1.0), object='fine_tuning.job.checkpoint', step_number=450),\n",
              " FineTuningJobCheckpoint(id='ftckpt_8HHwb2N5OMJkZb3fFPBvWn1P', created_at=1720724431, fine_tuned_model_checkpoint='ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWtPhm:ckpt-step-225', fine_tuning_job_id='ftjob-8QePFrG4ictRCjm2F8kOP3K9', metrics=Metrics(full_valid_loss=None, full_valid_mean_token_accuracy=None, step=225.0, train_loss=6.35782896551973e-07, train_mean_token_accuracy=1.0, valid_loss=None, valid_mean_token_accuracy=None), object='fine_tuning.job.checkpoint', step_number=225)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_ckpt = client.fine_tuning.jobs.checkpoints.list(fine_tuning_job_id=job_id).data[2].fine_tuned_model_checkpoint\n",
        "model_name_ckpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3UKu9E2RQrpj",
        "outputId": "fff25066-2fb1-4e40-ca1d-caea1b63c58f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ft:gpt-3.5-turbo-0125:personal:sentiment-analys-4:9jtWtPhm:ckpt-step-225'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=model_name_ckpt,\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": system},\n",
        "    {\"role\": \"user\", \"content\": X_test.loc[85]}\n",
        "  ],\n",
        "  temperature=0.0,\n",
        "  top_p=1.0\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dreDOWykQv4v",
        "outputId": "733c3ef2-e189-40e7-e3e8-23cf32ea87eb"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pUpg0DIVQzrd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}