{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Function Calling"
      ],
      "metadata": {
        "id": "KuMB5xs8loWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT 3.5 ve 4 modelleri, Haziran-Eylül 2021 ile Aralık 2023 ve öncesinden gelen verilerle eğitildiği için maalesef, bu modeller Haziran-Eylül 2021 veya Aralık 2023'den sonra ortaya çıkan ya da keşfedilen bilgileri bilemezler. Ayrıca, karmaşık matematik problemlerini çözmek için yetersizdirler. Bu durumda, modellerimiz, tanımadıkları veya zayıf oldukları konularda daha tutarlı ve doğru sonuçlar döndürebilmek için model için tanımladığımız fonksiyonu çağırarak kullanılabilir.\n",
        "\n",
        "Örneğin, vücut kitle indeksi hesaplaması Nisan 2023'den sonra keşfedilmiş olsun veya modeller vücut kitle indeksini hesaplama konusunda çok kötü olsun. Modelin vücut kitle indeksini doğru bir şekilde hesaplaması için aşağıda bir fonksiyon oluşturalım ve sonra bu fonksiyonu GPT modellerinde nasıl çağırdığımıza bakalım.\n",
        "\n",
        "Ayrıca bir text'den istediğimiz verileri çekmek içinde Function calling çok kullanışlıdır. Langchain dersinde örneğini göreceğiz.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hAHFT4NNusOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, since the GPT 3.5 and 4 models are trained with data from June-September 2021 and December 2023 and before, they cannot know information that emerges or is discovered after June-September 2021 or December 2023. They are also inadequate for solving complex math problems. In this case, our models can be used by calling the function we define for the model to return more consistent and accurate results on topics they are unfamiliar or weak in.\n",
        "\n",
        "For example, the calculation of body mass index was discovered after April 2023, or the models are very bad at calculating body mass index. Let's create a function below for the model to calculate BMI correctly and then see how we call this function in GPT models.\n",
        "\n",
        "Function calling is also very useful for pulling the data we want from a text. We will see an example in the Langchain tutorial.\n",
        "\n",
        "Translated with DeepL.com (free version)"
      ],
      "metadata": {
        "id": "Sb2k3z8vNXzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WduIsM2MmESY",
        "outputId": "6a0b7d80-6f0e-43b8-c061-e8d3ce45e480"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.36.0-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.7/328.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.36.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=userdata.get('openai_key')"
      ],
      "metadata": {
        "id": "zdFOjKXLFj0z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "q_Axunp8gstn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def body_mass_index(height,  weight):\n",
        "  \"calculate the body mass index\"\n",
        "  b_m_i=weight/(height*height)\n",
        "\n",
        "  return f\"The body mass index is about {b_m_i:.2f}\"\n",
        "\n",
        "# First, we define our function to calculate the body mass index."
      ],
      "metadata": {
        "id": "PFQgD_qDlJcw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "body_mass_index_func = {\n",
        "    \"name\": \"body_mass_index\",\n",
        "    \"description\": \"Calculates the body mass index.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"height\": {\n",
        "                \"type\": \"number\",\n",
        "                \"description\": \"person's height in meters\"\n",
        "            },\n",
        "            \"weight\": {\n",
        "                \"type\": \"number\",\n",
        "                \"description\": \"person's weight in kilograms\"\n",
        "            }\n",
        "\n",
        "        },\n",
        "        \"required\": [\"height\",  \"weight\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Fonksiyonun adını, açıklamasını (kullanım amacını), fonksiyon parametrelerini ve türlerini(type) yukarıda gösterildiği gibi tanımlıyoruz. Burada\n",
        "# en önemli kısım, yapılan açıklamalardır(description). Çünkü model, bir fonksiyonu çağırıp çağırmama kararını bu fonksiyon açıklamalarına dayalı olarak\n",
        "# verir. Ayrıca, parametre açıklamalarına dayalı olarak model, metinden ilgili parametreleri seçebilir.\n",
        "\n",
        "# Modelle sorduğunuz soru, fonksiyonun açıklamasına semantik(anlamsal) olarak çok yakınsa, model bu fonksiyonu çağıracaktır. Ancak, fonksiyon açıklaması\n",
        "# kötü yapılmış veya eksikse, model fonksiyonu çağırmayacaktır.\n",
        "\n",
        "# Model, fonksiyonu çağırdıktan sonra, doğru parametreleri metinden seçmek için iyi tanımlanmış parametre açıklamalarına sahip olmak önemlidir.\n",
        "# Eğer parametre açıklamaları iyi yapılmamışsa veya eksikse, model textden doğru parametreleri seçemez.\n",
        "\n",
        "# We define the function name, description (intended use), function parameters and type as shown above. Here we define the\n",
        "# The most important part is the description. Because the model decides whether to call a function based on these function descriptions\n",
        "# gives. Also, based on the parameter descriptions, the model can select relevant parameters from the text.\n",
        "\n",
        "# If the question you ask the model is semantically very close to the function description, the model will call that function. However, the function description\n",
        "# If it is poorly done or missing, the model will not call the function.\n",
        "\n",
        "# After the model calls the function, it is important to have well-defined parameter descriptions to select the correct parameters from the text.\n",
        "# If parameter descriptions are poorly done or missing, the model will not be able to select the correct parameters from the text.\n"
      ],
      "metadata": {
        "id": "mBz32oUBlJkW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(body_mass_index_func)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As0YG5EtJV6i",
        "outputId": "b40e89ea-064a-4630-a2dd-47c8237fe3b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the body mass index of a person who is 1800 mm tall and weight 80000 grams?\"\n",
        "\n",
        "res = client.chat.completions.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    functions=[body_mass_index_func],\n",
        "    function_call=\"auto\" # \"none\"\n",
        ")\n",
        "res\n",
        "\n",
        "# 'functions' parametresini 'body_mass_index_func' olarak ayarlayın.\n",
        "\n",
        "# 'function_call' parametresini 'auto' olarak ayarlayın."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT4bpFGBlJqV",
        "outputId": "83ecbd68-0138-47c4-b3ff-7193f5191f5b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9mzbHKFNJoMIj3ZGPuzDq8YVa7pS5', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"height\":1.8,\"weight\":80}', name='body_mass_index'), tool_calls=None))], created=1721463563, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=21, prompt_tokens=87, total_tokens=108))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res.choices[0].message.function_call.arguments\n",
        "# Prompt metninden seçilen parametreleri çekiyoruz.\n",
        "\n",
        "# Gördüğünüz gibi, çıktımız bir JSON formatındadır (string).\n",
        "# Python'da, JSON formatındaki veriyi doğrudan kullanamayız. Bu nedenle, json.loads() fonksiyonunu kullanarak veriyi dictionary formatına çevirerek\n",
        "# Python'da JSON formatındaki veriyi kullanabiliriz.\n",
        "# Veri dictionary formatına çevrildikten sonra, kolayca boy ve kilo bilgilerini çekebiliriz.\n",
        "\n",
        "# We extract the selected parameters from the Prompt text.\n",
        "\n",
        "# As you can see, our output is in JSON format (string).\n",
        "# In Python, we cannot use JSON format data directly. Therefore, we need to convert the data to dictionary format using the json.loads() function\n",
        "# In Python we can use data in JSON format.\n",
        "# Once the data is converted to dictionary format, we can easily extract height and weight information."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WGygIB04B-IS",
        "outputId": "45917beb-88c6-4ef1-eda1-7eb6e596ac95"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"height\":1.8,\"weight\":80}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "name = res.choices[0].message.function_call.name\n",
        "arguments = json.loads(res.choices[0].message.function_call.arguments) # Bir JSON dizesini bir dictionary çevirmek için json.loads() fonksiyonunu kullanın\n",
        "                                                                       # Bu fonksiyon, JSON string'i bir dictionarye çevirir.\n",
        "print(name)\n",
        "print(arguments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-1Hb8HdlJwG",
        "outputId": "b7b60bd5-3bd7-4d2f-c3d7-285cbae82839"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "body_mass_index\n",
            "{'height': 1.8, 'weight': 80}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(arguments[\"height\"])\n",
        "print(arguments[\"weight\"])\n",
        "\n",
        "# boy ve kilo bilgilerini kolayca çekebildik"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcvW2ZmJqASl",
        "outputId": "7a69d940-5433-4ea5-ab7a-bf501695a373"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8\n",
            "80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "body_mass_index(arguments[\"height\"],  arguments[\"weight\"])\n",
        "\n",
        "# tanımladığımız fonksiyon içine parametreleri yazınca istediğim outputu döndürüyor."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2KoVaY7rqk3c",
        "outputId": "2d5043f4-54be-4fb9-a74e-241e72b31ea1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The body mass index is about 24.69'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_conversation(prompt):\n",
        "\n",
        "    response = client.chat.completions.create(model='gpt-3.5-turbo',\n",
        "                                              messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                                              functions=[body_mass_index_func],\n",
        "                                              function_call=\"auto\")\n",
        "\n",
        "\n",
        "    # Step 1, modelin bir fonksiyon çağırmak(function_calling) isteyip istemediğini kontrol edin\n",
        "    if  response.choices[0].finish_reason == \"function_call\":\n",
        "        # json string'i dictionary'e çevirin\n",
        "        arguments = json.loads(response.choices[0].message.function_call.arguments)\n",
        "\n",
        "        # Step 2, fonksiyonu çağrın ve bir değişkene atayın\n",
        "        function_response = body_mass_index(arguments[\"height\"],\n",
        "                                            arguments[\"weight\"])\n",
        "\n",
        "        return function_response # fonksiyon sonucunu döndürün\n",
        "    return response.choices[0].message.content # eğer fonksiyon çağrılmıyorsa. Chatgpt'nin kendi cevabını döndür.\n",
        "\n",
        "\n",
        "# run_conversation fonksiyonuna verdiğimiz textden sonra model \"finish_reason\" olarak \"function_call\" döndürüyorsa,\n",
        "# cevaplarını body_mass_index fonksiyonu üzerinden verecektir. Aksi halde model cevaplarını kendi yöntemiyle verecektir.\n",
        "\n",
        "# If the model returns “function_call” as “finish_reason” after the text we give to the run_conversation function,\n",
        "# will give its answers via the body_mass_index function. Otherwise the model will give its answers in its own way."
      ],
      "metadata": {
        "id": "B0jvDck_qAbl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_conversation(\"What is the body mass index of a person who is 1800 mm tall and weight 80000 grams?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "V4hKu4BUqAjD",
        "outputId": "7b16dc2d-aa8f-43f1-bda5-836b83a940ec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The body mass index is about 24.69'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_conversation(\"What is the Capital of Spain?\")"
      ],
      "metadata": {
        "id": "vbGo6reupqcO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e21d86ea-819d-4682-e76a-b6a71c7c4b1e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of Spain is Madrid.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U9RwkvXEPMRP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}