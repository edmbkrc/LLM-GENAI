{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2c26845d",
      "metadata": {
        "id": "2c26845d"
      },
      "source": [
        "# **LangChain'e Giriş: Yapay Zeka Uygulamalarını Kolaylaştırma**\n",
        "\n",
        "Bu Colab Notebooku, Harrison Chase tarafından Ekim 2022'de açık kaynaklı proje olarak başlatılan ve geliştirilen yeni bir yapay zeka (AI) çerçevesi olan LangChain'i keşfetmenize yardımcı olmak için tasarlanmıştır. LangChain, büyük dil modelleri (LLM'ler) kullanarak güçlü ve ilgi çekici AI uygulamaları oluşturmayı kolaylaştıran açık kaynaklı bir platformdur.\n",
        "\n",
        "Langchain Nisan 2023'de ise şirket olarak kurulmuştur.\n",
        "\n",
        "## **LangChain'in Faydaları:**\n",
        "\n",
        "**Kullanımı Kolay:** LangChain, karmaşık AI kodlama becerilerine sahip olmasanız bile LLM'leri kullanmanıza olanak tanıyan sezgisel bir API sağlar.\n",
        "\n",
        "\n",
        "**Esnek:** LangChain, çeşitli görevleri gerçekleştirmek üzere özelleştirilebilen modüler bir mimariye sahiptir.\n",
        "\n",
        "**Güçlü:** LangChain, OpenAI, Gemini, Claude ve diğer en son LLM'leri de dahil olmak üzere çeşitli güçlü modellerden yararlanır."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction to LangChain: Facilitating Artificial Intelligence Applications\n",
        "\n",
        "This Colab Notebook is designed to help you explore LangChain, a new artificial intelligence (AI) framework launched and developed by Harrison Chase as an open source project in October 2022. LangChain is an open-source platform that makes it easy to build powerful and engaging AI applications using large language models (LLMs).\n",
        "\n",
        "Langchain was incorporated as a company in April 2023.\n",
        "\n",
        "Benefits of LangChain:\n",
        "Easy to Use: LangChain provides an intuitive API that allows you to use LLMs even if you don't have complex AI coding skills.\n",
        "\n",
        "Flexible: LangChain has a modular architecture that can be customized to perform various tasks.\n",
        "\n",
        "Powerful: LangChain leverages a variety of powerful models, including OpenAI, Gemini, Claude and other latest LLMs."
      ],
      "metadata": {
        "id": "OqYGT9MWwzuB"
      },
      "id": "OqYGT9MWwzuB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [**Langchain**](https://python.langchain.com/docs/get_started/quickstart)\n",
        "\n",
        "LangChain, AI uygulamalarının geliştirilmesini ve kullanılmasını kolaylaştırmak için tasarlanmış bir framework'dür. LangChain ile LLM'leri metin oluşturmak, dilleri tercüme etmek, kod yazmak ve çok daha fazlası için kullanabilirsiniz.\n",
        "\n",
        "## **LangChain consists of several components, including:**\n",
        "\n",
        "*   **Memory:** Used to store and manage data.\n",
        "*   **Chains:** Composed of a series of operations that perform a specific task.\n",
        "*   **Agents:** Used to run chains and interact with memory.\n",
        "*   **Tools:** Provide various utilities to make development easier.\n",
        "*   **Toolkits:** Easily use more than 1 tool together\n",
        "\n",
        "## **What You Can Do with LangChain:**\n",
        "\n",
        "LangChain can be used to perform a variety of tasks, such as:\n",
        "\n",
        "*   **Generating text**\n",
        "*   **Translating languages**\n",
        "*   **Writing code**\n",
        "*   **Answering questions**\n",
        "*   **Summarizing**\n",
        "*   **Classifying**"
      ],
      "metadata": {
        "id": "IG_Rohjs2Pp_"
      },
      "id": "IG_Rohjs2Pp_"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azZn36OAux4F",
        "outputId": "afc9b59c-e0f0-4e93-bd4f-d336b721dd79"
      },
      "id": "azZn36OAux4F",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.6/377.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -qU langchain"
      ],
      "metadata": {
        "id": "xEghZLvPSFv0"
      },
      "id": "xEghZLvPSFv0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-google-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhnUV7dAUjVN",
        "outputId": "956109a5-e9a5-4bb8-e7fa-6bccbed46ec2"
      },
      "id": "NhnUV7dAUjVN",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/75.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8j1QppD4WIp",
        "outputId": "3eb74a88-83d1-4e97-89ce-727fb2e9bf95"
      },
      "id": "J8j1QppD4WIp",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/47.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f31c4cc6",
      "metadata": {
        "id": "f31c4cc6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('openai_key')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ed0dc6a",
      "metadata": {
        "id": "9ed0dc6a"
      },
      "source": [
        "## LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa352d5f",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa352d5f",
        "outputId": "b2c454f0-9913-49be-cebd-e4c3940309eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='\"German Treasures\"' response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 28, 'total_tokens': 33}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-cdbc7c05-bbe2-4cea-ad98-ff6eca91e494-0' usage_metadata={'input_tokens': 28, 'output_tokens': 5, 'total_tokens': 33}\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",\n",
        "                 temperature=0.1,\n",
        "                 top_p=1.0)\n",
        "name = llm.invoke(\"I want to open a store selling regional materials from Germany. Suggest a fantastic name for the store.\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(name.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EdZKNxtGF6p",
        "outputId": "a9e08d26-f803-4e0c-a563-f52912e08b8c"
      },
      "id": "8EdZKNxtGF6p",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"German Treasures\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0782a2dd",
      "metadata": {
        "id": "0782a2dd"
      },
      "source": [
        "## Prompt Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a306b9d",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a306b9d",
        "outputId": "1d9d9795-1e04-4e70-a4e2-cda7ab7fe9e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to open a store selling regional products from Germany. Suggest a fantastic name for the store.\n"
          ]
        }
      ],
      "source": [
        "# langchain.prompts: Langchain, büyük dil modelleriyle (LLM'ler) çalışmayı kolaylaştırmak için tasarlanmış güçlü bir kütüphanedir.\n",
        "# langchain.prompts modülü, LLM'ler için promptları verimli bir şekilde tasarlamak için araçlar sunar.\n",
        "\n",
        "# Prompt Templates: LLM'lere vermek istediğiniz input için yeniden kullanılabilir bir yapı oluşturur. Effektif promptlar oluşturmak için\n",
        "# daha sonra doldurulan yer tutucuları (placeholders-input variables) içerir.\n",
        "\n",
        "# Input Variables: Bunlar, şablonunuzdaki prompt içeriğini dinamik olarak değiştirmenize olanak tanıyan yer tutuculardır(placeholders).\n",
        "# Formatting: Şablon içindeki yer tutucuların(input variables) gerçek değerlerle değiştirilmesi işlemidir.\n",
        "\n",
        "# langchain.prompts: Langchain is a powerful library designed to make working with large language models (LLMs) easier.\n",
        "# langchain.prompts module provides tools to efficiently design prompts for LLMs.\n",
        "\n",
        "# Prompt Templates: Creates a reusable structure for the input you want to give to LLMs. To create effective prompts\n",
        "# includes placeholders (placeholders-input variables) that are filled in later.\n",
        "\n",
        "# Input Variables: These are placeholders that allow you to dynamically change the prompt content in your template.\n",
        "# Formatting: The process of replacing the placeholders (input variables) in the template with actual values.\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['country'],\n",
        "    template = \"I want to open a store selling regional products from {country}. Suggest a fantastic name for the store.\"\n",
        ")\n",
        "p = prompt_template_name.format(country=\"Germany\")\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.invoke(p).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ImCvVPYF8OQ",
        "outputId": "01cddfcd-8a8b-45c3-e3bd-c6d42b8e94f3"
      },
      "id": "-ImCvVPYF8OQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Deutschland Delights\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for multiple inputs\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['product','country'],\n",
        "    template = \"I want to open a store selling regional {product} from {country}. Suggest a fantastic name for this.\"\n",
        ")\n",
        "p = prompt_template_name.format(product=\"dessert\", country=\"Germany\")\n",
        "print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVFdTqUV70wE",
        "outputId": "435fe817-188b-45ed-a5b9-9a3f177e9c50"
      },
      "id": "DVFdTqUV70wE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to open a store selling regional dessert from Germany. Suggest a fantastic name for this.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.invoke(p).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMFqIe4A7037",
        "outputId": "2f50a0ec-4ca7-4a67-accb-a0eb86988c9d"
      },
      "id": "PMFqIe4A7037",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"German Sweet Treats\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af406b92",
      "metadata": {
        "id": "af406b92"
      },
      "source": [
        "## Chains\n",
        "\n",
        "**LLM Chains:** Büyük Dil Modelleriyle Çalışmak İçin Güçlü Bir Tool'dür\n",
        "\n",
        "What are LLM Chains?\n",
        "\n",
        "LLM chains, büyük dil modelleriyle (LLM'ler) çalışmak üzere tasarlanmış özel Langchain bileşenleridir. LLM'leri promptlarla kolayca entegre etmenize ve zincirleme iş akışları oluşturmanıza olanak tanır.\n",
        "\n",
        "**Importance of LLM Chains:**\n",
        "\n",
        "* **LLM Yeteneklerini Genişletme:** LLM'ler metin oluşturma ve anlama açısından güçlü olsa da sınırlı yeteneklere sahiptir. LangChain Zincirleri, LLM'leri harici araçlarla (API'ler, calculator, databases vb.) birleştirerek bu sınırlamaların ötesine geçmenize olanak tanır.\n",
        "\n",
        "* **Veri işleme Esnekliği:** LLM Zincirleri, veri işleme için esnek frameworkler sağlar. LLM'ler zincirdeki diğer araçların (tools) çıktılarını kullanarak ham verileri anlayabilir ve soruları yanıtlayabilir.\n",
        "\n",
        "* **Karmaşık İşlevlerin Kolay Oluşturulması:** Zincirler, \"Araştırın, özetleyin ve ardından bu bilgiye dayanarak sorumu yanıtlayın\" gibi karmaşık talimatları step step işleyebileceği daha basit talimatlara dönüştürür.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "* **LLM Entegrasyonu:** LLM zincirleri, GPT-3 veya Jurassic-1 Jumbo gibi farklı LLM'lerini promptlarla entegre etmenize olanak tanır.\n",
        "* **İş Akışlarını Zincirleme:** LLM zincirleri, birden fazla LLM'i sırayla kullanarak karmaşık görevleri gerçekleştiren zincirleme iş akışları oluşturmanıza olanak tanır.\n",
        "* **Esneklik:** Farklı promptlar ve LLM'ler kullanarak LLM zincirlerini özelleştirebilir ve çeşitli alanlarda benzer zincirleme iş akışları oluşturabilirsiniz..\n",
        "\n",
        "**Advantages of LLM Chains:**\n",
        "\n",
        "* **Artırılmış Kapsamlılık:** Birden fazla LLM kullanmak, daha kapsamlı ve bilgilendirici sonuçlar elde etmenize olanak tanır.\n",
        "* **Artırılmış Verimlilik:** Tekrarlanan görevlerin otomatikleştirilmesi, zamandan ve emekten tasarruf sağlar.\n",
        "* **Artırılmış Esneklik:** LLM zincirleri, farklı promptlar ve LLM'ler kullanılarak özelleştirilebilir ve çeşitli görevlere uyarlanabilir."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chains\n",
        "LLM Chains: A Powerful Tool for Working with Large Language Models\n",
        "\n",
        "What are LLM Chains?\n",
        "\n",
        "LLM chains are specialized Langchain components designed to work with large language models (LLMs). They allow you to easily integrate LLMs with prompts and create chained workflows.\n",
        "\n",
        "Importance of LLM Chains:\n",
        "\n",
        "Expanding LLM Capabilities: While LLMs are powerful in terms of text generation and comprehension, they have limited capabilities. LangChain Chains allow you to go beyond these limitations by combining LLMs with external tools (APIs, calculator, databases, etc.).\n",
        "\n",
        "Data processing Flexibility: LLM Chains provide flexible frameworks for data processing. LLMs can understand raw data and answer questions using the output of other tools in the chain.\n",
        "\n",
        "Easy Creation of Complex Functions: Chains transform complex instructions such as “Research, summarize, and then answer my question based on this information” into simpler instructions that can be processed step by step.\n",
        "\n",
        "Key Features:\n",
        "\n",
        "LLM Integration: LLM chains allow you to integrate different LLMs such as GPT-3 or Jurassic-1 Jumbo with prompts.\n",
        "Chaining Workflows: LLM chains allow you to create chained workflows that perform complex tasks using multiple LLMs in sequence.\n",
        "\n",
        "Flexibility: You can customize LLM chains using different prompts and LLMs and create similar chaining workflows in various fields.\n",
        "Advantages of LLM Chains:\n",
        "\n",
        "Increased Comprehensiveness: Using multiple LLMs allows you to achieve more comprehensive and informative results.\n",
        "Increased Efficiency: Automating repetitive tasks saves time and effort.\n",
        "Increased Flexibility: LLM chains can be customized and adapted to various tasks using different prompts and LLMs."
      ],
      "metadata": {
        "id": "8Y9OPZyFwd-7"
      },
      "id": "8Y9OPZyFwd-7"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxYu7FA3s9kk",
        "outputId": "44c001c3-6889-4813-d8be-8eff6aee6c1a"
      },
      "id": "VxYu7FA3s9kk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['country', 'product'], template='I want to open a store selling regional {product} from {country}. Suggest a fantastic name for this.')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba65c213",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba65c213",
        "outputId": "67bcca06-ffb5-43f2-e7df-600245fe7bbb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='\"Turkish Delight Haven\"', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 27, 'total_tokens': 34}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3185f510-7b66-47ab-935d-146bbc2ce0e9-0', usage_metadata={'input_tokens': 27, 'output_tokens': 7, 'total_tokens': 34})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "chain = prompt_template_name | llm #LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "name= chain.invoke({\"product\":\"dessert\", \"country\":\"Turkey\"})\n",
        "name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(name.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8c_pSA5HYB9",
        "outputId": "c855e389-48cd-4ff2-c66c-c83fbac94f43"
      },
      "id": "_8c_pSA5HYB9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Turkish Delight Haven\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "\n",
        "chain = prompt_template_name | llm | StrOutputParser() #LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "name= chain.invoke({\"product\":\"dessert\", \"country\":\"Turkey\"})\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoK2Oyojcwj5",
        "outputId": "cfa5ef25-6df2-4a11-9ea0-87f4cbdf3eae"
      },
      "id": "GoK2Oyojcwj5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Turkish Delight Haven\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5ccee75",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5ccee75",
        "outputId": "0d9d0903-7168-465e-fca3-eb153e56f245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'product': 'dessert', 'country': 'Turkey', 'text': '\"Turkish Delight Haven\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "## DEPRECATED\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "chain.invoke({\"product\":\"dessert\", \"country\":\"Turkey\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sequential Chain"
      ],
      "metadata": {
        "id": "8U3BnOnDDCXM"
      },
      "id": "8U3BnOnDDCXM"
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",\n",
        "                 temperature=0.6,\n",
        "                 top_p=1.0)\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['product','country'],\n",
        "    template = \"I want to open a store selling regional {product} from {country}. Suggest a fantastic name for this.\"\n",
        ")\n",
        "\n",
        "name_chain = prompt_template_name | llm | StrOutputParser()\n",
        "\n",
        "prompt_template_products = PromptTemplate(\n",
        "    input_variables = ['store_name'],\n",
        "    template=\"recommend some products that I can sell in the store based on {store_name}\"\n",
        ")\n",
        "\n",
        "products_chain = {\"store_name\": name_chain} | prompt_template_products | llm | StrOutputParser()\n",
        "\n",
        "content=products_chain.invoke({\"product\":\"dessert\", \"country\":\"Spain\"})\n",
        "\n",
        "print(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O7hKEtbdk6l",
        "outputId": "394b2c9b-1c76-4470-d433-24f948a77349"
      },
      "id": "1O7hKEtbdk6l",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Spanish olive oil\n",
            "2. Jamón ibérico (Iberian ham)\n",
            "3. Manchego cheese\n",
            "4. Chorizo sausage\n",
            "5. Paella rice\n",
            "6. Spanish saffron\n",
            "7. Turron (Spanish nougat)\n",
            "8. Gazpacho soup\n",
            "9. Sangria mix\n",
            "10. Spanish wines and sherries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain Tools, Toolkits and Agents: Büyük Dil Modellerini Güçlendirmek\n",
        "\n",
        "Langchain, GenerativeAI modellerini geliştirmek için tasarlanmış bir toolkit ve agent  framework'üdür. Bunu, modellere ek işlevler sağlayan ve onları daha güçlü hale getiren tool ve agent'lar aracılığıyla yapar.\n",
        "\n",
        "**LangChain Tools**\n",
        "\n",
        "Bunları bir LLM'in süper güçleri olarak düşünün. Tıpkı bir süper kahramanın tool (web, hesap makinesi vb.)'leri kullanması gibi, bir dil modeli de (GPT-4 gibi) özel şeyler yapmak için LangChain tool'lerini kullanabilir.\n",
        "\n",
        "**Example Tools:**\n",
        "\n",
        "* **Search Engine:** LLM bilgi için internette arama yapmasına olanak tanır.\n",
        "* **Calculator:** LLM'in daha iyi hesaplamalar yapmasını sağlar.\n",
        "* **Wikipedia:** LLM'in wikipedia'daki bilgileri baz olarak daha doğru cevaplar vermesini sağlar.\n",
        "\n",
        "**Langchain Toolkits**\n",
        "\n",
        "toolkit, belirli bir görev yelpazesini ele almak için tasarlanmış bir dizi Tool'dür. Süper Kahramanın aynı anda birden fazla süper gücünü kullanması gibi düşünebilirsiniz.\n",
        "\n",
        "Here are some examples of toolkits:\n",
        "\n",
        "* **Natural Language Processing Toolkit:** Bu araç seti, metin özetleme, duygu analizi ve metin oluşturma gibi görevlere yönelik araçlar içerir.\n",
        "* **Machine Learning Toolkit:** Bu araç seti, veri ön işleme, model eğitimi ve model değerlendirme gibi görevlere yönelik araçlar içerir.\n",
        "* **Data Science Toolkit:** Bu araç seti, veri görselleştirme, veri temizleme ve istatistiksel analiz gibi görevlere yönelik araçlar içerir.\n",
        "\n",
        "**LangChain Agents**\n",
        "\n",
        "* Onları operasyonun arkasındaki beyinler olarak düşünün. Bir agent, LLM için bir görev planlayıcı gibidir. Hangi araçların ne zaman kullanılacağına karar verir.\n",
        "* **The mission:** Agent görevi yerine getirmek için LLM'lerin süper güçlerini (tools) kullanır.\n",
        "\n",
        "**Analogy: The Superhero Team**\n",
        "\n",
        "* **The Language Model:** Bu, dili anlama ve üretme konusunda temel yeteneğe sahip olan süper kahramanınızdır.\n",
        "* **The Tools:** Bunlar süper kahramanınızın süper güçleridir: internete bağlanma, wikipedia'ya bağlanma, hesaplama desteği alma.\n",
        "* **The Toolkits:** Bunu, birden fazla süper gücü aynı anda kullanan süper kahraman olarak tanımlayabiliriz.\n",
        "* **The Agent:** Bunu, Yenilmezler(The Avengers)'deki Nick Fury gibi bir strateji uzmanı olarak düşünebiliriz. Ajan, süper kahramanlara görevler verir ve onlara güçlerini ne zaman kullanacaklarını söyler.\n",
        "\n",
        "\n",
        "**How It Works Together**\n",
        "\n",
        "* **The Task:** Agent, \"Fransa ekonomisi hakkında bir rapor yazın\" gibi bir görev alır.\n",
        "* **The Plan:** Agent karar verir:\n",
        "Güncel bilgilere ihtiyacım var (Arama Motoru aracı)\n",
        "Analiz edilecek karmaşık veriler olabilir (Calculator aracı)\n",
        "* **The Action:** Agent, LLM'e şunları söyler:\n",
        "\"İnternette 'Fransa ekonomisi' diye arama yapın.\"\n",
        "\"Önemli noktaları özetleyin ve veriler üzerinde bazı hesaplamalar yapın.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "OdOCRlO9VEmH"
      },
      "id": "OdOCRlO9VEmH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain Tools, Toolkits and Agents: Empowering Big Language Models\n",
        "\n",
        "Langchain is a toolkit and agent framework designed to enhance GenerativeAI models. It does this through tools and agents that provide additional functionality to models and make them more powerful.\n",
        "\n",
        "LangChain Tools\n",
        "\n",
        "Think of these as the superpowers of an LLM. Just like a superhero uses tools (web, calculator, etc.), a language model (like GPT-4) can use LangChain tools to do special things.\n",
        "\n",
        "Example Tools:\n",
        "\n",
        "Search Engine: Allows the LLM to search the internet for information.\n",
        "Calculator: Allows LLM to do better calculations.\n",
        "Wikipedia: Allows LLM to give more accurate answers based on information from wikipedia.\n",
        "Langchain Toolkits\n",
        "\n",
        "A toolkit is a set of Tools designed to address a specific range of tasks. Think of it like a Superhero using multiple superpowers at the same time.\n",
        "\n",
        "Here are some examples of toolkits:\n",
        "\n",
        "Natural Language Processing Toolkit: This toolkit includes tools for tasks such as text summarization, sentiment analysis and text generation.\n",
        "Machine Learning Toolkit: This toolkit includes tools for tasks such as data preprocessing, model training and model evaluation.\n",
        "Data Science Toolkit: This toolkit includes tools for tasks such as data visualization, data cleaning, and statistical analysis.\n",
        "\n",
        "LangChain Agents\n",
        "\n",
        "Think of them as the brains behind the operation. An agent is like a mission planner for LLM. It decides which tools to use and when.\n",
        "The mission: The agent uses the LLMs superpowers (tools) to fulfill the mission.\n",
        "Analogy: The Superhero Team\n",
        "\n",
        "The Language Model: This is your superhero with the basic ability to understand and produce language.\n",
        "The Tools: These are the superpowers of your superhero: connect to the internet, connect to wikipedia, get computational support.\n",
        "The Toolkits: We can define this as a superhero who uses multiple superpowers at the same time.\n",
        "The Agent: We can think of this as a strategist, like Nick Fury in The Avengers. The Agent gives superheroes tasks and tells them when to use their powers.\n",
        "How It Works Together\n",
        "\n",
        "The Task: Agent receives a task such as “Write a report on the French economy”.\n",
        "The Plan: The Agent decides: I need up-to-date information (Search Engine tool) There may be complex data to analyze (Calculator tool)\n",
        "The Action: Agent tells LLM: “Search the internet for 'French economy'.” “Summarize the key points and do some calculations on the data.”\n",
        "\n"
      ],
      "metadata": {
        "id": "vSczAEEMxJ6A"
      },
      "id": "vSczAEEMxJ6A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### [Langchain Tools](https://python.langchain.com/docs/modules/agents/tools/)"
      ],
      "metadata": {
        "id": "C7aXK0NZ2E5Q"
      },
      "id": "C7aXK0NZ2E5Q"
    },
    {
      "cell_type": "markdown",
      "id": "471b2c6b",
      "metadata": {
        "id": "471b2c6b"
      },
      "source": [
        "### [Tavily Search](https://python.langchain.com/v0.1/docs/integrations/tools/tavily_search/)\n",
        "\n",
        "##### [Please get apikey for internet search](https://tavily.com/)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('tavily_key')"
      ],
      "metadata": {
        "id": "iKhD5yq-zk3s"
      },
      "id": "iKhD5yq-zk3s",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "search_tool = TavilySearchResults(max_results=1)"
      ],
      "metadata": {
        "id": "NdBj8v7DlWgJ"
      },
      "id": "NdBj8v7DlWgJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"Obama's first name?\"\n",
        "output=search_tool.invoke(question)\n",
        "output\n",
        "\n",
        "# The output returned after the imputation we gave to the Google search engine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD3ZDDPCT_GW",
        "outputId": "3afa5867-a35d-4f4b-aab1-b2c5843a1d6a"
      },
      "id": "SD3ZDDPCT_GW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://en.wikipedia.org/wiki/Early_life_and_career_of_Barack_Obama',\n",
              "  'content': \"He served on the board of directors of the Woods Fund of Chicago, which in 1985 had been the first foundation to fund Obama's DCP, from 1993 to 2002, and served on the board of directors of The Joyce Foundation from 1994 to 2002.[55] Membership on the Joyce and Wood foundation boards, which gave out tens of millions of dollars to various local organizations while Obama was a member, helped Obama get to know and be known by influential liberal groups and cultivate a network of community activists that later supported his political career.[69] Obama served on the board of directors of the Chicago Annenberg Challenge from 1995 to 2002, as founding president and chairman of the board of directors from 1995 to 1999.[55] They married on the Hawaiian island of Maui on February 2, 1961.[6]\\nBarack Hussein Obama II, born in Honolulu on August 4, 1961, at the old Kapiolani Maternity and Gynecological Hospital at 1611 Bingham Street (a predecessor of the Kapiʻolani Medical Center for Women and Children at 1319 Punahou Street), was named for his father.[4][7][8]\\nThe Honolulu Advertiser and the Honolulu Star-Bulletin announced the birth.[9]\\nSoon after their son's birth, while Obama's father continued his education at the University of Hawaii, Ann Dunham took the infant to Seattle, Washington, where she took classes at the University of Washington from September 1961 to June 1962. Two of these cases involved ACORN suing Governor Jim Edgar under the new Motor Voter Act,[78][79] one involved a voter suing Mayor Daley under the Voting Rights Act,[80] and one involved, in the only case Obama orally argued, a whistleblowing stockbroker suing his former employer.[81]\\nAll of these appeals were resolved in favor of Obama's clients, with all the opinions authored by Obama's University of Chicago colleague Chief Judge Richard Posner.[82]\\nObama was a founding member of the board of directors of Public Allies in 1992, resigning before his wife, Michelle, became the founding executive director of Public Allies Chicago in early 1993.[55][83] From sixth grade through eighth grade at Punahou, Obama lived with his mother and Maya.[35][36]\\nObama's mother completed her coursework at the University of Hawaii for an M.A. in anthropology in December 1974.[37] After three years in Hawaii, she and Maya returned to Jakarta in August 1975,[38] where Dunham completed her contract with the Institute of Management Education and Development and started anthropological fieldwork.[39]\\nObama chose to stay with his grandparents in Honolulu to continue his studies at Punahou School for his high school years.[8][40]\\n In the summer of 1981, Obama traveled to Jakarta to visit his mother and half-sister Maya, and visited the families of Occidental College friends in Hyderabad (India) and Karachi (Pakistan) for three weeks.[49]\\nHe then transferred to Columbia University in New York City, where he majored in political science with a speciality in international relations[50][51] and in English literature.[52] Obama lived off campus in a modest rented apartment at 142 West 109th Street.[53][54]\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['question','output'],\n",
        "    template = \"Answer the question of '{question}' according to '{output}'\"\n",
        ")\n",
        "p = prompt_template_name.format(question=question, output=output)\n",
        "print(p)\n",
        "\n",
        "# We are creating a prompt template to use in the llmchain I will create below."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MlkI8OWZqyE",
        "outputId": "a04d8d64-623e-40c0-e2b5-9360f0388736"
      },
      "id": "4MlkI8OWZqyE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the question of 'Obama's first name?' according to '[{'url': 'https://en.wikipedia.org/wiki/Early_life_and_career_of_Barack_Obama', 'content': \"He served on the board of directors of the Woods Fund of Chicago, which in 1985 had been the first foundation to fund Obama's DCP, from 1993 to 2002, and served on the board of directors of The Joyce Foundation from 1994 to 2002.[55] Membership on the Joyce and Wood foundation boards, which gave out tens of millions of dollars to various local organizations while Obama was a member, helped Obama get to know and be known by influential liberal groups and cultivate a network of community activists that later supported his political career.[69] Obama served on the board of directors of the Chicago Annenberg Challenge from 1995 to 2002, as founding president and chairman of the board of directors from 1995 to 1999.[55] They married on the Hawaiian island of Maui on February 2, 1961.[6]\\nBarack Hussein Obama II, born in Honolulu on August 4, 1961, at the old Kapiolani Maternity and Gynecological Hospital at 1611 Bingham Street (a predecessor of the Kapiʻolani Medical Center for Women and Children at 1319 Punahou Street), was named for his father.[4][7][8]\\nThe Honolulu Advertiser and the Honolulu Star-Bulletin announced the birth.[9]\\nSoon after their son's birth, while Obama's father continued his education at the University of Hawaii, Ann Dunham took the infant to Seattle, Washington, where she took classes at the University of Washington from September 1961 to June 1962. Two of these cases involved ACORN suing Governor Jim Edgar under the new Motor Voter Act,[78][79] one involved a voter suing Mayor Daley under the Voting Rights Act,[80] and one involved, in the only case Obama orally argued, a whistleblowing stockbroker suing his former employer.[81]\\nAll of these appeals were resolved in favor of Obama's clients, with all the opinions authored by Obama's University of Chicago colleague Chief Judge Richard Posner.[82]\\nObama was a founding member of the board of directors of Public Allies in 1992, resigning before his wife, Michelle, became the founding executive director of Public Allies Chicago in early 1993.[55][83] From sixth grade through eighth grade at Punahou, Obama lived with his mother and Maya.[35][36]\\nObama's mother completed her coursework at the University of Hawaii for an M.A. in anthropology in December 1974.[37] After three years in Hawaii, she and Maya returned to Jakarta in August 1975,[38] where Dunham completed her contract with the Institute of Management Education and Development and started anthropological fieldwork.[39]\\nObama chose to stay with his grandparents in Honolulu to continue his studies at Punahou School for his high school years.[8][40]\\n In the summer of 1981, Obama traveled to Jakarta to visit his mother and half-sister Maya, and visited the families of Occidental College friends in Hyderabad (India) and Karachi (Pakistan) for three weeks.[49]\\nHe then transferred to Columbia University in New York City, where he majored in political science with a speciality in international relations[50][51] and in English literature.[52] Obama lived off campus in a modest rented apartment at 142 West 109th Street.[53][54]\"}]'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = prompt_template_name | llm | StrOutputParser() #LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "name= chain.invoke({\"question\":question, \"output\":output})\n",
        "name\n",
        "\n",
        "# We get the answer to our question by combining the google search tool and LLM we created with the LLMchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AGi7C9JBAFa3",
        "outputId": "c0705e5b-c999-4f7f-f0a4-e6d4ef8eb6c1"
      },
      "id": "AGi7C9JBAFa3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Barack'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",\n",
        "                 temperature=0,\n",
        "                 top_p=1,\n",
        "                 max_tokens=250)\n",
        "\n",
        "ai_msg = llm.invoke(\"2023 şampiyonlar ligini hangi takım kazandı?\")\n",
        "ai_msg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwUMXsARmBuj",
        "outputId": "2906ca47-72b8-4c77-b727-c6791c577778"
      },
      "id": "gwUMXsARmBuj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"2023 Şampiyonlar Ligi'ni kazanan takım henüz belli değil çünkü 2023 Şampiyonlar Ligi henüz oynanmamıştır. Şampiyonlar Ligi genellikle her yıl Mayıs ayında oynanır, bu yüzden 2023 Şampiyonlar Ligi'nin kazananı önümüzdeki yıl belli olacaktır.\", response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 24, 'total_tokens': 125}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-691ebf2e-b507-4272-8309-394655f34fb4-0', usage_metadata={'input_tokens': 24, 'output_tokens': 101, 'total_tokens': 125})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o\",\n",
        "                 temperature=0,\n",
        "                 top_p=1,\n",
        "                 max_tokens=250)\n",
        "\n",
        "o_msg = llm.invoke(\"2024 şampiyonlar ligini hangi takım kazandı?\")\n",
        "o_msg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwoECuGI2hhv",
        "outputId": "a77a454a-92c7-4df7-8587-527caca65127"
      },
      "id": "IwoECuGI2hhv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Üzgünüm, gelecekle ilgili olaylar hakkında bilgi veremem. 2024 UEFA Şampiyonlar Ligi'ni hangi takımın kazanacağını öğrenmek için turnuvanın sonuçlarını beklememiz gerekecek. Başka bir konuda yardımcı olabilir miyim?\", response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 20, 'total_tokens': 78}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518', 'finish_reason': 'stop', 'logprobs': None}, id='run-c471f658-c540-4461-a07b-6feeb970b697-0', usage_metadata={'input_tokens': 20, 'output_tokens': 58, 'total_tokens': 78})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"2023 şampiyonlar ligini hangi takım kazandı?\"\n",
        "output=search_tool.invoke(question)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWPCZlzTATo-",
        "outputId": "e15ef390-be96-4031-a0d0-734398bf399c"
      },
      "id": "GWPCZlzTATo-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://www.ensonhaber.com/bilgi/sampiyonlar-ligini-kim-kazandi-2023-uefa-sampiyonlar-ligi-kupasini-hangi-takim-aldi',\n",
              "  'content': \"Şampiyonlar Ligi'ni kim kazandı? 2023 UEFA Şampiyonlar Ligi kupasını hangi takım aldı? Avrupa futbolunun en büyük organizasyonu olan Şampiyonlar Ligi'nde kupayı kazanan takım belli ...\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question1 = \"2024 şampiyonlar ligini hangi takım kazandı?\"\n",
        "output1 = search_tool.invoke(question1)\n",
        "output1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvK0hdSF3EB1",
        "outputId": "73e0878e-a865-4617-e4ca-c575f1d7801b"
      },
      "id": "xvK0hdSF3EB1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://tr.wikipedia.org/wiki/2024_UEFA_Şampiyonlar_Ligi_finali',\n",
              "  'content': '2024 UEFA Şampiyonlar Ligi finali, ... Takım. O P; 1 Borussia Dortmund: 6 11 2 Paris Saint-Germain: 6 8 3 Milan: 6 8 4 Newcastle United: 6 5 Kaynak: UEFA. Sonuç C Grubu birincisi. Sıra Takım. O P; 1 Real Madrid: 6 18 2 Napoli: 6 10 3 Braga: 6 4 4 Union Berlin: 6 2 Kaynak: UEFA. Rakip Toplam skor'}]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template_name | llm | StrOutputParser()\n",
        "chain.invoke({\"question\":\"2023 şampiyonlar ligini hangi takım kazandı?\", \"output\":output})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "L7rqtZvicdlF",
        "outputId": "6eea7831-63f0-45f5-81cf-6233f27a1cf6"
      },
      "id": "L7rqtZvicdlF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2023 UEFA Şampiyonlar Ligi kupasını kazanan takım hakkında bilgiye ulaşmak için belirtilen kaynağı ziyaret edebilirsiniz: [ensonhaber.com](https://www.ensonhaber.com/bilgi/sampiyonlar-ligini-kim-kazandi-2023-uefa-sampiyonlar-ligi-kupasini-hangi-takim-aldi).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template_name | llm | StrOutputParser()\n",
        "chain.invoke({\"question\":\"2024 şampiyonlar ligini hangi takım kazandı?\", \"output\":output1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ImD_9NiI3Rbu",
        "outputId": "26e4b0fb-a8e5-4815-d49b-2d2b62b33c77"
      },
      "id": "ImD_9NiI3Rbu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Verilen bilgiye göre, 2024 UEFA Şampiyonlar Ligi finalini hangi takımın kazandığı belirtilmemiştir. Bu nedenle, 2024 Şampiyonlar Ligi'ni hangi takımın kazandığını belirlemek için daha fazla bilgiye ihtiyaç vardır.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o\",\n",
        "                 temperature=0,\n",
        "                 top_p=1,\n",
        "                 max_tokens=250)\n",
        "\n",
        "o_msg = llm.invoke(\"2024 Avrupa futbol şampiyanasını hangi takım kazandı?\")\n",
        "o_msg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHjJreF64bK6",
        "outputId": "7db7e9cf-6d36-496b-978b-d703dd3f28d2"
      },
      "id": "LHjJreF64bK6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Üzgünüm, 2024 Avrupa Futbol Şampiyonası'nın sonuçları hakkında bilgiye sahip değilim. Bilgilerim 2023 yılına kadar günceldir ve gelecekteki olaylar hakkında bilgi veremem. Şampiyonanın sonuçlarını öğrenmek için güncel spor haberlerini takip edebilirsiniz.\", response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 20, 'total_tokens': 89}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_400f27fa1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-63f8507d-bb08-414c-9ed1-7860ebbb80e5-0', usage_metadata={'input_tokens': 20, 'output_tokens': 69, 'total_tokens': 89})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"2024 Avrupa futbol şampiyanasını hangi takım kazandı?\"\n",
        "output = search_tool.invoke(question)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTHCzIfY4bb3",
        "outputId": "0fb2f7eb-9002-49bf-8968-e85c83d6fe63"
      },
      "id": "zTHCzIfY4bb3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://beinsports.com.tr/haber/iste-euro-2024te-ceyrek-finale-yukselen-takimlar-ve-eslesmeler',\n",
              "  'content': \"2024 Avrupa Futbol Şampiyonası'nda (EURO 2024) çeyrek final eşleşmeleri ve maç programı belli oldu. Almanya'nın ev sahipliğinde düzenlenen turnuvada, grup maçlarının ardından son 16 turunda eleme müsabakaları yapıldı. ... Tek maç üzerinden yapılan karşılaşmaların ardından 8 takım çeyrek finale kalma başarısı ...\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template_name | llm | StrOutputParser()\n",
        "chain.invoke({\"question\":\"2024 Avrupa futbol şampiyanasını hangi takım kazandı?\", \"output\":output})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "sVH2PmP34-gu",
        "outputId": "ba79acb4-c62d-44ea-950b-d7508dd36bdf"
      },
      "id": "sVH2PmP34-gu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Verilen bilgiye göre, 2024 Avrupa Futbol Şampiyonası'nda (EURO 2024) çeyrek final eşleşmeleri ve maç programı belli olmuş durumda. Ancak, bu bilgiye dayanarak turnuvayı hangi takımın kazandığını belirlemek mümkün değil. Çeyrek finale yükselen takımlar ve eşleşmeler hakkında bilgi verilmiş, ancak turnuvanın kazananı hakkında herhangi bir bilgi bulunmamaktadır.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Which football team won the 2024 Europe Championship?\"\n",
        "output = search_tool.invoke(question)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obvYqDhT5d8I",
        "outputId": "a8a16f64-cf4d-44ab-9bf8-06376a615242"
      },
      "id": "obvYqDhT5d8I",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://www.cnn.com/2024/07/14/sport/spain-england-euro-2024-final-spt-intl/index.html',\n",
              "  'content': \"Spain won a record-breaking fourth European Championship, defeating England 2-1 following a drama-filled second half in the Euro 2024 final on Sunday in Berlin. Nico Williams - one of Spain's ...\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template_name | llm | StrOutputParser()\n",
        "chain.invoke({\"question\":\"2024 Avrupa futbol şampiyanasını hangi takım kazandı?\", \"output\":output})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iphuP_RX5eVP",
        "outputId": "ca7a32bc-1a70-44be-af48-230eecb556a2"
      },
      "id": "iphuP_RX5eVP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"2024 Avrupa Futbol Şampiyonası'nı İspanya kazandı. İspanya, Berlin'de oynanan finalde İngiltere'yi 2-1 yenerek dördüncü Avrupa Şampiyonası zaferini elde etti.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template_name | llm | StrOutputParser()\n",
        "chain.invoke({\"question\":\"Which football team won the 2024 Europe Championship?\", \"output\":output})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WGH_l2SD52Lf",
        "outputId": "0b35b5e8-e601-41b9-8222-a5cac7ee5c64"
      },
      "id": "WGH_l2SD52Lf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'According to the provided content from the URL, Spain won the 2024 European Championship by defeating England 2-1 in the final.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Agent Types](https://python.langchain.com/docs/modules/agents/agent_types/)\n",
        "\n",
        "Evet, yukarıda gördüğünüz gibi öncelikle araçları(tools) oluşturmamız ve daha sonra bu araçların sonuçlarını LMM'ler ile birleştirmemiz gerekiyor. Bu işlemleri daha pratik bir şekilde gerçekleştirmek için Langchain Agent'ları kullanacağız.\n",
        "\n",
        "Seçeceğimiz Agent tipine göre kullanacağımız prompt değişecektir. Bu Agent'lar için hazırlanan default promptları langchain hub'ından çekeceğiz."
      ],
      "metadata": {
        "id": "73zf4tKqm4Ch"
      },
      "id": "73zf4tKqm4Ch"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is LangChain Hub?\n",
        "**LangChain Hub**, LangChain ile çalışan developerların prompts, chains(zincirler) ve agents gibi LLM bileşenlerini paylaşabileceği, keşfedebileceği ve yönetebileceği bir platformdur. Hugging Face Hub'dan ilham alan platform, LLM'leri kullanarak karmaşık LLM uygulamalarını geliştirmeyi kolaylaştırmayı amaçlıyor.\n",
        "\n",
        "**Features of LangChain Hub:**\n",
        "\n",
        "* **Sharing and Discovering Prompts:** Kullanıcılar kendi promptlarını platforma yükleyebilir ve diğer kullanıcılar tarafından oluşturulan promptlara göz atabilir..\n",
        "* **Chain Creation:** Karmaşık LLM uygulamaları oluşturmak amacıyla farklı prompt ve agentları birleştirmek için zincirler oluşturabilirsiniz.\n",
        "* **Version Management:** Promptların ve zincirlerin farklı sürümlerini takip edebilir ve uygulamanızda belirli bir sürümü kullanabilirsiniz.\n",
        "* **Easy Integration:** LangChain Hub, LangSmith ile entegredir; bu, promptları ve zincirleri doğrudan LangSmith arayüzünden kullanabileceğiniz anlamına gelir.\n",
        "\n",
        "**Benefits of LangChain Hub:**\n",
        "\n",
        "* **Rapid Development:** Platforma yüklenen hazır promptlar ve zincirler, LLM uygulamalarını geliştirme sürecini hızlandırır.\n",
        "*   **Innovative Applications:** Farklı LLM bileşenlerini birleştirerek yeni ve yaratıcı LLM uygulamaları oluşturabilirsiniz.\n",
        "*   **Community Engagement:** LangChain Hub, LLM developerlarının bir araya gelip bilgi paylaşması için bir platform sağlar.\n",
        "\n",
        "**[Langchainhub](https://smith.langchain.com/hub?organizationId=69f3fb0f-fa77-5041-8d83-c0240ae7e012)**"
      ],
      "metadata": {
        "id": "N8wTKy4OXIqE"
      },
      "id": "N8wTKy4OXIqE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, as you can see above, we first need to create tools and then combine the results of these tools with LMMs. We will use Langchain Agents to perform these operations in a more practical way.\n",
        "\n",
        "The prompt we will use will change according to the type of Agent we choose. We will pull the default prompts prepared for these Agents from the langchain hub.\n",
        "\n",
        "What is LangChain Hub?\n",
        "LangChain Hub is a platform where developers working with LangChain can share, discover and manage LLM components such as prompts, chains and agents. Inspired by Hugging Face Hub, the platform aims to make it easier to develop complex LLM applications using LLMs.\n",
        "\n",
        "Features of LangChain Hub:\n",
        "\n",
        "Sharing and Discovering Prompts: Users can upload their own prompts to the platform and browse prompts created by other users.\n",
        "Chain Creation: You can create chains to combine different prompts and agents to create complex LLM applications.\n",
        "Version Management: You can track different versions of prompts and chains and use a specific version in your application.\n",
        "Easy Integration: LangChain Hub is integrated with LangSmith, which means you can use prompts and chains directly from the LangSmith interface.\n",
        "\n",
        "Benefits of LangChain Hub:\n",
        "\n",
        "Rapid Development: Ready-made prompts and chains uploaded to the platform accelerate the process of developing LLM applications.\n",
        "Innovative Applications: By combining different LLM components, you can create new and creative LLM applications.\n",
        "Community Engagement: LangChain Hub provides a platform for LLM developers to come together and share knowledge."
      ],
      "metadata": {
        "id": "_ZMaP9lV6PI5"
      },
      "id": "_ZMaP9lV6PI5"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchainhub"
      ],
      "metadata": {
        "id": "CP26TER_bg6P"
      },
      "id": "CP26TER_bg6P",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We pull a prompt prepared for openai-tools-agent from the langchain hub. We can change this prompt if we want.\n",
        "\n",
        "from langchain import hub\n",
        "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuXCcD8xmS8j",
        "outputId": "039d5da9-ace9-4f52-a3c0-5a0cb444e6db"
      },
      "id": "HuXCcD8xmS8j",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-tools-agent', 'lc_hub_commit_hash': 'c18672812789a3b9697656dd539edf0120285dcae36396d0b548ae42a4ed66f5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.messages[0].prompt.template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bLYjRROTU4rZ",
        "outputId": "65acbd9e-6f8a-41e0-a2bb-b81ebc8758af"
      },
      "id": "bLYjRROTU4rZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are a helpful assistant'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt[0].prompt.template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3aRpnRVtdBTL",
        "outputId": "64f82010-ed07-461b-9153-a2dcdc82e5b8"
      },
      "id": "3aRpnRVtdBTL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are a helpful assistant'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change Default Prompt"
      ],
      "metadata": {
        "id": "4LvID5dhqPd8"
      },
      "id": "4LvID5dhqPd8"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt[0].prompt.template=\"\"\"Answer the question asked correctly. If you're not sure, answer \"I don't know.\" \"\"\""
      ],
      "metadata": {
        "id": "yYWSE_wUqVCP"
      },
      "id": "yYWSE_wUqVCP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvzseKvVqVJI",
        "outputId": "3743f943-f7e6-47b1-ce92-9e6f291e3e4a"
      },
      "id": "gvzseKvVqVJI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-tools-agent', 'lc_hub_commit_hash': 'c18672812789a3b9697656dd539edf0120285dcae36396d0b548ae42a4ed66f5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Answer the question asked correctly. If you\\'re not sure, answer \"I don\\'t know.\" ')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt[0].prompt.template=\"You are a helpful assistant\"\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN6ll26C6-Ux",
        "outputId": "d4e035d4-5355-4798-8baf-21ed34bcbade"
      },
      "id": "hN6ll26C6-Ux",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-tools-agent', 'lc_hub_commit_hash': 'c18672812789a3b9697656dd539edf0120285dcae36396d0b548ae42a4ed66f5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tavily-Search with Agent"
      ],
      "metadata": {
        "id": "9TVcmvcmfU05"
      },
      "id": "9TVcmvcmfU05"
    },
    {
      "cell_type": "code",
      "source": [
        "user_input=\"Which team won the 2023 UEFA champions league?\""
      ],
      "metadata": {
        "id": "DYc82jlUoG-d"
      },
      "id": "DYc82jlUoG-d",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_name = PromptTemplate.from_template(\n",
        "    \"\"\"Answer correctly the question of \"{question}\". If you're not sure, answer \"I don't know.\" \"\"\" #input_variables =['question'],\n",
        ")\n",
        "prompt_template = prompt_template_name.format(question=user_input)\n",
        "prompt_template\n",
        "\n",
        "# prompt_template_name = PromptTemplate(\n",
        "#    input_variables =['question'],\n",
        "#    template =  \"\"\"Answer correctly the question of \"{question}\". If you're not sure, answer \"I don't know.\" \"\"\")\n",
        "# p = prompt_template_name.format(question=user_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1dcsz9T2nToT",
        "outputId": "8f99aa55-8033-4773-ce9d-743d0b2bac41"
      },
      "id": "1dcsz9T2nToT",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer correctly the question of \"Which team won the 2023 UEFA champions league?\". If you\\'re not sure, answer \"I don\\'t know.\" '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "                 model=\"gpt-3.5-turbo-0125\",\n",
        "                 temperature=0,\n",
        "                 top_p=1)\n",
        "llm.invoke(prompt_template).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bbZ-GHCmhK_W",
        "outputId": "12695d0d-c4c0-4cf1-d8e7-6f8483c8864a"
      },
      "id": "bbZ-GHCmhK_W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "                 model=\"gpt-4o-mini\",\n",
        "                 temperature=0,\n",
        "                 top_p=1)\n",
        "llm.invoke(prompt_template).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P4ehTz_W90s6",
        "outputId": "48442ac7-26cd-44e5-ef34-d03aeded893c"
      },
      "id": "P4ehTz_W90s6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "                 model=\"gpt-4o\",\n",
        "                 temperature=0,\n",
        "                 top_p=1)\n",
        "llm.invoke(prompt_template).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fPTFfWDe-J3i",
        "outputId": "81e31297-9209-427c-9e63-f7bcef9abbd6"
      },
      "id": "fPTFfWDe-J3i",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Manchester City won the 2023 UEFA Champions League.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [OpenAI Tools Agent](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/openai_tools/)"
      ],
      "metadata": {
        "id": "zPhIyoWRFruv"
      },
      "id": "zPhIyoWRFruv"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN9J1wK5MJ0T",
        "outputId": "7fa870e2-dbd4-45f0-dea4-575ba17e39ca"
      },
      "id": "QN9J1wK5MJ0T",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-tools-agent', 'lc_hub_commit_hash': 'c18672812789a3b9697656dd539edf0120285dcae36396d0b548ae42a4ed66f5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "llm = ChatOpenAI(temperature=0.0,\n",
        "                 model=\"gpt-3.5-turbo-0125\",\n",
        "                 top_p=1.0)\n",
        "\n",
        "# The tools we'll give the Agent access to.\n",
        "tools=[search_tool]\n",
        "\n",
        "# Finally, let's create an openai agent with the tools, the language model, and the prompt\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "# agent_executor performs all the preparations necessary to execute a query.\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "# Let's test it out!\n",
        "agent_executor.invoke({\"input\": user_input})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCDrIgyMF4fs",
        "outputId": "1e945f4d-8bf7-4391-bcb4-01553a6f981a"
      },
      "id": "oCDrIgyMF4fs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `tavily_search_results_json` with `{'query': '2023 UEFA Champions League winner'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.sportingnews.com/us/soccer/news/who-won-champions-league-final-2023-man-city-inter-treble-rodri/ypj0yxkofcr9ki7zfnsi7i6c', 'content': \"MORE: Enjoy a minute-by-minute recap of the 2023 Champions League final as City win the European title\\nMan City win 2023 Champions League final\\nPep Guardiola and Manchester City have finally summited the mountain they began climbing seven years ago, winning the club's first European crown by topping Inter Milan 1-0 in the 2023 UEFA Champions League final.\\n The former Manchester United star had a cruel own goal in the 2020 Europa League final with Inter in defeat to Sevilla, and then suffered an outrageous string of misses with Belgium in the 2022 World Cup at the end of their eventual defeat to Croatia in the group stage.\\n Rodri gets breakthrough goal for Man City\\nFinally, the moment the game was begging for came in the 68th minute as Rodri smashed home the opening goal of the match, putting Man City in front.\\n A goal from Rodri in the 68th minute was enough to do the job, as Inter put up a true challenge but failed to find the back of the net through the match.\\n With nobody in the area, Rodri pounced, charging onto the loose ball and blasting a shot that curled around two Inter defenders and into the back of the net, as Andre Onana was rooted to the spot.\\n\"}]\u001b[0m\u001b[32;1m\u001b[1;3mManchester City won the 2023 UEFA Champions League by defeating Inter Milan 1-0 in the final. Rodri scored the breakthrough goal for Manchester City in the 68th minute.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Which team won the 2023 UEFA champions league?',\n",
              " 'output': 'Manchester City won the 2023 UEFA Champions League by defeating Inter Milan 1-0 in the final. Rodri scored the breakthrough goal for Manchester City in the 68th minute.'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "llm = ChatOpenAI(temperature=0.0,\n",
        "                 model=\"gpt-4o-mini\",\n",
        "                 top_p=1.0)\n",
        "\n",
        "# The tools we'll give the Agent access to.\n",
        "tools=[search_tool]\n",
        "\n",
        "# Finally, let's create an openai agent with the tools, the language model, and the prompt\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "# agent_executor performs all the preparations necessary to execute a query.\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "# Let's test it out!\n",
        "agent_executor.invoke({\"input\": user_input})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOfePHFw-6J-",
        "outputId": "7049490e-118d-43d1-d483-122d0985e50c"
      },
      "id": "eOfePHFw-6J-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `tavily_search_results_json` with `{'query': '2023 UEFA Champions League winner'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://en.wikipedia.org/wiki/2023_UEFA_Champions_League_Final', 'content': \"The 2023 UEFA Champions League final was the final match of the 2022-23 UEFA Champions League, the 68th season of Europe's premier club football tournament organised by UEFA.It was played at the Atatürk Olympic Stadium in Istanbul, Turkey, on 10 June 2023, between English club Manchester City and Italian club Inter Milan, with Manchester City winning 1-0 via a second-half goal by Rodri ...\"}]\u001b[0m\u001b[32;1m\u001b[1;3mManchester City won the 2023 UEFA Champions League, defeating Inter Milan 1-0 in the final match held on June 10, 2023. The only goal of the match was scored by Rodri in the second half.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Which team won the 2023 UEFA champions league?',\n",
              " 'output': 'Manchester City won the 2023 UEFA Champions League, defeating Inter Milan 1-0 in the final match held on June 10, 2023. The only goal of the match was scored by Rodri in the second half.'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_tool.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2DOGkMBV-NkO",
        "outputId": "59edf01a-7e8a-4369-a789-f4fcebb2444a"
      },
      "id": "2DOGkMBV-NkO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tavily_search_results_json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Tool Calling Agent](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/)\n",
        "\n",
        "#### With Gemini Models"
      ],
      "metadata": {
        "id": "a-WxMszbMMNG"
      },
      "id": "a-WxMszbMMNG"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-google-genai # for gemini models"
      ],
      "metadata": {
        "id": "SSgh_VUBMtzY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "SSgh_VUBMtzY"
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_API_KEY']=userdata.get('gemini_key')"
      ],
      "metadata": {
        "id": "RRlvnObj9nL9"
      },
      "id": "RRlvnObj9nL9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")"
      ],
      "metadata": {
        "id": "0rMJftp-Mtze"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0rMJftp-Mtze"
    },
    {
      "cell_type": "code",
      "source": [
        "search_tool.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IAyLwPra9sqD",
        "outputId": "6a522bd0-5cf4-4b85-e4f6-0a0c9a9af5f2"
      },
      "id": "IAyLwPra9sqD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tavily_search_results_json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Make sure to use the tavily_search_results_json tool for information.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"placeholder\", \"{agent_scratchpad}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "tools=[search_tool]\n",
        "\n",
        "# Construct the Tools agent\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "agent_executor.invoke({\"input\": \"What team won the 2023 UEFA champions league? And 5+5=? And What is langchain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c5edeb-965c-4286-e9c6-65ed5f690889",
        "id": "Q-G29hQWMtze"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `tavily_search_results_json` with `{'query': 'Who won the UEFA champions league 2023'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.sportingnews.com/us/soccer/news/who-won-champions-league-final-2023-man-city-inter-treble-rodri/ypj0yxkofcr9ki7zfnsi7i6c', 'content': \"MORE: Enjoy a minute-by-minute recap of the 2023 Champions League final as City win the European title\\nMan City win 2023 Champions League final\\nPep Guardiola and Manchester City have finally summited the mountain they began climbing seven years ago, winning the club's first European crown by topping Inter Milan 1-0 in the 2023 UEFA Champions League final.\\n The former Manchester United star had a cruel own goal in the 2020 Europa League final with Inter in defeat to Sevilla, and then suffered an outrageous string of misses with Belgium in the 2022 World Cup at the end of their eventual defeat to Croatia in the group stage.\\n Rodri gets breakthrough goal for Man City\\nFinally, the moment the game was begging for came in the 68th minute as Rodri smashed home the opening goal of the match, putting Man City in front.\\n A goal from Rodri in the 68th minute was enough to do the job, as Inter put up a true challenge but failed to find the back of the net through the match.\\n With nobody in the area, Rodri pounced, charging onto the loose ball and blasting a shot that curled around two Inter defenders and into the back of the net, as Andre Onana was rooted to the spot.\\n\"}]\u001b[0m\u001b[32;1m\u001b[1;3mManchester City won the 2023 UEFA Champions League. 5+5=10. Langchain is a framework for building applications powered by large language models. \n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What team won the 2023 UEFA champions league? And 5+5=? And What is langchain?',\n",
              " 'output': 'Manchester City won the 2023 UEFA Champions League. 5+5=10. Langchain is a framework for building applications powered by large language models. \\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "id": "Q-G29hQWMtze"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [With Claude Models](https://claude.ai/login?returnTo=%2F%3F)\n",
        "\n",
        "##### [Claude Models Names](https://docs.anthropic.com/en/docs/about-claude/models)"
      ],
      "metadata": {
        "id": "oCfo_VexPS5g"
      },
      "id": "oCfo_VexPS5g"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-anthropic"
      ],
      "metadata": {
        "id": "qdg9syltF4s8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8981e592-d657-426f-d4ee-e9c31348636d"
      },
      "id": "qdg9syltF4s8",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/866.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m860.2/866.6 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.6/866.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/318.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CLAUDE_API_KEY\"] = userdata.get(\"claude-api-key\")"
      ],
      "metadata": {
        "id": "3HwpdiJHRpx4"
      },
      "id": "3HwpdiJHRpx4",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "llm = ChatAnthropic(model='claude-3-5-sonnet-20240620', #claude-3-sonnet, claude-3-opus, claude-3-haiku\n",
        "                    temperature=0,\n",
        "                    top_p=1.0,\n",
        "                    api_key=os.environ[\"CLAUDE_API_KEY\"]\n",
        "                    )"
      ],
      "metadata": {
        "id": "XSFUgxggF4zD"
      },
      "id": "XSFUgxggF4zD",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define the search tool\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "search_tool = TavilySearchResults(max_results=1)\n",
        "\n",
        "tools=[search_tool]"
      ],
      "metadata": {
        "id": "HLXvQDXY2I8D"
      },
      "id": "HLXvQDXY2I8D",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "agent_executor.invoke({\"input\": \"What team won the 2023 UEFA champions league? And 5+5=? And What is langchain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "Bt1xQEjURhE4",
        "outputId": "0b496385-23d0-46ed-bfae-8f29b75a8f44",
        "collapsed": true
      },
      "id": "Bt1xQEjURhE4",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Claude API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-d3d063055e15>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0magent_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgentExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0magent_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"What team won the 2023 UEFA champions league? And 5+5=? And What is langchain?\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             outputs = (\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0;31m# We now enter the agent loop (until it returns something).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_continue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_elapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m             next_step_output = self._take_next_step(\n\u001b[0m\u001b[1;32m   1613\u001b[0m                 \u001b[0mname_to_tool_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m                 \u001b[0mcolor_mapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1316\u001b[0m     ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\n\u001b[1;32m   1317\u001b[0m         return self._consume_next_step(\n\u001b[0;32m-> 1318\u001b[0;31m             [\n\u001b[0m\u001b[1;32m   1319\u001b[0m                 \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m                 for a in self._iter_next_step(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1316\u001b[0m     ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\n\u001b[1;32m   1317\u001b[0m         return self._consume_next_step(\n\u001b[0;32m-> 1318\u001b[0;31m             [\n\u001b[0m\u001b[1;32m   1319\u001b[0m                 \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m                 for a in self._iter_next_step(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0;31m# Call the LLM to see what to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m             output = self.agent.plan(\n\u001b[0m\u001b[1;32m   1347\u001b[0m                 \u001b[0mintermediate_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36mplan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;31m# Because the response from the plan is not a generator, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0;31m# accumulate the output into final output and return that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunnable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfinal_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                     \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3251\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3252\u001b[0m     ) -> Iterator[Output]:\n\u001b[0;32m-> 3253\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3255\u001b[0m     async def atransform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3238\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3239\u001b[0m     ) -> Iterator[Output]:\n\u001b[0;32m-> 3240\u001b[0;31m         yield from self._transform_stream_with_config(\n\u001b[0m\u001b[1;32m   3241\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3242\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   2051\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2053\u001b[0;31m                     \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2054\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfinal_output_supported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3200\u001b[0m                 \u001b[0mfinal_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3202\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_pipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3203\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0mgot_first_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1271\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0michunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1272\u001b[0m             \u001b[0;31m# The default implementation of transform is to buffer input and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0;31m# then call stream.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5265\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5266\u001b[0m     ) -> Iterator[Output]:\n\u001b[0;32m-> 5267\u001b[0;31m         yield from self.bound.transform(\n\u001b[0m\u001b[1;32m   5268\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5269\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgot_first_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     async def atransform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m                     ),\n\u001b[1;32m    372\u001b[0m                 )\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                         \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"run-{run_manager.run_id}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_anthropic/chat_models.py\u001b[0m in \u001b[0;36m_stream\u001b[0;34m(self, messages, stop, run_manager, stream_usage, **kwargs)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_request_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0mcoerce_content_to_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_tools_in_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/resources/messages.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_given\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDEFAULT_TIMEOUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;34m\"/v1/messages\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m         )\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 942\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    943\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Claude API. Please go to Plans & Billing to upgrade or purchase credits.'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Re-Act Agent](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/react/)"
      ],
      "metadata": {
        "id": "e0WYMOtM6y9r"
      },
      "id": "e0WYMOtM6y9r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With Open Source Models"
      ],
      "metadata": {
        "id": "IqdIlOpqwVt4"
      },
      "id": "IqdIlOpqwVt4"
    },
    {
      "cell_type": "code",
      "source": [
        "## Libraries Required\n",
        "!pip install -qU langchain-huggingface\n",
        "## For API Calls\n",
        "!pip install -qU huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a13d81ed-18b2-4915-83d9-6e19a010733d",
        "id": "2-d2Zdu3wVt_"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.3/417.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "id": "2-d2Zdu3wVt_"
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=userdata.get(\"hf_read\")"
      ],
      "metadata": {
        "id": "zfUdRE4qwVt_"
      },
      "execution_count": 23,
      "outputs": [],
      "id": "zfUdRE4qwVt_"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "llm=HuggingFaceEndpoint(repo_id=repo_id,\n",
        "                        temperature=0.1,\n",
        "                        max_new_tokens=500,\n",
        "                        top_p=0.95)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a20f416e-8a7b-49a6-9d37-7ff67ce10caa",
        "id": "jMVpgilbwVuA"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "id": "jMVpgilbwVuA"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "prompt = hub.pull(\"hwchase17/react\")\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b36c13-2ea7-46d6-d5e1-f52907afe430",
        "id": "VBYou4jJ6XyS"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'], metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'}, template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "id": "VBYou4jJ6XyS"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_react_agent\n",
        "\n",
        "# The tools we'll give the Agent access to.\n",
        "tools = [search_tool]\n",
        "\n",
        "# Finally, let's create an openai agent with the tools, the language model, and the prompt\n",
        "agent = create_react_agent(llm, tools, prompt) #create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "# agent_executor performs all the preparations necessary to execute a query.\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "# Let's test it out!\n",
        "agent_executor.invoke({\"input\":\"What team won the 2023 UEFA champions league?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b8283f4-35bc-4ca0-8d2a-840ad32cd7c7",
        "id": "YO4xUplL6XyS"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out which team won the 2023 UEFA champions league\n",
            "Action: tavily_search_results_json\n",
            "Action Input: 2023 UEFA champions league winner\n",
            "Observation\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.sportingnews.com/us/soccer/news/who-won-champions-league-final-2023-man-city-inter-treble-rodri/ypj0yxkofcr9ki7zfnsi7i6c', 'content': \"MORE: Enjoy a minute-by-minute recap of the 2023 Champions League final as City win the European title\\nMan City win 2023 Champions League final\\nPep Guardiola and Manchester City have finally summited the mountain they began climbing seven years ago, winning the club's first European crown by topping Inter Milan 1-0 in the 2023 UEFA Champions League final.\\n The former Manchester United star had a cruel own goal in the 2020 Europa League final with Inter in defeat to Sevilla, and then suffered an outrageous string of misses with Belgium in the 2022 World Cup at the end of their eventual defeat to Croatia in the group stage.\\n Rodri gets breakthrough goal for Man City\\nFinally, the moment the game was begging for came in the 68th minute as Rodri smashed home the opening goal of the match, putting Man City in front.\\n A goal from Rodri in the 68th minute was enough to do the job, as Inter put up a true challenge but failed to find the back of the net through the match.\\n With nobody in the area, Rodri pounced, charging onto the loose ball and blasting a shot that curled around two Inter defenders and into the back of the net, as Andre Onana was rooted to the spot.\\n\"}]\u001b[0m\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Manchester City won the 2023 UEFA champions league.</s>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What team won the 2023 UEFA champions league?',\n",
              " 'output': 'Manchester City won the 2023 UEFA champions league.</s>'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "id": "YO4xUplL6XyS"
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id=\"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
        "llm=HuggingFaceEndpoint(repo_id=repo_id,\n",
        "                        temperature=0.1,\n",
        "                        max_new_tokens=500,\n",
        "                        top_p=0.95)\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
        "\n",
        "# Let's test it out!\n",
        "agent_executor.invoke({\"input\":\"What is langchain framework?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrUmv6XexBND",
        "outputId": "c483fa98-c1b9-4b4d-c1a0-3e83af002f76"
      },
      "id": "SrUmv6XexBND",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I don't know what langchain framework is, I need to find out.\n",
            "Action: tavily_search_results_json\n",
            "Action Input: langchain framework\n",
            "Observation\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://python.langchain.com/v0.2/docs/concepts/', 'content': 'LangChain as a framework consists of a number of packages. ... The executor will return the results of the tool call back to the model as an observation. This process repeats until the agent chooses to respond. There are general prompting based implementations that do not require any model-specific features, ...'}]\u001b[0m\u001b[32;1m\u001b[1;3m Based on the search result, LangChain is a framework that consists of a number of packages for using AI models. It's designed to work with various tools and models, and the executor returns the results of the tool call back to the model.\n",
            "Final Answer: LangChain is a framework that consists of a number of packages for using AI models. It's designed to work with various tools and models, and the executor returns the results of the tool call back to the model.</s>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is langchain framework?',\n",
              " 'output': \"LangChain is a framework that consists of a number of packages for using AI models. It's designed to work with various tools and models, and the executor returns the results of the tool call back to the model.</s>\"}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
        "\n",
        "# Let's test it out!\n",
        "agent_executor.invoke({\"input\":\"5+10=?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS-K8wXVAGuf",
        "outputId": "551131ad-bf9b-4e43-810d-5257853f0cf3"
      },
      "id": "VS-K8wXVAGuf",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m This is a simple arithmetic question, I don't need to search for the answer.\n",
            "Action: None\n",
            "Observation\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'\u001b[32;1m\u001b[1;3m5 + 10 = 15\n",
            "Final Answer: 15</s>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': '5+10=?', 'output': '15</s>'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tavily-Search and Any Function"
      ],
      "metadata": {
        "id": "7N8_1Z8vYTKM"
      },
      "id": "7N8_1Z8vYTKM"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "from langchain.tools.base import StructuredTool\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class bmi(BaseModel): # Basemodel şablon belirleme sınıfını temsil eder.\n",
        "    height: float =Field(description=\"Height in metres of person\")\n",
        "    weight: float =Field(description=\"weight in kilograms of person\")\n",
        "\n",
        "\n",
        "def calculate_bmi(height: float, weight: float) -> str:\n",
        "    return str(weight / (height * height))#mostly LLM returs str\n",
        "\n",
        "calculate_bmi=StructuredTool(\n",
        "                              name='calculate_bmi',\n",
        "                              func=calculate_bmi,\n",
        "                              description=\"Used to calculate body mass index of a person\",\n",
        "                              args_schema=bmi,\n",
        ")"
      ],
      "metadata": {
        "id": "pS0fVLk5yMWK"
      },
      "id": "pS0fVLk5yMWK",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8d6fe95-4229-4ad7-ad80-a199d25d75f6",
        "id": "P3kLjCLqAcXP"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-tools-agent', 'lc_hub_commit_hash': 'c18672812789a3b9697656dd539edf0120285dcae36396d0b548ae42a4ed66f5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "id": "P3kLjCLqAcXP"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "YDcCfjLL71h1"
      },
      "id": "YDcCfjLL71h1",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.0,\n",
        "                 model=\"gpt-4o-mini\",\n",
        "                 top_p=1.0)\n",
        "\n",
        "# The tools we'll give the Agent access to.\n",
        "tools=[search_tool, calculate_bmi]\n",
        "\n",
        "# Finally, let's create an openai agent with the tools, the language model, and the prompt\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "# agent_executor performs all the preparations necessary to execute a query.\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "# Let's test it out!\n",
        "agent_executor.invoke({\"input\": \"\"\"Which team won the 2023 UEFA champions league? And What is the body mass index of a person \\\n",
        "whose height is 1.8 m and weight is 80000 g?\"\"\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c50af82-c83c-484d-c7f0-b8b0287e08e5",
        "id": "xUIm4ZLrAcXQ"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n",
            "WARNING:langchain_community.chat_models.openai:WARNING! top_p is not default parameter.\n",
            "                    top_p was transferred to model_kwargs.\n",
            "                    Please confirm that top_p is what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `tavily_search_results_json` with `{'query': '2023 UEFA Champions League winner'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://en.wikipedia.org/wiki/2023_UEFA_Champions_League_Final', 'content': \"The 2023 UEFA Champions League final was the final match of the 2022-23 UEFA Champions League, the 68th season of Europe's premier club football tournament organised by UEFA.It was played at the Atatürk Olympic Stadium in Istanbul, Turkey, on 10 June 2023, between English club Manchester City and Italian club Inter Milan, with Manchester City winning 1-0 via a second-half goal by Rodri ...\"}]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `calculate_bmi` with `{'height': 1.8, 'weight': 80}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m24.691358024691358\u001b[0m\u001b[32;1m\u001b[1;3mThe 2023 UEFA Champions League was won by Manchester City, who defeated Inter Milan 1-0 in the final match held on June 10, 2023. The match took place at the Atatürk Olympic Stadium in Istanbul, Turkey, and the only goal was scored by Rodri in the second half. You can read more about it [here](https://en.wikipedia.org/wiki/2023_UEFA_Champions_League_Final).\n",
            "\n",
            "The body mass index (BMI) of a person with a height of 1.8 meters and a weight of 80 kilograms (80,000 grams) is approximately 24.69.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Which team won the 2023 UEFA champions league? And What is the body mass index of a person whose height is 1.8 m and weight is 80000 g?',\n",
              " 'output': 'The 2023 UEFA Champions League was won by Manchester City, who defeated Inter Milan 1-0 in the final match held on June 10, 2023. The match took place at the Atatürk Olympic Stadium in Istanbul, Turkey, and the only goal was scored by Rodri in the second half. You can read more about it [here](https://en.wikipedia.org/wiki/2023_UEFA_Champions_League_Final).\\n\\nThe body mass index (BMI) of a person with a height of 1.8 meters and a weight of 80 kilograms (80,000 grams) is approximately 24.69.'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "id": "xUIm4ZLrAcXQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Structured Agent](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/structured_chat/)"
      ],
      "metadata": {
        "id": "9x5Z3m1wBkHV"
      },
      "id": "9x5Z3m1wBkHV"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_structured_chat_agent\n",
        "\n",
        "prompt = hub.pull(\"hwchase17/structured-chat-agent\")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.0,\n",
        "                 model=\"gpt-4o-mini\",\n",
        "                 top_p=1.0)\n",
        "\n",
        "tools=[search_tool, calculate_bmi]\n",
        "\n",
        "# Construct the JSON agent\n",
        "agent = create_structured_chat_agent(llm, tools, prompt)\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
        "\n",
        "# Let's test it out!\n",
        "agent_executor.invoke({\"input\": \"5+5=?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1u7CtV5Bc6U",
        "outputId": "d8a273d0-9e17-404b-e6ca-66824367b49b"
      },
      "id": "U1u7CtV5Bc6U",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.chat_models.openai:WARNING! top_p is not default parameter.\n",
            "                    top_p was transferred to model_kwargs.\n",
            "                    Please confirm that top_p is what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```\n",
            "{\n",
            "  \"action\": \"Final Answer\",\n",
            "  \"action_input\": \"5 + 5 = 10\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': '5+5=?', 'output': '5 + 5 = 10'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09cd3a12",
      "metadata": {
        "id": "09cd3a12"
      },
      "source": [
        "### Wikipedia and Any Function tool"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0Y_n6sCf3Zm",
        "outputId": "e46a01a8-4c47-4267-acd8-6fcf3ba936a3"
      },
      "id": "X0Y_n6sCf3Zm",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "wiki_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=3, doc_content_chars_max=4000))"
      ],
      "metadata": {
        "id": "50dilH2IL_S4"
      },
      "id": "50dilH2IL_S4",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n",
        "#from langchain.tools import Tool\n",
        "\n",
        "#wiki = WikipediaAPIWrapper(top_k_results=1)\n",
        "\n",
        "# We manually create a tool that performs wikipedia search.\n",
        "#wiki_tool = Tool(\n",
        "#    name=\"wikipedia_search\",\n",
        "#    description=\"Search wikipedia for recent results.\",\n",
        "#    func=wiki.run\n",
        "#)"
      ],
      "metadata": {
        "id": "rk7Fb0bRe6IA"
      },
      "id": "rk7Fb0bRe6IA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai"
      ],
      "metadata": {
        "id": "pnOfTIh484PW"
      },
      "id": "pnOfTIh484PW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_API_KEY']=userdata.get('gemini_key')"
      ],
      "metadata": {
        "id": "srTfLJS_86WY"
      },
      "id": "srTfLJS_86WY",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")"
      ],
      "metadata": {
        "id": "ZlP5ioaLYZ0k"
      },
      "execution_count": 42,
      "outputs": [],
      "id": "ZlP5ioaLYZ0k"
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_tool.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VuBgkjQqDmbP",
        "outputId": "8a24f148-2a34-4b0b-98ce-c13f89911970"
      },
      "id": "VuBgkjQqDmbP",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wikipedia'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_bmi.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "F8j6MXGeKsc2",
        "outputId": "382ec7ec-9c3d-4058-d5d9-649343b2bcfd"
      },
      "id": "F8j6MXGeKsc2",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'calculate_bmi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Make sure to use the wikipedia tool for information. And make sure to use calculate_bmi tool for body mass index calculations\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"placeholder\", \"{agent_scratchpad}\")\n",
        "    ]\n",
        ")\n",
        "tools=[wiki_tool, calculate_bmi]\n",
        "# Construct the Tools agent\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "agent_executor.invoke({\"input\": \"What team won the 2023 UEFA champions league?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POv1v_97YZ0k",
        "outputId": "91f4c771-9e45-4f7a-d415-e9849af280ac"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `wikipedia` with `{'query': '2023 UEFA Champions League Final'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mPage: 2023 UEFA Champions League final\n",
            "Summary: The 2023 UEFA Champions League final was the final match of the 2022–23 UEFA Champions League, the 68th season of Europe's premier club football tournament organised by UEFA. It was played at the Atatürk Olympic Stadium in Istanbul, Turkey, on 10 June 2023, between English club Manchester City and Italian club Inter Milan, with Manchester City winning 1–0 via a second-half goal by Rodri, who was named man of the match by UEFA. For Manchester City, this was their first-ever European Cup, and first European trophy since 1970. Having earlier won the Premier League and FA Cup titles, they achieved the continental treble, only the second time it had been achieved in English men's football history. As winners, Manchester City earned the right to play against Sevilla, the winners of the 2022–23 UEFA Europa League, in the 2023 UEFA Super Cup, as well as qualifying for the 2023 FIFA Club World Cup; they went on to win both competitions. They also qualified for the 2025 FIFA Club World Cup through UEFA's champions pathway (winners of the 2021–2024 Champions Leagues).\n",
            "The final was originally scheduled to be played at Wembley Stadium in London, England. However, due to the postponement and relocation of the 2020 final because of the COVID-19 pandemic, the scheduled hosts for subsequent finals were shifted back a year, and the Allianz Arena in Munich was assigned the 2023 final. When the 2021 final, which had been scheduled to be played in Istanbul, also had to be relocated due to the impact of the COVID-19 pandemic, the 2023 final was given to Istanbul instead, and Munich received the 2025 final.\n",
            "\n",
            "\n",
            "\n",
            "Page: 2024 UEFA Champions League final\n",
            "Summary: The 2024 UEFA Champions League final was the final match of the 2023–24 UEFA Champions League, the 69th season of Europe's premier club football tournament organised by UEFA, and the 32nd season since it was renamed from the European Champion Clubs' Cup to the UEFA Champions League. It was held at Wembley Stadium in London, England, on 1 June 2024, between German club Borussia Dortmund and Spanish club Real Madrid. Due to the postponement and relocation of the 2020 final, the final hosts were shifted back a year, with London instead hosting the 2024 final.\n",
            "Real Madrid won the match 2–0 for a record-extending 15th title, and their sixth in eleven seasons. As winners, they earned the right to play against the winners of the 2023–24 UEFA Europa League, Atalanta, in the 2024 UEFA Super Cup, and to compete in the inaugural edition of the FIFA Intercontinental Cup. As they had already qualified for the 2025 FIFA Club World Cup, the spot intended for the winners was redistributed via the UEFA club ranking.\n",
            "\n",
            "Page: 2025 UEFA Champions League final\n",
            "Summary: The 2025 UEFA Champions League final will be the final match of the 2024–25 UEFA Champions League, the 70th season of Europe's premier club football tournament organised by UEFA, and the 33rd season since it was renamed from the European Champion Clubs' Cup to the UEFA Champions League. It will be played at Allianz Arena in Munich, Germany, on 31 May 2025. This will be the first UEFA Champions League final played under the new format of the Swiss-system.\n",
            "The winners will earn the right to play against the winners of the 2024–25 UEFA Europa League in the 2025 UEFA Super Cup.\u001b[0m\u001b[32;1m\u001b[1;3mManchester City won the 2023 UEFA Champions League. \n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What team won the 2023 UEFA champions league?',\n",
              " 'output': 'Manchester City won the 2023 UEFA Champions League. \\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "id": "POv1v_97YZ0k"
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test it out!\n",
        "agent_executor.invoke({\"input\": \"When was Albert Einstein born? If he was alive, how old would he be right now in 2024?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe7d2ba-8f25-4222-c1d3-d86d3affe226",
        "id": "8DBaFp7VgDeD"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `wikipedia` with `{'query': 'Albert Einstein'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Albert Einstein\n",
            "Summary: Albert Einstein ( EYEN-styne; German: [ˈalbɛɐt ˈʔaɪnʃtaɪn] ; 14 March 1879 – 18 April 1955) was a German-born theoretical physicist who is widely held as one of the most influential scientists. Best known for developing the theory of relativity, Einstein also made important contributions to quantum mechanics. His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world's most famous equation\". He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory. His intellectual achievements and originality have made the word Einstein broadly synonymous with genius. \n",
            "Born in the German Empire, Einstein moved to Switzerland in 1895, forsaking his German citizenship (as a subject of the Kingdom of Württemberg) the following year. In 1897, at the age of seventeen, he enrolled in the mathematics and physics teaching diploma program at the Swiss federal polytechnic school in Zürich, graduating in 1900. In 1901, he acquired Swiss citizenship, which he kept for the rest of his life. In 1903, he secured a permanent position at the Swiss Patent Office in Bern. In 1905, he submitted a successful PhD dissertation to the University of Zurich. In 1914, he moved to Berlin in order to join the Prussian Academy of Sciences and the Humboldt University of Berlin. In 1917, he became director of the Kaiser Wilhelm Institute for Physics; he also became a German citizen again, this time as a subject of the Kingdom of Prussia. In 1933, while he was visiting the United States, Adolf Hitler came to power in Germany. Horrified by the Nazi war of extermination against his fellow Jews, Einstein decided to remain in the US, and was granted American citizenship in 1940. On the eve of World War II, he endorsed a letter to President Franklin D. Roosevelt alerting him to the potential German nuclear weapons program and recommended that the US begin similar research. Einstein supported the Allies but generally viewed the idea of nuclear weapons with great dismay.\n",
            "Einstein's work is also known for its influence on the philosophy of science. In 1905, he published four groundbreaking papers, sometimes described as his annus mirabilis (miracle year). These papers outlined a theory of the photoelectric effect, explained Brownian motion, introduced his special theory of relativity—a theory which addressed the inability of classical mechanics to account satisfactorily for the behavior of the electromagnetic field—and demonstrated that if the special theory is correct, mass and energy are equivalent to each other. In 1915, he proposed a general theory of relativity that extended his system of mechanics to incorporate gravitation. A cosmological paper that he published the following year laid out the implications of general relativity for the modeling of the structure and evolution of the universe as a whole.\n",
            "In the middle part of his career, Einstein made important contributions to statistical mechanics and quantum theory. Especially notable was his work on the quantum physics of radiation, in which light consists of particles, subsequently called photons. With the Indian physicist Satyendra Nath Bose, he laid the groundwork for Bose-Einstein statistics. For much of the last phase of his academic life, Einstein worked on two endeavors that proved ultimately unsuccessful. First, he advocated against quantum theory's introduction of fundamental randomness into science's picture of the world, objecting that \"God does not play dice\". Second, he attempted to devise a unified field theory by generalizing his geometric theory of gravitation to include electromagnetism too. As a result, he became increasingly isolated from the mainstream modern physics. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was rank\u001b[0m\u001b[32;1m\u001b[1;3mAlbert Einstein was born on March 14, 1879.  If he were alive today in 2024, he would be 145 years old. \n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'When was Albert Einstein born? If he was alive, how old would he be right now in 2024?',\n",
              " 'output': 'Albert Einstein was born on March 14, 1879.  If he were alive today in 2024, he would be 145 years old. \\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "id": "8DBaFp7VgDeD"
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.memory"
      ],
      "metadata": {
        "id": "cvw8807_4GR_"
      },
      "id": "cvw8807_4GR_",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agents with Memory"
      ],
      "metadata": {
        "id": "KY31J0Gv3xL4"
      },
      "id": "KY31J0Gv3xL4"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Make sure to use the wikipedia tool for information. And make sure to use calculate_bmi tool for body mass index calculations\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"placeholder\", \"{agent_scratchpad}\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "krcnOrfHZYzh"
      },
      "id": "krcnOrfHZYzh",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "def agent_history(user):\n",
        "  from langchain_core.messages import AIMessage, HumanMessage\n",
        "  from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "  global chat_history\n",
        "  llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
        "\n",
        "  # Finally, let's create an openai agent with the tools, the language model, and the prompt\n",
        "  agent = create_tool_calling_agent(llm, tools, prompt=prompt)\n",
        "\n",
        "  # agent_executor performs all the preparations necessary to execute a query.\n",
        "  agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
        "\n",
        "  input_message = {\"input\": user, \"chat_history\": chat_history}\n",
        "\n",
        "  # Let's test it out!\n",
        "  response= agent_executor.invoke(input_message)\n",
        "\n",
        "  chat_history+=[HumanMessage(content=user)]\n",
        "  chat_history+=[AIMessage(content=response[\"output\"])]\n",
        "  chat_history=chat_history[-4:]\n",
        "  return response[\"output\"]\n"
      ],
      "metadata": {
        "id": "cnSIVBuWIKfd"
      },
      "id": "cnSIVBuWIKfd",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_history(\"'When was Albert Einstein born?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "iZgimrapI1jq",
        "outputId": "edcfa472-8444-4985-f900-ddd8fee7557f"
      },
      "id": "iZgimrapI1jq",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Albert Einstein was born on March 14, 1879. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_history(\"So when did he die?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "gg_jfbXxMa_r",
        "outputId": "1a828879-0362-45a8-c097-0384fb4ac578"
      },
      "id": "gg_jfbXxMa_r",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Albert Einstein died on April 18, 1955. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [LLMChain with Memory](https://python.langchain.com/v0.2/docs/how_to/message_history/#messages-input-messages-output)"
      ],
      "metadata": {
        "id": "eGLDFjvV4ruv"
      },
      "id": "eGLDFjvV4ruv"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
        "\n",
        "\n",
        "def get_session_history(session_id):\n",
        "    return SQLChatMessageHistory(session_id, \"sqlite:///memory.db\")"
      ],
      "metadata": {
        "id": "tVm5-CfullDw"
      },
      "id": "tVm5-CfullDw",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")"
      ],
      "metadata": {
        "id": "K8YDElW9kOZd"
      },
      "id": "K8YDElW9kOZd",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You're an assistant who speaks in {language}. Respond in 20 words or fewer\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"history\", n_messages=3),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "runnable = prompt | model\n",
        "\n",
        "runnable_with_history = RunnableWithMessageHistory(\n",
        "    runnable,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ],
      "metadata": {
        "id": "idc8Qyq9kOTl"
      },
      "id": "idc8Qyq9kOTl",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content= runnable_with_history.invoke(\n",
        "    {\"language\": \"turkish\", \"input\": \"hi im bob!\"},\n",
        "    config={\"configurable\": {\"session_id\": \"2\"}}\n",
        ")\n",
        "content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHXSCyNjkOMo",
        "outputId": "82d5bdbe-950e-400c-a5ce-1ba266fa099f"
      },
      "id": "MHXSCyNjkOMo",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/chat_message_histories/sql.py:186: LangChainDeprecationWarning: `connection_string` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. Use Use connection instead instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Merhaba Bob!  Benim adım Bard. 😊 \\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-4aa70e09-a078-4132-a18c-1161aa0156a0-0', usage_metadata={'input_tokens': 23, 'output_tokens': 10, 'total_tokens': 33})"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "W1Gj9VPrqlW8",
        "outputId": "f0a3a14b-22a2-4f20-9eef-9c6aac33124b"
      },
      "id": "W1Gj9VPrqlW8",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Merhaba Bob!  Benim adım Bard. 😊 \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content= runnable_with_history.invoke(\n",
        "    {\"language\": \"turkish\", \"input\": \"what is my name?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"2\"}},\n",
        ")\n",
        "content.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fCYXzql8urNr",
        "outputId": "26ec0df4-b267-4801-c76b-e32455284464"
      },
      "id": "fCYXzql8urNr",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Senin adın Bob. 😊 \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content= runnable_with_history.invoke(\n",
        "    {\"language\": \"turkish\", \"input\": \"what is my name?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"3\"}},\n",
        ")\n",
        "content.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Jn6qwti0u7l0",
        "outputId": "9ba5e89f-5c40-45f8-df31-f2c557420d82"
      },
      "id": "Jn6qwti0u7l0",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bana adını söylemedin. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Analysis [Toolkit](https://python.langchain.com/docs/integrations/toolkits/csv/)\n",
        "\n",
        "## [Langchain Toolkits](https://python.langchain.com/docs/integrations/toolkits/)"
      ],
      "metadata": {
        "id": "RNModKU0bvFP"
      },
      "id": "RNModKU0bvFP"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain_experimental"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a237dcfb-2301-4f2c-8a6b-8d178bbb4943",
        "id": "aTatPjPiaan4"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/203.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/203.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "id": "aTatPjPiaan4"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
        "\n",
        "llm=ChatOpenAI(temperature=0.0,model=\"gpt-4o-mini\")\n",
        "\n",
        "agent = create_csv_agent(\n",
        "    llm,\n",
        "    \"/content/Advertising.csv\",\n",
        "    verbose=True,\n",
        "    agent_type=AgentType.OPENAI_FUNCTIONS, #ZERO_SHOT_REACT_DESCRIPTION\n",
        "    allow_dangerous_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "yidZiu41aan5"
      },
      "execution_count": 64,
      "outputs": [],
      "id": "yidZiu41aan5"
    },
    {
      "cell_type": "code",
      "source": [
        "input=\"What is the average of advertisements on TV?\"\n",
        "agent.invoke({\"input\":input})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NbHVAl_qUl0",
        "outputId": "37962c3d-40cc-4202-c955-22d5b047d08e"
      },
      "id": "1NbHVAl_qUl0",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': \"df['TV'].mean()\"}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m147.0425\u001b[0m\u001b[32;1m\u001b[1;3mThe average of advertisements on TV is 147.04.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the average of advertisements on TV?',\n",
              " 'output': 'The average of advertisements on TV is 147.04.'}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke(\"What is the median of advertisements on radio?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5559f25e-315e-4bb3-ca87-3f3e8a154c4b",
        "id": "HtwEsfHXaan5"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': \"df['radio'].median()\"}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m22.9\u001b[0m\u001b[32;1m\u001b[1;3mThe median of advertisements on radio is 22.9.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the median of advertisements on radio?',\n",
              " 'output': 'The median of advertisements on radio is 22.9.'}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "id": "HtwEsfHXaan5"
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke(\"What is the average of 20 or more commercials on TV and 5 or more commercials on radio?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1GL-a25iaJU",
        "outputId": "c6a490a3-5e72-4a2b-8d13-539db50e0841"
      },
      "id": "q1GL-a25iaJU",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': \"average_sales = df[(df['TV'] >= 20) & (df['radio'] >= 5)]['sales'].mean()\\naverage_sales\"}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m15.627272727272729\u001b[0m\u001b[32;1m\u001b[1;3mThe average sales for the entries with 20 or more commercials on TV and 5 or more commercials on radio is approximately 15.63.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the average of 20 or more commercials on TV and 5 or more commercials on radio?',\n",
              " 'output': 'The average sales for the entries with 20 or more commercials on TV and 5 or more commercials on radio is approximately 15.63.'}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/Advertising.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "3KmFM6ErifPG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "256c84be-537f-48fc-832a-4c955fdffe9c"
      },
      "id": "3KmFM6ErifPG",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        TV  radio  newspaper  sales\n",
              "0    230.1   37.8       69.2   22.1\n",
              "1     44.5   39.3       45.1   10.4\n",
              "2     17.2   45.9       69.3    9.3\n",
              "3    151.5   41.3       58.5   18.5\n",
              "4    180.8   10.8       58.4   12.9\n",
              "..     ...    ...        ...    ...\n",
              "195   38.2    3.7       13.8    7.6\n",
              "196   94.2    4.9        8.1    9.7\n",
              "197  177.0    9.3        6.4   12.8\n",
              "198  283.6   42.0       66.2   25.5\n",
              "199  232.1    8.6        8.7   13.4\n",
              "\n",
              "[200 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-342cffae-5d1b-4c59-95c4-3aac6715a33a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TV</th>\n",
              "      <th>radio</th>\n",
              "      <th>newspaper</th>\n",
              "      <th>sales</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>230.1</td>\n",
              "      <td>37.8</td>\n",
              "      <td>69.2</td>\n",
              "      <td>22.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44.5</td>\n",
              "      <td>39.3</td>\n",
              "      <td>45.1</td>\n",
              "      <td>10.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>17.2</td>\n",
              "      <td>45.9</td>\n",
              "      <td>69.3</td>\n",
              "      <td>9.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>151.5</td>\n",
              "      <td>41.3</td>\n",
              "      <td>58.5</td>\n",
              "      <td>18.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>180.8</td>\n",
              "      <td>10.8</td>\n",
              "      <td>58.4</td>\n",
              "      <td>12.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>38.2</td>\n",
              "      <td>3.7</td>\n",
              "      <td>13.8</td>\n",
              "      <td>7.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>94.2</td>\n",
              "      <td>4.9</td>\n",
              "      <td>8.1</td>\n",
              "      <td>9.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>177.0</td>\n",
              "      <td>9.3</td>\n",
              "      <td>6.4</td>\n",
              "      <td>12.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>283.6</td>\n",
              "      <td>42.0</td>\n",
              "      <td>66.2</td>\n",
              "      <td>25.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>232.1</td>\n",
              "      <td>8.6</td>\n",
              "      <td>8.7</td>\n",
              "      <td>13.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-342cffae-5d1b-4c59-95c4-3aac6715a33a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-342cffae-5d1b-4c59-95c4-3aac6715a33a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-342cffae-5d1b-4c59-95c4-3aac6715a33a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3b238151-4ddd-4a7c-8012-ebfc346d8929\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3b238151-4ddd-4a7c-8012-ebfc346d8929')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3b238151-4ddd-4a7c-8012-ebfc346d8929 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d255c5bd-477c-462d-a75c-41c4448ecd0f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d255c5bd-477c-462d-a75c-41c4448ecd0f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 200,\n  \"fields\": [\n    {\n      \"column\": \"TV\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 85.85423631490808,\n        \"min\": 0.7,\n        \"max\": 296.4,\n        \"num_unique_values\": 190,\n        \"samples\": [\n          287.6,\n          286.0,\n          78.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"radio\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.846809176168724,\n        \"min\": 0.0,\n        \"max\": 49.6,\n        \"num_unique_values\": 167,\n        \"samples\": [\n          8.2,\n          36.9,\n          44.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"newspaper\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 21.778620838522826,\n        \"min\": 0.3,\n        \"max\": 114.0,\n        \"num_unique_values\": 172,\n        \"samples\": [\n          22.3,\n          5.7,\n          17.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sales\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.217456565710478,\n        \"min\": 1.6,\n        \"max\": 27.0,\n        \"num_unique_values\": 121,\n        \"samples\": [\n          11.4,\n          21.2,\n          12.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[(df['TV'] >= 20) & (df['radio'] >= 5)].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "NJ3G92_-tk23",
        "outputId": "67825ae0-bc4f-4640-ea0e-bf6c91a56da7"
      },
      "id": "NJ3G92_-tk23",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TV           161.919481\n",
              "radio         26.636364\n",
              "newspaper     31.531169\n",
              "sales         15.627273\n",
              "dtype: float64"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>TV</th>\n",
              "      <td>161.919481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radio</th>\n",
              "      <td>26.636364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>newspaper</th>\n",
              "      <td>31.531169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sales</th>\n",
              "      <td>15.627273</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V76SnlVPtv8Q"
      },
      "id": "V76SnlVPtv8Q",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}