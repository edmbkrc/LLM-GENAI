{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "624628b16bb24cdcb5f6f324376f3f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f7990d9682b41aead768a22ca169940",
              "IPY_MODEL_d26353388666468da0c5a3a82a9eb30d",
              "IPY_MODEL_91be57a5a643410c8da458df61198c1f"
            ],
            "layout": "IPY_MODEL_d455c3f1eac84dea9d4a8911320f4894"
          }
        },
        "1f7990d9682b41aead768a22ca169940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1aa8db41a2a645e79839097eefbfac64",
            "placeholder": "​",
            "style": "IPY_MODEL_49693c7224cd4e0aa675f0564c8dbd12",
            "value": "yolox_l0.05.onnx: 100%"
          }
        },
        "d26353388666468da0c5a3a82a9eb30d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0063020499b40f9a1fe286c04d5aa26",
            "max": 216625723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9159f127d0054ade99d8c9e80bbc4d03",
            "value": 216625723
          }
        },
        "91be57a5a643410c8da458df61198c1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cae1f524d6c94eacbb7ca162eca2f81e",
            "placeholder": "​",
            "style": "IPY_MODEL_94acb3934fa24a7ebff10d274cf5816c",
            "value": " 217M/217M [00:01&lt;00:00, 180MB/s]"
          }
        },
        "d455c3f1eac84dea9d4a8911320f4894": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1aa8db41a2a645e79839097eefbfac64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49693c7224cd4e0aa675f0564c8dbd12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0063020499b40f9a1fe286c04d5aa26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9159f127d0054ade99d8c9e80bbc4d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cae1f524d6c94eacbb7ca162eca2f81e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94acb3934fa24a7ebff10d274cf5816c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multimodal Agentic (Self Control) RAG Chatbot"
      ],
      "metadata": {
        "id": "mkncnDATmgfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Proje Amacı:**\n",
        "Bu proje, \"Artificial Intelligence with Python-334 sayfa\" kitabındaki metin, tablo ve görselleri kullanarak belirli bir konu üzerinde soru-cevap etkileşimi gerçekleştirebilen bir Multimodal Agentic (Self-Control) RAG (Retrieval-Augmented Generation) chatbot geliştirmeyi amaçlamaktadır. Chatbot, kitaptaki bilgileri en doğru ve ilişkili şekilde kullanarak sorulara yanıt vermelidir. Aynı zamanda, kitap haricindeki sorulara yanıt verirken Wikipedia aracı üzerinden bilgi sağlaması öngörülmektedir.\n",
        "\n",
        "**Proje Tanımı:**\n",
        "Projenin odak noktası, kullanıcıların sorduğu sorulara kitap içeriğinden yanıt verebilecek ve gerektiğinde Wikipedia'dan ek bilgi sağlayabilecek bir chatbot oluşturmaktır. Modelin yanıt üretim süreci, aşağıdaki adımları içermektedir:\n",
        "\n",
        "**Retriever ve Model İlişkilendirme:** Kullanıcı tarafından sorulan sorular, retriever tarafından kitaptaki ilgili içeriklerle eşleştirilecektir. Bu eşleştirme süreci, döndürülen içerik ile sorunun ilişkili olup olmadığını kontrol edecektir. İlişkili içerikler filtrelenip, modelin bu filtrelenmiş içerik üzerinden cevap vermesi sağlanacaktır.\n",
        "\n",
        "**Cevap ve İçerik İlişkilendirme:** Model tarafından üretilen cevap, retriever'ın döndürdüğü filtrelenmiş içerikle karşılaştırılacaktır. Cevap ile içerik arasında ilişki bulunmazsa, model aynı içerik üzerinden tekrar bir cevap üretecektir. Bu işlem, maksimum üç denemeye kadar devam edecektir.\n",
        "\n",
        "**Soru ve Cevap İlişkilendirme:** Kullanıcının sorduğu soru ile modelin döndürdüğü cevabın birbiriyle ilişkili olup olmadığı kontrol edilecektir. Eğer bir ilişki tespit edilmezse, soru revize edilerek modele tekrar beslenecektir. Bu süreç de maksimum üç denemeye kadar tekrarlanacaktır.\n",
        "\n",
        "**Wikipedia Entegrasyonu:** Kitap dışında sorulan tüm sorular, Wikipedia aracı kullanılarak cevaplanacaktır. Bu sayede, chatbot sadece belirli bir kaynaktan değil, genel bilgiye erişim sağlayabilecektir.\n"
      ],
      "metadata": {
        "id": "-_pnxpomgucg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Objective:**\n",
        "This project aims to develop a Multimodal Agentic (Self-Control) RAG (Retrieval-Augmented Generation) chatbot capable of engaging in question-and-answer interactions using the text, tables, and images from the book \"Artificial Intelligence with Python - 334 pages.\" The chatbot should accurately and contextually use the information from the book to answer questions. Additionally, for questions outside the scope of the book, the chatbot is expected to provide information via a Wikipedia tool.\n",
        "\n",
        "**Project Description:**\n",
        "The focus of this project is to create a chatbot that can respond to user questions using the content of the book and, when necessary, provide additional information from Wikipedia. The process of generating responses involves the following steps:\n",
        "\n",
        "**Retriever and Model Alignment:** Questions posed by the user will be matched with relevant content from the book by the retriever. This alignment process will verify whether the retrieved content is related to the question. The relevant content will be filtered, and the model will generate a response based on this filtered content.\n",
        "\n",
        "**Answer and Content Alignment:** The answer generated by the model will be compared with the filtered content retrieved by the retriever. If there is no alignment between the answer and the content, the model will be prompted to generate a new response based on the same content. This process will be repeated up to three attempts.\n",
        "\n",
        "**Question and Answer Alignment:** The relationship between the user’s question and the model’s response will be evaluated. If no relationship is detected, the question will be revised and fed back to the model. This process will also be repeated up to three attempts.\n",
        "\n",
        "**Wikipedia Integration:** All questions that fall outside the content of the book will be answered using the Wikipedia tool. This ensures that the chatbot can access not only specific information from the book but also general knowledge."
      ],
      "metadata": {
        "id": "ODKqtlot1ACa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain unstructured[all-docs] pydantic lxml pillow tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Drj2Suhqmhkj",
        "outputId": "635db2ad-e610-4a4d-b604-846307b88c1d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.14-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting unstructured[all-docs]\n",
            "  Downloading unstructured-0.15.7-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.32 (from langchain)\n",
            "  Downloading langchain_core-0.2.34-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.101-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.2.0)\n",
            "Collecting filetype (from unstructured[all-docs])\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured[all-docs])\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.12.3)\n",
            "Collecting emoji (from unstructured[all-docs])\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting dataclasses-json (from unstructured[all-docs])\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting python-iso639 (from unstructured[all-docs])\n",
            "  Downloading python_iso639-2024.4.27-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langdetect (from unstructured[all-docs])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz (from unstructured[all-docs])\n",
            "  Downloading rapidfuzz-3.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting backoff (from unstructured[all-docs])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.12.2)\n",
            "Collecting unstructured-client (from unstructured[all-docs])\n",
            "  Downloading unstructured_client-0.25.5-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.9.5)\n",
            "Collecting pdfminer.six (from unstructured[all-docs])\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pikepdf (from unstructured[all-docs])\n",
            "  Downloading pikepdf-9.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting python-pptx>=1.0.1 (from unstructured[all-docs])\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting onnx (from unstructured[all-docs])\n",
            "  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.3)\n",
            "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[all-docs])\n",
            "  Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.6)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.0.1)\n",
            "Collecting python-docx>=1.1.2 (from unstructured[all-docs])\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting pdf2image (from unstructured[all-docs])\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pypdf (from unstructured[all-docs])\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting google-cloud-vision (from unstructured[all-docs])\n",
            "  Downloading google_cloud_vision-3.7.4-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting effdet (from unstructured[all-docs])\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.1.5)\n",
            "Collecting pillow-heif (from unstructured[all-docs])\n",
            "  Downloading pillow_heif-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.1.4)\n",
            "Collecting pypandoc (from unstructured[all-docs])\n",
            "  Downloading pypandoc-1.13-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting python-oxmsg (from unstructured[all-docs])\n",
            "  Downloading python_oxmsg-0.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting unstructured-inference==0.7.36 (from unstructured[all-docs])\n",
            "  Downloading unstructured_inference-0.7.36-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting layoutparser (from unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting python-multipart (from unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (0.23.5)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (4.10.0.84)\n",
            "Collecting onnxruntime>=1.17.0 (from unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading onnxruntime-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (3.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (2.3.1+cu121)\n",
            "Collecting timm (from unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading timm-1.0.8-py3-none-any.whl.metadata (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (4.42.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.20.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.32->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (24.1)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting XlsxWriter>=0.5.7 (from python-pptx>=1.0.1->unstructured[all-docs])\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[all-docs]) (2.6)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->unstructured[all-docs])\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->unstructured[all-docs])\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]) (0.18.1+cu121)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]) (2.0.8)\n",
            "Collecting omegaconf>=2.0 (from effdet->unstructured[all-docs])\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (2.19.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (1.24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured[all-docs]) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (1.4.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured[all-docs]) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2024.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]) (42.0.8)\n",
            "Collecting pillow\n",
            "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting Deprecated (from pikepdf->unstructured[all-docs])\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting olefile (from python-oxmsg->unstructured[all-docs])\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting deepdiff>=6.0 (from unstructured-client->unstructured[all-docs])\n",
            "  Downloading deepdiff-7.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured[all-docs])\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mypy-extensions>=1.0.0 (from unstructured-client->unstructured[all-docs])\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.6.0)\n",
            "Collecting requests-toolbelt>=1.0.0 (from unstructured-client->unstructured[all-docs])\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (1.17.0)\n",
            "Collecting ordered-set<4.2.0,>=4.1.0 (from deepdiff>=6.0->unstructured-client->unstructured[all-docs])\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.63.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.32->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[all-docs])\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting coloredlogs (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (1.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (3.1.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->unstructured-inference==0.7.36->unstructured[all-docs]) (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (3.15.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.36->unstructured[all-docs]) (0.19.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]) (1.13.1)\n",
            "Collecting iopath (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfplumber (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.6.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting portalocker (from iopath->layoutparser->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->unstructured-inference==0.7.36->unstructured[all-docs]) (2.1.5)\n",
            "Collecting pdfminer.six (from unstructured[all-docs])\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser->unstructured-inference==0.7.36->unstructured[all-docs])\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (1.3.0)\n",
            "Downloading langchain-0.2.14-py3-none-any.whl (997 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_inference-0.7.36-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.34-py3-none-any.whl (393 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.9/393.9 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.101-py3-none-any.whl (148 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.9/148.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_cloud_vision-3.7.4-py2.py3-none-any.whl (467 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.5/467.5 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pikepdf-9.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypandoc-1.13-py3-none-any.whl (21 kB)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured-0.15.7-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.25.5-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deepdiff-7.0.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-1.0.8-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: langdetect, antlr4-python3-runtime, iopath\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=0d142eb3d181056d5807cdb1ff578e7f800fc52eadc01a970a2c34f84750a95e\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=617caf729f0c4b7e784b82b8f150a54e391d7076c52a71d7e7b8cf08e5845a48\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31529 sha256=bb3d1e42d833b51e45b09026b0cc17a16700ad2df087065773256658a4ac0818\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built langdetect antlr4-python3-runtime iopath\n",
            "Installing collected packages: filetype, antlr4-python3-runtime, XlsxWriter, tenacity, rapidfuzz, python-multipart, python-magic, python-iso639, python-docx, pypdfium2, pypdf, pypandoc, portalocker, pillow, orjson, ordered-set, onnx, omegaconf, olefile, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, langdetect, jsonpointer, jsonpath-python, humanfriendly, h11, emoji, Deprecated, backoff, unstructured.pytesseract, typing-inspect, tiktoken, requests-toolbelt, python-pptx, python-oxmsg, pillow-heif, pikepdf, pdf2image, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jsonpatch, iopath, httpcore, deepdiff, coloredlogs, pdfminer.six, onnxruntime, nvidia-cusolver-cu12, httpx, dataclasses-json, unstructured-client, pdfplumber, langsmith, unstructured, layoutparser, langchain-core, google-cloud-vision, timm, langchain-text-splitters, unstructured-inference, langchain, effdet\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "Successfully installed Deprecated-1.2.14 XlsxWriter-3.2.0 antlr4-python3-runtime-4.9.3 backoff-2.2.1 coloredlogs-15.0.1 dataclasses-json-0.6.7 deepdiff-7.0.1 effdet-0.4.1 emoji-2.12.1 filetype-1.2.0 google-cloud-vision-3.7.4 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 humanfriendly-10.0 iopath-0.1.10 jsonpatch-1.33 jsonpath-python-1.0.6 jsonpointer-3.0.0 langchain-0.2.14 langchain-core-0.2.34 langchain-text-splitters-0.2.2 langdetect-1.0.9 langsmith-0.1.101 layoutparser-0.3.4 marshmallow-3.22.0 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 olefile-0.47 omegaconf-2.3.0 onnx-1.16.2 onnxruntime-1.19.0 ordered-set-4.1.0 orjson-3.10.7 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.4 pikepdf-9.1.2 pillow-10.4.0 pillow-heif-0.18.0 portalocker-2.10.1 pypandoc-1.13 pypdf-4.3.1 pypdfium2-4.30.0 python-docx-1.1.2 python-iso639-2024.4.27 python-magic-0.4.27 python-multipart-0.0.9 python-oxmsg-0.0.1 python-pptx-1.0.2 rapidfuzz-3.9.6 requests-toolbelt-1.0.0 tenacity-8.5.0 tiktoken-0.7.0 timm-1.0.8 typing-inspect-0.9.0 unstructured-0.15.7 unstructured-client-0.25.5 unstructured-inference-0.7.36 unstructured.pytesseract-0.3.13\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "google",
                  "pydevd_plugins"
                ]
              },
              "id": "6664a96f9f114ba5a37c212b376769ba"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSrvJ8iQWs8W",
        "outputId": "d7ef8ca0-349d-4166-8ec5-0744aa3847a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "print(nltk.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3F4Tr1bW1se",
        "outputId": "020034e7-9407-4063-f78e-a28ae3709c44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-chroma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWniH1CBmkro",
        "outputId": "5c2d329e-cbbf-46a3-aa2e-fe24e85b7d22"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.7/427.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZkt3lHGntx_",
        "outputId": "106b32cd-8d52-4e40-e9ee-8415091a3e6e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSio0xu-nuON",
        "outputId": "53c9a5b5-197c-4e4d-9267-818dd4c8e8ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/362.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/318.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_key')"
      ],
      "metadata": {
        "id": "MVUQ6QX1n4FJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "#from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "Xxn3IBvjoAw_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Partition PDF tables, text, and images"
      ],
      "metadata": {
        "id": "79pLGUo2oJyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils tesseract-ocr -y libmagic-dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8pukbmcoK9C",
        "outputId": "6f1e9026-19cf-4599-87c7-37d595581e52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  libmagic-dev poppler-utils tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 5 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 5,107 kB of archives.\n",
            "After this operation, 16.7 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libmagic-dev amd64 1:5.41-3ubuntu0.1 [105 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 5,107 kB in 2s (2,512 kB/s)\n",
            "Selecting previously unselected package libmagic-dev:amd64.\n",
            "(Reading database ... 123595 files and directories currently installed.)\n",
            "Preparing to unpack .../libmagic-dev_1%3a5.41-3ubuntu0.1_amd64.deb ...\n",
            "Unpacking libmagic-dev:amd64 (1:5.41-3ubuntu0.1) ...\n",
            "Selecting previously unselected package poppler-utils.\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up libmagic-dev:amd64 (1:5.41-3ubuntu0.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18UuqI-E3ZzH",
        "outputId": "d5838231-01ac-405f-b5c1-d9318595f1fd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path= \"/content/drive/MyDrive/GENAI-LLM/Projects/Multimodel RAG chatbot with Artificial Intelligence with Python\""
      ],
      "metadata": {
        "id": "_ouAX67DoO7z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured.partition.auto import partition\n",
        "#from unstructured.partition.pdf import partition_pdf\n",
        "#from unstructured.partition.text import partition_text\n",
        "\n",
        "# Get elements\n",
        "raw_pdf_elements = partition(\n",
        "                              filename=\"/content/drive/MyDrive/GENAI-LLM/Projects/Multimodel RAG chatbot with Artificial Intelligence with Python/Artificial Intelligence with Python.pdf\",\n",
        "                              extract_images_in_pdf=True,\n",
        "                              #pdf_infer_table_structure=True,\n",
        "                              chunking_strategy=\"by_title\",\n",
        "                              max_characters=4000,    # Considering chunking_strategy, all chunks can contain a maximum of 4000 characters.\n",
        "                              new_after_n_chars=3800, # Considering chunking_strategy, all chunks can typically contain up to 3800 characters.\n",
        "                              combine_text_under_n_chars=2000,\n",
        "                              image_output_dir_path=path, #file path where images will be saved\n",
        "                              skip_infer_table_types=[\"pdf\", \"jpg\", \"png\", \"heic\"],\n",
        "                              #url=\"\"\n",
        ")\n",
        "#OSError: No such file or directory: '/root/nltk_data/tokenizers/punkt/PY3_tab'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "624628b16bb24cdcb5f6f324376f3f31",
            "1f7990d9682b41aead768a22ca169940",
            "d26353388666468da0c5a3a82a9eb30d",
            "91be57a5a643410c8da458df61198c1f",
            "d455c3f1eac84dea9d4a8911320f4894",
            "1aa8db41a2a645e79839097eefbfac64",
            "49693c7224cd4e0aa675f0564c8dbd12",
            "a0063020499b40f9a1fe286c04d5aa26",
            "9159f127d0054ade99d8c9e80bbc4d03",
            "cae1f524d6c94eacbb7ca162eca2f81e",
            "94acb3934fa24a7ebff10d274cf5816c"
          ]
        },
        "id": "lRvw0LNjoVcw",
        "outputId": "d38c456f-fafe-44c5-99c7-277f5a7fe209"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "yolox_l0.05.onnx:   0%|          | 0.00/217M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "624628b16bb24cdcb5f6f324376f3f31"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_pdf_elements\n",
        "\n",
        "# compositeElement=text chunk\n",
        "# Table=table chunk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0eUQRpzoo1H",
        "outputId": "68fb16bb-e979-4487-eeb2-be0b8635085c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<unstructured.documents.elements.CompositeElement at 0x78e46c4657e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46c46ba90>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46c46a440>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46c486cb0>,\n",
              " <unstructured.documents.elements.Table at 0x78e46c485120>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46c4bc730>,\n",
              " <unstructured.documents.elements.Table at 0x78e46c4d46d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46c4e4f40>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46c4b7c10>,\n",
              " <unstructured.documents.elements.Table at 0x78e464707b50>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464727b20>,\n",
              " <unstructured.documents.elements.Table at 0x78e46c4b4e20>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464742410>,\n",
              " <unstructured.documents.elements.Table at 0x78e464735d80>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647566e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464766da0>,\n",
              " <unstructured.documents.elements.Table at 0x78e4647657b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464766e00>,\n",
              " <unstructured.documents.elements.Table at 0x78e464764550>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464777df0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464775540>,\n",
              " <unstructured.documents.elements.Table at 0x78e464777e50>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464789480>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647880a0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464789720>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464789f00>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46478a0b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464789cc0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464789e10>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464789150>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46478a1a0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46478a200>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46478b160>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647885e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46479c9a0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647afe50>,\n",
              " <unstructured.documents.elements.Table at 0x78e4647afac0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647bcac0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647bdcc0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647bc7c0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647bdfc0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647bdf60>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647bd150>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647be0b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647bd240>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647d1090>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e1510>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e17b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e1270>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e1ba0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e1cf0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e1900>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e2080>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e2470>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e24d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e2320>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e29b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e2a10>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e2710>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e3b20>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e2c50>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e3d60>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f45e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f4490>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f51b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f53f0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f4c10>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f6110>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f6aa0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f7130>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f5fc0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f77c0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f7e50>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f6200>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f7df0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646022f0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464602200>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646166e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464614070>,\n",
              " <unstructured.documents.elements.Table at 0x78e464616f50>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464623280>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464622710>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464623c10>,\n",
              " <unstructured.documents.elements.Table at 0x78e464623a60>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646347c0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464634670>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464634eb0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464635e10>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464635a20>,\n",
              " <unstructured.documents.elements.Table at 0x78e464635bd0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646364a0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464636e30>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646370d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646378b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464635b70>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464637b50>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646355d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464634190>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464634ca0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464637550>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464636b90>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646374c0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464648850>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464648700>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464648df0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464649180>,\n",
              " <unstructured.documents.elements.Table at 0x78e464649090>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464648ee0>,\n",
              " <unstructured.documents.elements.Table at 0x78e464649b10>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464649c00>,\n",
              " <unstructured.documents.elements.Table at 0x78e464648610>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46464b8e0>,\n",
              " <unstructured.documents.elements.Table at 0x78e46464b6a0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46465d8a0>,\n",
              " <unstructured.documents.elements.Table at 0x78e46465cd30>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46465d210>,\n",
              " <unstructured.documents.elements.Table at 0x78e46465dba0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46465c4f0>,\n",
              " <unstructured.documents.elements.Table at 0x78e46465f880>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46465f5e0>,\n",
              " <unstructured.documents.elements.Table at 0x78e46465f670>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46466dae0>,\n",
              " <unstructured.documents.elements.Table at 0x78e46465e620>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46466ece0>,\n",
              " <unstructured.documents.elements.Table at 0x78e46466e3b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46466c730>,\n",
              " <unstructured.documents.elements.Table at 0x78e46466f8b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464680520>,\n",
              " <unstructured.documents.elements.Table at 0x78e46466eaa0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464680b50>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646814e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464681630>,\n",
              " <unstructured.documents.elements.Table at 0x78e464682590>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464682d70>,\n",
              " <unstructured.documents.elements.Table at 0x78e464682dd0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464690af0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464691510>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464691810>,\n",
              " <unstructured.documents.elements.Table at 0x78e464691660>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464690460>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b46d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b4580>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b4d60>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b4d00>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b4eb0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b5150>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646b4fa0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b5240>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646b53f0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b68f0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b6c80>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b6dd0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b6b30>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b71c0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b6f20>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b74c0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b7850>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b7af0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b70d0>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646b78b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a56f0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a6800>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a65c0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a6860>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a6e90>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a6c50>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a6b00>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a6fe0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a6d40>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a7190>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a7580>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a7d60>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a7e50>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646a7a60>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a76d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c4af0>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646c4370>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c55d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c5810>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646c5ea0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c6a70>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646c6530>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c7160>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c7640>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c77f0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c78e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c7be0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c7cd0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c7a90>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d43a0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d4400>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d48e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d4940>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d42b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d4be0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d4cd0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d54b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d53c0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d5360>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d5510>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d5210>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d5b40>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d5660>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d5e40>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d5f90>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646d5f30>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d6b60>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d6080>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d71f0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d7340>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d7580>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d7880>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d7ac0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d7c70>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d7d60>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d7490>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d7dc0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646e42e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646e4040>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646e4580>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646f20b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646f24a0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646f1780>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646f3160>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646f2f20>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646f3400>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46451a1d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464504310>]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(raw_pdf_elements)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMABjVdauhV5",
        "outputId": "925bf902-57bc-4390-d17a-eb93cc32853e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "228"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separating Table and Text Elements"
      ],
      "metadata": {
        "id": "JCwads9YouoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_elements = []\n",
        "table_elements = []\n",
        "\n",
        "\n",
        "for element in raw_pdf_elements:\n",
        "  if element.category == \"Table\":\n",
        "    table_elements.append(element)\n",
        "  else:\n",
        "    text_elements.append(element)"
      ],
      "metadata": {
        "id": "jjLbZC-0o8vf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_elements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orNZQArqHpik",
        "outputId": "185fbc61-ed53-42ff-a55f-d4d5c1a2608a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<unstructured.documents.elements.CompositeElement at 0x78e46c4657e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46c46ba90>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46c46a440>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46c486cb0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46c4bc730>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46c4e4f40>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46c4b7c10>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464727b20>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464742410>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647566e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464766da0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464766e00>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464777df0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464775540>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464789480>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647880a0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464789720>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464789f00>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46478a0b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464789cc0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464789e10>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464789150>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46478a1a0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46478a200>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46478b160>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647885e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46479c9a0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647afe50>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647bcac0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647bdcc0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647bc7c0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647bdfc0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647bdf60>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647bd150>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647be0b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647bd240>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647d1090>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e1510>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e17b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e1270>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e1ba0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e1cf0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e1900>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e2080>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e2470>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e24d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e2320>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e29b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e2a10>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e2710>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e3b20>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e2c50>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647e3d60>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f45e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f4490>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f51b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f53f0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f4c10>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f6110>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f6aa0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f7130>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f5fc0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f77c0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f7e50>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f6200>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4647f7df0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646022f0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464602200>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646166e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464614070>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464623280>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464622710>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464623c10>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646347c0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464634670>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464634eb0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464635e10>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464635a20>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646364a0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464636e30>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646370d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646378b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464635b70>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464637b50>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646355d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464634190>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464634ca0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464637550>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464636b90>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646374c0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464648850>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464648700>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464648df0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464649180>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464648ee0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464649c00>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46464b8e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46465d8a0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46465d210>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46465c4f0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46465f5e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46466dae0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46466ece0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46466c730>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464680520>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464680b50>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464681630>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464682d70>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464690af0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464691510>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464691810>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464690460>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b46d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b4580>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b4d60>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b4d00>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b4eb0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b5150>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b5240>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b68f0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b6c80>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b6dd0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b6b30>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b71c0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b6f20>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b74c0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b7850>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b7af0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646b70d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a56f0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a6800>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a65c0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a6860>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a6e90>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a6c50>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a6b00>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a6fe0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a6d40>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a7190>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a7580>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a7d60>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a7e50>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646a76d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c4af0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c55d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c5810>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c6a70>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c7160>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c7640>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c77f0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c78e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c7be0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c7cd0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646c7a90>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d43a0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d4400>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d48e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d4940>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d42b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d4be0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d4cd0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d54b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d53c0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d5360>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d5510>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d5210>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d5b40>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d5660>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d5e40>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d5f90>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d6b60>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d6080>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d71f0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d7340>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d7580>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d7880>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d7ac0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d7c70>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d7d60>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d7490>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646d7dc0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646e42e0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646e4040>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646e4580>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646f20b0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646f24a0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646f1780>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646f3160>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646f2f20>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e4646f3400>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e46451a1d0>,\n",
              " <unstructured.documents.elements.CompositeElement at 0x78e464504310>]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table_elements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F663OJ5S6VRF",
        "outputId": "fe7c05cd-c911-45af-ff08-95f9dfc31d92"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<unstructured.documents.elements.Table at 0x78e46c485120>,\n",
              " <unstructured.documents.elements.Table at 0x78e46c4d46d0>,\n",
              " <unstructured.documents.elements.Table at 0x78e464707b50>,\n",
              " <unstructured.documents.elements.Table at 0x78e46c4b4e20>,\n",
              " <unstructured.documents.elements.Table at 0x78e464735d80>,\n",
              " <unstructured.documents.elements.Table at 0x78e4647657b0>,\n",
              " <unstructured.documents.elements.Table at 0x78e464764550>,\n",
              " <unstructured.documents.elements.Table at 0x78e464777e50>,\n",
              " <unstructured.documents.elements.Table at 0x78e4647afac0>,\n",
              " <unstructured.documents.elements.Table at 0x78e464616f50>,\n",
              " <unstructured.documents.elements.Table at 0x78e464623a60>,\n",
              " <unstructured.documents.elements.Table at 0x78e464635bd0>,\n",
              " <unstructured.documents.elements.Table at 0x78e464649090>,\n",
              " <unstructured.documents.elements.Table at 0x78e464649b10>,\n",
              " <unstructured.documents.elements.Table at 0x78e464648610>,\n",
              " <unstructured.documents.elements.Table at 0x78e46464b6a0>,\n",
              " <unstructured.documents.elements.Table at 0x78e46465cd30>,\n",
              " <unstructured.documents.elements.Table at 0x78e46465dba0>,\n",
              " <unstructured.documents.elements.Table at 0x78e46465f880>,\n",
              " <unstructured.documents.elements.Table at 0x78e46465f670>,\n",
              " <unstructured.documents.elements.Table at 0x78e46465e620>,\n",
              " <unstructured.documents.elements.Table at 0x78e46466e3b0>,\n",
              " <unstructured.documents.elements.Table at 0x78e46466f8b0>,\n",
              " <unstructured.documents.elements.Table at 0x78e46466eaa0>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646814e0>,\n",
              " <unstructured.documents.elements.Table at 0x78e464682590>,\n",
              " <unstructured.documents.elements.Table at 0x78e464682dd0>,\n",
              " <unstructured.documents.elements.Table at 0x78e464691660>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646b4fa0>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646b53f0>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646b78b0>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646a7a60>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646c4370>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646c5ea0>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646c6530>,\n",
              " <unstructured.documents.elements.Table at 0x78e4646d5f30>]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting The Text Content of Table and Text Elements"
      ],
      "metadata": {
        "id": "85ADt9SZpcxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_elements[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "jICsqQJjpdVJ",
        "outputId": "538d15e3-35c9-4002-a70b-45c567db6d36"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Machine Learning: Foundations, Methodologies,  and Applications  Teik Toe Teoh Zheng Rong \\n\\nArtificial  Intelligence  with Python \\n\\nArtificial Intelligence with Python\\n\\nMachine Learning: Foundations, Methodologies, and Applications\\n\\nSeries Editors Kay Chen Tan, Department of Computing, Hong Kong Polytechnic University, Hong Kong, China\\n\\nDacheng Tao, University of Technology, Sydney, Australia\\n\\nBooks published in this series focus on the theory and computational foundations, advanced methodologies and practical applications of machine learning, ideally combining mathematically rigorous treatments of a contemporary topics in machine learning with speciﬁc illustrations in relevant algorithm designs and demonstrations in real-world applications. The intended readership includes research students and researchers in computer science, computer engineering, electrical engineering, data science, and related areas seeking a convenient medium to track the progresses made in the foundations, methodologies, and applications of machine learning.\\n\\nTopics considered include all areas of machine learning, including but not limited to:\\n\\nDecision tree • Artiﬁcial neural networks • Kernel learning • Bayesian learning • Ensemble methods • Dimension reduction and metric learning • Reinforcement learning • Meta learning and learning to learn • • Computational learning theory • Probabilistic graphical models • Transfer learning • Multi-view and multi-task learning • Graph neural networks • Generative adversarial networks • Federated learning\\n\\nintroductory and advanced textbooks, and state-of-the-art collections. Furthermore, it supports Open Access publication mode.\\n\\nMore information about this series at https://link.springer.com/bookseries/16715\\n\\nTeik Toe Teoh • Zheng Rong\\n\\nArtiﬁcial Intelligence with Python\\n\\nY) Springer\\n\\nTeik Toe Teoh Nanyang Business School Nanyang Technological University Singapore, Singapore\\n\\nZheng Rong Nanyang Technological University Singapore, Singapore\\n\\nISSN 2730-9908 ISSN 2730-9916 (electronic) Machine Learning: Foundations, Methodologies, and Applications ISBN 978-981-16-8614-6 https://doi.org/10.1007/978-981-16-8615-3\\n\\n© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022 This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations.\\n\\nThis Springer imprint is published by the registered company Springer Nature Singapore Pte Ltd. The registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721, Singapore'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table_elements[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "aQMgLFzrpg7D",
        "outputId": "fffad454-ee18-4e5a-9879-a08b49f1b807"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Python for Artiﬁcial Intelligence . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . Common Uses. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 1.1 Relative Popularity .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 1.1.1 Features .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 1.1.2 Syntax and Design . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 1.1.3 Scientiﬁc Programming.. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . Why Python for Artiﬁcial Intelligence .. . . . . . . .. . . . . . . . . . . . . . . . . . . . 1.2 1.3 2.1 2.2 2.3 2.4 2.5 An Introductory Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 3.1 The Task: Plotting a White Noise Process . . . . .. . . . . . . . . . . . . . . . . . . . 3.2 Our First Program .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 3.3 Imports .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 3.3.1 Importing Names Directly . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 3.3.2 Random Draws . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 3.3.3 3 3 4 5 5 6 6 9 9 9 10 10 13 14 14 15 15 17 21 22 27 27 27 28 28 30 30'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table_elements = [i.text for i in table_elements] # convert to each composite element to text\n",
        "text_elements = [i.text for i in text_elements]   # convert to each table to text\n",
        "\n",
        "# Tables\n",
        "print(len(table_elements))\n",
        "\n",
        "# Text\n",
        "print(len(text_elements))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRJazLYmpiyk",
        "outputId": "d80851a7-3451-4afa-bb82-b8684e216093"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n",
            "192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert Images to A Text-based Representation Using Base64 Encoding"
      ],
      "metadata": {
        "id": "sjrw41kqpq-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "\n",
        "def encode_image(image_path): # This function takes an image_path (the path to an image file) as input.\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')"
      ],
      "metadata": {
        "id": "tsW0nKC7pm0R"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "output_path=\"/content/figures\" ##/content/drive/MyDrive/figures\n",
        "\n",
        "os.listdir(output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmH_mmwuqIzX",
        "outputId": "5e8f48eb-ffe9-41c7-d953-330ba284bdfd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['figure-159-2081.jpg',\n",
              " 'figure-258-3625.jpg',\n",
              " 'figure-65-459.jpg',\n",
              " 'figure-315-4400.jpg',\n",
              " 'figure-214-2965.jpg',\n",
              " 'figure-215-3015.jpg',\n",
              " 'figure-199-2775.jpg',\n",
              " 'figure-269-3737.jpg',\n",
              " 'figure-214-2962.jpg',\n",
              " 'figure-253-3553.jpg',\n",
              " 'figure-60-335.jpg',\n",
              " 'figure-258-3609.jpg',\n",
              " 'figure-67-492.jpg',\n",
              " 'figure-98-1097.jpg',\n",
              " 'figure-215-2997.jpg',\n",
              " 'figure-199-2758.jpg',\n",
              " 'figure-253-3557.jpg',\n",
              " 'figure-60-358.jpg',\n",
              " 'figure-66-474.jpg',\n",
              " 'figure-204-2838.jpg',\n",
              " 'figure-96-1041.jpg',\n",
              " 'figure-60-359.jpg',\n",
              " 'figure-160-2104.jpg',\n",
              " 'figure-247-3476.jpg',\n",
              " 'figure-170-2416.jpg',\n",
              " 'figure-201-2792.jpg',\n",
              " 'figure-48-138.jpg',\n",
              " 'figure-246-3446.jpg',\n",
              " 'figure-168-2335.jpg',\n",
              " 'figure-124-1570.jpg',\n",
              " 'figure-304-4225.jpg',\n",
              " 'figure-107-1286.jpg',\n",
              " 'figure-127-1631.jpg',\n",
              " 'figure-67-490.jpg',\n",
              " 'figure-183-2536.jpg',\n",
              " 'figure-44-94.jpg',\n",
              " 'figure-247-3486.jpg',\n",
              " 'figure-152-1972.jpg',\n",
              " 'figure-96-1047.jpg',\n",
              " 'figure-196-2735.jpg',\n",
              " 'figure-238-3299.jpg',\n",
              " 'figure-230-3148.jpg',\n",
              " 'figure-288-4042.jpg',\n",
              " 'figure-92-900.jpg',\n",
              " 'figure-283-3913.jpg',\n",
              " 'figure-192-2689.jpg',\n",
              " 'figure-159-2054.jpg',\n",
              " 'figure-113-1396.jpg',\n",
              " 'figure-182-2529.jpg',\n",
              " 'figure-293-4113.jpg',\n",
              " 'figure-271-3780.jpg',\n",
              " 'figure-166-2273.jpg',\n",
              " 'figure-280-3877.jpg',\n",
              " 'figure-270-3762.jpg',\n",
              " 'figure-89-867.jpg',\n",
              " 'figure-143-1921.jpg',\n",
              " 'figure-62-401.jpg',\n",
              " 'figure-244-3412.jpg',\n",
              " 'figure-58-271.jpg',\n",
              " 'figure-59-309.jpg',\n",
              " 'figure-261-3658.jpg',\n",
              " 'figure-241-3389.jpg',\n",
              " 'figure-164-2228.jpg',\n",
              " 'figure-115-1426.jpg',\n",
              " 'figure-77-653.jpg',\n",
              " 'figure-271-3786.jpg',\n",
              " 'figure-98-1096.jpg',\n",
              " 'figure-286-4016.jpg',\n",
              " 'figure-296-4162.jpg',\n",
              " 'figure-270-3765.jpg',\n",
              " 'figure-82-712.jpg',\n",
              " 'figure-239-3315.jpg',\n",
              " 'figure-168-2366.jpg',\n",
              " 'figure-93-937.jpg',\n",
              " 'figure-62-407.jpg',\n",
              " 'figure-174-2448.jpg',\n",
              " 'figure-65-450.jpg',\n",
              " 'figure-50-159.jpg',\n",
              " 'figure-97-1081.jpg',\n",
              " 'figure-169-2376.jpg',\n",
              " 'figure-327-4564.jpg',\n",
              " 'figure-269-3740.jpg',\n",
              " 'figure-239-3317.jpg',\n",
              " 'figure-262-3687.jpg',\n",
              " 'figure-70-544.jpg',\n",
              " 'figure-252-3549.jpg',\n",
              " 'figure-138-1806.jpg',\n",
              " 'figure-304-4229.jpg',\n",
              " 'figure-236-3241.jpg',\n",
              " 'figure-41-46.jpg',\n",
              " 'figure-243-3411.jpg',\n",
              " 'figure-208-2887.jpg',\n",
              " 'figure-163-2192.jpg',\n",
              " 'figure-83-745.jpg',\n",
              " 'figure-141-1870.jpg',\n",
              " 'figure-71-572.jpg',\n",
              " 'figure-61-387.jpg',\n",
              " 'figure-110-1351.jpg',\n",
              " 'figure-121-1466.jpg',\n",
              " 'figure-240-3343.jpg',\n",
              " 'figure-233-3159.jpg',\n",
              " 'figure-142-1903.jpg',\n",
              " 'figure-86-791.jpg',\n",
              " 'figure-152-1977.jpg',\n",
              " 'figure-119-1435.jpg',\n",
              " 'figure-304-4223.jpg',\n",
              " 'figure-203-2821.jpg',\n",
              " 'figure-203-2817.jpg',\n",
              " 'figure-316-4410.jpg',\n",
              " 'figure-112-1381.jpg',\n",
              " 'figure-309-4303.jpg',\n",
              " 'figure-270-3756.jpg',\n",
              " 'figure-95-970.jpg',\n",
              " 'figure-245-3425.jpg',\n",
              " 'figure-135-1702.jpg',\n",
              " 'figure-140-1837.jpg',\n",
              " 'figure-285-3976.jpg',\n",
              " 'figure-218-3082.jpg',\n",
              " 'figure-184-2555.jpg',\n",
              " 'figure-58-276.jpg',\n",
              " 'figure-96-1033.jpg',\n",
              " 'figure-77-660.jpg',\n",
              " 'figure-43-75.jpg',\n",
              " 'figure-92-909.jpg',\n",
              " 'figure-269-3732.jpg',\n",
              " 'figure-260-3646.jpg',\n",
              " 'figure-102-1196.jpg',\n",
              " 'figure-62-388.jpg',\n",
              " 'figure-106-1276.jpg',\n",
              " 'figure-103-1229.jpg',\n",
              " 'figure-85-783.jpg',\n",
              " 'figure-322-4477.jpg',\n",
              " 'figure-82-724.jpg',\n",
              " 'figure-238-3305.jpg',\n",
              " 'figure-119-1437.jpg',\n",
              " 'figure-98-1111.jpg',\n",
              " 'figure-305-4245.jpg',\n",
              " 'figure-134-1693.jpg',\n",
              " 'figure-194-2715.jpg',\n",
              " 'figure-140-1835.jpg',\n",
              " 'figure-315-4397.jpg',\n",
              " 'figure-285-3940.jpg',\n",
              " 'figure-97-1079.jpg',\n",
              " 'figure-70-546.jpg',\n",
              " 'figure-119-1432.jpg',\n",
              " 'figure-139-1811.jpg',\n",
              " 'figure-141-1876.jpg',\n",
              " 'figure-96-1022.jpg',\n",
              " 'figure-114-1408.jpg',\n",
              " 'figure-257-3596.jpg',\n",
              " 'figure-97-1074.jpg',\n",
              " 'figure-194-2726.jpg',\n",
              " 'figure-159-2064.jpg',\n",
              " 'figure-239-3318.jpg',\n",
              " 'figure-293-4111.jpg',\n",
              " 'figure-107-1292.jpg',\n",
              " 'figure-284-3932.jpg',\n",
              " 'figure-107-1299.jpg',\n",
              " 'figure-189-2649.jpg',\n",
              " 'figure-69-529.jpg',\n",
              " 'figure-59-303.jpg',\n",
              " 'figure-127-1636.jpg',\n",
              " 'figure-96-1018.jpg',\n",
              " 'figure-315-4401.jpg',\n",
              " 'figure-127-1632.jpg',\n",
              " 'figure-162-2165.jpg',\n",
              " 'figure-161-2145.jpg',\n",
              " 'figure-71-555.jpg',\n",
              " 'figure-136-1736.jpg',\n",
              " 'figure-315-4395.jpg',\n",
              " 'figure-235-3226.jpg',\n",
              " 'figure-85-776.jpg',\n",
              " 'figure-209-2894.jpg',\n",
              " 'figure-326-4553.jpg',\n",
              " 'figure-124-1544.jpg',\n",
              " 'figure-249-3511.jpg',\n",
              " 'figure-198-2751.jpg',\n",
              " 'figure-169-2371.jpg',\n",
              " 'figure-314-4383.jpg',\n",
              " 'figure-143-1919.jpg',\n",
              " 'figure-316-4415.jpg',\n",
              " 'figure-106-1270.jpg',\n",
              " 'figure-263-3697.jpg',\n",
              " 'figure-322-4484.jpg',\n",
              " 'figure-194-2705.jpg',\n",
              " 'figure-175-2469.jpg',\n",
              " 'figure-113-1394.jpg',\n",
              " 'figure-291-4072.jpg',\n",
              " 'figure-233-3176.jpg',\n",
              " 'figure-60-334.jpg',\n",
              " 'figure-169-2395.jpg',\n",
              " 'figure-323-4498.jpg',\n",
              " 'figure-237-3275.jpg',\n",
              " 'figure-326-4556.jpg',\n",
              " 'figure-285-3948.jpg',\n",
              " 'figure-313-4359.jpg',\n",
              " 'figure-288-4053.jpg',\n",
              " 'figure-160-2106.jpg',\n",
              " 'figure-135-1704.jpg',\n",
              " 'figure-250-3523.jpg',\n",
              " 'figure-139-1816.jpg',\n",
              " 'figure-110-1362.jpg',\n",
              " 'figure-184-2542.jpg',\n",
              " 'figure-141-1890.jpg',\n",
              " 'figure-201-2795.jpg',\n",
              " 'figure-97-1077.jpg',\n",
              " 'figure-184-2570.jpg',\n",
              " 'figure-284-3929.jpg',\n",
              " 'figure-101-1177.jpg',\n",
              " 'figure-242-3392.jpg',\n",
              " 'figure-89-861.jpg',\n",
              " 'figure-169-2387.jpg',\n",
              " 'figure-126-1610.jpg',\n",
              " 'figure-249-3510.jpg',\n",
              " 'figure-142-1907.jpg',\n",
              " 'figure-283-3915.jpg',\n",
              " 'figure-244-3423.jpg',\n",
              " 'figure-263-3692.jpg',\n",
              " 'figure-108-1318.jpg',\n",
              " 'figure-235-3221.jpg',\n",
              " 'figure-163-2198.jpg',\n",
              " 'figure-298-4197.jpg',\n",
              " 'figure-324-4517.jpg',\n",
              " 'figure-49-150.jpg',\n",
              " 'figure-224-3122.jpg',\n",
              " 'figure-194-2727.jpg',\n",
              " 'figure-60-345.jpg',\n",
              " 'figure-57-255.jpg',\n",
              " 'figure-74-588.jpg',\n",
              " 'figure-299-4213.jpg',\n",
              " 'figure-233-3170.jpg',\n",
              " 'figure-184-2569.jpg',\n",
              " 'figure-41-57.jpg',\n",
              " 'figure-60-332.jpg',\n",
              " 'figure-105-1251.jpg',\n",
              " 'figure-95-972.jpg',\n",
              " 'figure-122-1476.jpg',\n",
              " 'figure-247-3456.jpg',\n",
              " 'figure-142-1902.jpg',\n",
              " 'figure-248-3492.jpg',\n",
              " 'figure-241-3375.jpg',\n",
              " 'figure-128-1657.jpg',\n",
              " 'figure-263-3713.jpg',\n",
              " 'figure-222-3101.jpg',\n",
              " 'figure-74-586.jpg',\n",
              " 'figure-272-3815.jpg',\n",
              " 'figure-114-1409.jpg',\n",
              " 'figure-115-1425.jpg',\n",
              " 'figure-98-1089.jpg',\n",
              " 'figure-143-1920.jpg',\n",
              " 'figure-111-1370.jpg',\n",
              " 'figure-85-770.jpg',\n",
              " 'figure-60-318.jpg',\n",
              " 'figure-69-511.jpg',\n",
              " 'figure-236-3250.jpg',\n",
              " 'figure-297-4174.jpg',\n",
              " 'figure-86-797.jpg',\n",
              " 'figure-61-367.jpg',\n",
              " 'figure-104-1243.jpg',\n",
              " 'figure-165-2241.jpg',\n",
              " 'figure-270-3763.jpg',\n",
              " 'figure-58-296.jpg',\n",
              " 'figure-59-314.jpg',\n",
              " 'figure-258-3630.jpg',\n",
              " 'figure-128-1655.jpg',\n",
              " 'figure-101-1187.jpg',\n",
              " 'figure-160-2102.jpg',\n",
              " 'figure-285-3960.jpg',\n",
              " 'figure-105-1253.jpg',\n",
              " 'figure-181-2513.jpg',\n",
              " 'figure-77-644.jpg',\n",
              " 'figure-126-1618.jpg',\n",
              " 'figure-138-1799.jpg',\n",
              " 'figure-169-2377.jpg',\n",
              " 'figure-95-996.jpg',\n",
              " 'figure-247-3480.jpg',\n",
              " 'figure-136-1733.jpg',\n",
              " 'figure-314-4384.jpg',\n",
              " 'figure-114-1411.jpg',\n",
              " 'figure-240-3353.jpg',\n",
              " 'figure-96-1029.jpg',\n",
              " 'figure-205-2865.jpg',\n",
              " 'figure-160-2094.jpg',\n",
              " 'figure-96-1030.jpg',\n",
              " 'figure-327-4562.jpg',\n",
              " 'figure-285-3955.jpg',\n",
              " 'figure-214-2971.jpg',\n",
              " 'figure-44-109.jpg',\n",
              " 'figure-202-2807.jpg',\n",
              " 'figure-196-2741.jpg',\n",
              " 'figure-1-1.jpg',\n",
              " 'figure-182-2528.jpg',\n",
              " 'figure-226-3130.jpg',\n",
              " 'figure-286-4000.jpg',\n",
              " 'figure-298-4185.jpg',\n",
              " 'figure-66-480.jpg',\n",
              " 'figure-296-4169.jpg',\n",
              " 'figure-177-2486.jpg',\n",
              " 'figure-103-1224.jpg',\n",
              " 'figure-252-3544.jpg',\n",
              " 'figure-44-108.jpg',\n",
              " 'figure-95-963.jpg',\n",
              " 'figure-62-415.jpg',\n",
              " 'figure-263-3702.jpg',\n",
              " 'figure-124-1541.jpg',\n",
              " 'figure-185-2584.jpg',\n",
              " 'figure-109-1333.jpg',\n",
              " 'figure-106-1275.jpg',\n",
              " 'figure-238-3282.jpg',\n",
              " 'figure-261-3657.jpg',\n",
              " 'figure-256-3587.jpg',\n",
              " 'figure-87-807.jpg',\n",
              " 'figure-293-4103.jpg',\n",
              " 'figure-96-1017.jpg',\n",
              " 'figure-225-3129.jpg',\n",
              " 'figure-97-1076.jpg',\n",
              " 'figure-114-1407.jpg',\n",
              " 'figure-77-656.jpg',\n",
              " 'figure-160-2103.jpg',\n",
              " 'figure-307-4263.jpg',\n",
              " 'figure-107-1306.jpg',\n",
              " 'figure-287-4018.jpg',\n",
              " 'figure-210-2901.jpg',\n",
              " 'figure-167-2322.jpg',\n",
              " 'figure-285-3972.jpg',\n",
              " 'figure-281-3888.jpg',\n",
              " 'figure-199-2770.jpg',\n",
              " 'figure-158-2040.jpg',\n",
              " 'figure-123-1520.jpg',\n",
              " 'figure-228-3133.jpg',\n",
              " 'figure-25-14.jpg',\n",
              " 'figure-269-3746.jpg',\n",
              " 'figure-286-3995.jpg',\n",
              " 'figure-285-3962.jpg',\n",
              " 'figure-23-9.jpg',\n",
              " 'figure-159-2055.jpg',\n",
              " 'figure-254-3568.jpg',\n",
              " 'figure-233-3168.jpg',\n",
              " 'figure-138-1788.jpg',\n",
              " 'figure-305-4253.jpg',\n",
              " 'figure-153-1992.jpg',\n",
              " 'figure-154-2004.jpg',\n",
              " 'figure-48-137.jpg',\n",
              " 'figure-136-1724.jpg',\n",
              " 'figure-143-1914.jpg',\n",
              " 'figure-66-478.jpg',\n",
              " 'figure-107-1289.jpg',\n",
              " 'figure-76-622.jpg',\n",
              " 'figure-240-3332.jpg',\n",
              " 'figure-136-1712.jpg',\n",
              " 'figure-62-398.jpg',\n",
              " 'figure-137-1764.jpg',\n",
              " 'figure-205-2867.jpg',\n",
              " 'figure-136-1726.jpg',\n",
              " 'figure-318-4455.jpg',\n",
              " 'figure-96-1044.jpg',\n",
              " 'figure-129-1664.jpg',\n",
              " 'figure-223-3105.jpg',\n",
              " 'figure-262-3682.jpg',\n",
              " 'figure-86-798.jpg',\n",
              " 'figure-59-304.jpg',\n",
              " 'figure-215-3004.jpg',\n",
              " 'figure-43-81.jpg',\n",
              " 'figure-41-51.jpg',\n",
              " 'figure-34-28.jpg',\n",
              " 'figure-71-563.jpg',\n",
              " 'figure-71-568.jpg',\n",
              " 'figure-61-378.jpg',\n",
              " 'figure-198-2752.jpg',\n",
              " 'figure-66-468.jpg',\n",
              " 'figure-92-923.jpg',\n",
              " 'figure-185-2582.jpg',\n",
              " 'figure-198-2756.jpg',\n",
              " 'figure-115-1415.jpg',\n",
              " 'figure-174-2454.jpg',\n",
              " 'figure-185-2578.jpg',\n",
              " 'figure-94-953.jpg',\n",
              " 'figure-134-1696.jpg',\n",
              " 'figure-96-1035.jpg',\n",
              " 'figure-269-3745.jpg',\n",
              " 'figure-324-4511.jpg',\n",
              " 'figure-163-2206.jpg',\n",
              " 'figure-194-2717.jpg',\n",
              " 'figure-295-4147.jpg',\n",
              " 'figure-155-2011.jpg',\n",
              " 'figure-165-2231.jpg',\n",
              " 'figure-152-1979.jpg',\n",
              " 'figure-91-893.jpg',\n",
              " 'figure-248-3498.jpg',\n",
              " 'figure-97-1055.jpg',\n",
              " 'figure-228-3134.jpg',\n",
              " 'figure-180-2503.jpg',\n",
              " 'figure-276-3869.jpg',\n",
              " 'figure-161-2141.jpg',\n",
              " 'figure-114-1405.jpg',\n",
              " 'figure-249-3514.jpg',\n",
              " 'figure-137-1779.jpg',\n",
              " 'figure-124-1569.jpg',\n",
              " 'figure-271-3776.jpg',\n",
              " 'figure-238-3303.jpg',\n",
              " 'figure-215-3017.jpg',\n",
              " 'figure-237-3265.jpg',\n",
              " 'figure-296-4170.jpg',\n",
              " 'figure-257-3597.jpg',\n",
              " 'figure-246-3439.jpg',\n",
              " 'figure-159-2063.jpg',\n",
              " 'figure-184-2564.jpg',\n",
              " 'figure-240-3362.jpg',\n",
              " 'figure-83-747.jpg',\n",
              " 'figure-127-1638.jpg',\n",
              " 'figure-92-904.jpg',\n",
              " 'figure-288-4045.jpg',\n",
              " 'figure-124-1553.jpg',\n",
              " 'figure-115-1422.jpg',\n",
              " 'figure-137-1774.jpg',\n",
              " 'figure-319-4464.jpg',\n",
              " 'figure-44-91.jpg',\n",
              " 'figure-215-2994.jpg',\n",
              " 'figure-253-3560.jpg',\n",
              " 'figure-309-4304.jpg',\n",
              " 'figure-169-2403.jpg',\n",
              " 'figure-199-2771.jpg',\n",
              " 'figure-204-2837.jpg',\n",
              " 'figure-161-2151.jpg',\n",
              " 'figure-108-1315.jpg',\n",
              " 'figure-92-926.jpg',\n",
              " 'figure-152-1968.jpg',\n",
              " 'figure-313-4354.jpg',\n",
              " 'figure-271-3783.jpg',\n",
              " 'figure-183-2532.jpg',\n",
              " 'figure-76-628.jpg',\n",
              " 'figure-98-1103.jpg',\n",
              " 'figure-223-3104.jpg',\n",
              " 'figure-161-2146.jpg',\n",
              " 'figure-54-183.jpg',\n",
              " 'figure-230-3146.jpg',\n",
              " 'figure-313-4365.jpg',\n",
              " 'figure-131-1689.jpg',\n",
              " 'figure-108-1319.jpg',\n",
              " 'figure-314-4385.jpg',\n",
              " 'figure-105-1249.jpg',\n",
              " 'figure-191-2664.jpg',\n",
              " 'figure-166-2280.jpg',\n",
              " 'figure-317-4422.jpg',\n",
              " 'figure-306-4257.jpg',\n",
              " 'figure-314-4379.jpg',\n",
              " 'figure-175-2474.jpg',\n",
              " 'figure-246-3445.jpg',\n",
              " 'figure-293-4100.jpg',\n",
              " 'figure-123-1514.jpg',\n",
              " 'figure-110-1349.jpg',\n",
              " 'figure-325-4543.jpg',\n",
              " 'figure-138-1802.jpg',\n",
              " 'figure-94-948.jpg',\n",
              " 'figure-162-2155.jpg',\n",
              " 'figure-95-993.jpg',\n",
              " 'figure-173-2423.jpg',\n",
              " 'figure-63-438.jpg',\n",
              " 'figure-135-1698.jpg',\n",
              " 'figure-98-1085.jpg',\n",
              " 'figure-163-2186.jpg',\n",
              " 'figure-313-4345.jpg',\n",
              " 'figure-245-3436.jpg',\n",
              " 'figure-167-2301.jpg',\n",
              " 'figure-244-3415.jpg',\n",
              " 'figure-142-1912.jpg',\n",
              " 'figure-248-3488.jpg',\n",
              " 'figure-126-1625.jpg',\n",
              " 'figure-204-2853.jpg',\n",
              " 'figure-270-3761.jpg',\n",
              " 'figure-136-1730.jpg',\n",
              " 'figure-163-2218.jpg',\n",
              " 'figure-47-131.jpg',\n",
              " 'figure-74-591.jpg',\n",
              " 'figure-316-4406.jpg',\n",
              " 'figure-248-3497.jpg',\n",
              " 'figure-124-1572.jpg',\n",
              " 'figure-234-3198.jpg',\n",
              " 'figure-158-2044.jpg',\n",
              " 'figure-48-143.jpg',\n",
              " 'figure-189-2646.jpg',\n",
              " 'figure-71-571.jpg',\n",
              " 'figure-217-3069.jpg',\n",
              " 'figure-296-4156.jpg',\n",
              " 'figure-189-2655.jpg',\n",
              " 'figure-288-4038.jpg',\n",
              " 'figure-323-4500.jpg',\n",
              " 'figure-125-1586.jpg',\n",
              " 'figure-209-2891.jpg',\n",
              " 'figure-187-2603.jpg',\n",
              " 'figure-109-1334.jpg',\n",
              " 'figure-263-3695.jpg',\n",
              " 'figure-271-3801.jpg',\n",
              " 'figure-270-3759.jpg',\n",
              " 'figure-168-2347.jpg',\n",
              " 'figure-88-840.jpg',\n",
              " 'figure-135-1700.jpg',\n",
              " 'figure-286-4003.jpg',\n",
              " 'figure-255-3573.jpg',\n",
              " 'figure-165-2254.jpg',\n",
              " 'figure-70-541.jpg',\n",
              " 'figure-136-1740.jpg',\n",
              " 'figure-235-3220.jpg',\n",
              " 'figure-69-532.jpg',\n",
              " 'figure-50-158.jpg',\n",
              " 'figure-206-2869.jpg',\n",
              " 'figure-114-1410.jpg',\n",
              " 'figure-165-2247.jpg',\n",
              " 'figure-96-1005.jpg',\n",
              " 'figure-293-4102.jpg',\n",
              " 'figure-135-1710.jpg',\n",
              " 'figure-109-1336.jpg',\n",
              " 'figure-245-3438.jpg',\n",
              " 'figure-318-4445.jpg',\n",
              " 'figure-98-1088.jpg',\n",
              " 'figure-275-3830.jpg',\n",
              " 'figure-214-2970.jpg',\n",
              " 'figure-325-4547.jpg',\n",
              " 'figure-124-1550.jpg',\n",
              " 'figure-168-2328.jpg',\n",
              " 'figure-154-2002.jpg',\n",
              " 'figure-86-792.jpg',\n",
              " 'figure-173-2427.jpg',\n",
              " 'figure-122-1483.jpg',\n",
              " 'figure-110-1358.jpg',\n",
              " 'figure-169-2409.jpg',\n",
              " 'figure-241-3382.jpg',\n",
              " 'figure-107-1290.jpg',\n",
              " 'figure-80-695.jpg',\n",
              " 'figure-126-1594.jpg',\n",
              " 'figure-95-978.jpg',\n",
              " 'figure-83-741.jpg',\n",
              " 'figure-249-3512.jpg',\n",
              " 'figure-66-487.jpg',\n",
              " 'figure-62-391.jpg',\n",
              " 'figure-292-4082.jpg',\n",
              " 'figure-318-4442.jpg',\n",
              " 'figure-247-3455.jpg',\n",
              " 'figure-256-3585.jpg',\n",
              " 'figure-215-2985.jpg',\n",
              " 'figure-77-657.jpg',\n",
              " 'figure-124-1535.jpg',\n",
              " 'figure-81-704.jpg',\n",
              " 'figure-119-1442.jpg',\n",
              " 'figure-249-3513.jpg',\n",
              " 'figure-168-2359.jpg',\n",
              " 'figure-124-1552.jpg',\n",
              " 'figure-55-215.jpg',\n",
              " 'figure-257-3603.jpg',\n",
              " 'figure-71-557.jpg',\n",
              " 'figure-307-4269.jpg',\n",
              " 'figure-223-3106.jpg',\n",
              " 'figure-240-3357.jpg',\n",
              " 'figure-83-743.jpg',\n",
              " 'figure-74-594.jpg',\n",
              " 'figure-61-375.jpg',\n",
              " 'figure-272-3811.jpg',\n",
              " 'figure-63-420.jpg',\n",
              " 'figure-315-4391.jpg',\n",
              " 'figure-87-815.jpg',\n",
              " 'figure-323-4497.jpg',\n",
              " 'figure-193-2693.jpg',\n",
              " 'figure-120-1455.jpg',\n",
              " 'figure-293-4110.jpg',\n",
              " 'figure-246-3441.jpg',\n",
              " 'figure-58-274.jpg',\n",
              " 'figure-46-122.jpg',\n",
              " 'figure-216-3057.jpg',\n",
              " 'figure-285-3961.jpg',\n",
              " 'figure-56-235.jpg',\n",
              " 'figure-138-1804.jpg',\n",
              " 'figure-140-1839.jpg',\n",
              " 'figure-59-300.jpg',\n",
              " 'figure-57-252.jpg',\n",
              " 'figure-115-1428.jpg',\n",
              " 'figure-174-2461.jpg',\n",
              " 'figure-317-4427.jpg',\n",
              " 'figure-212-2929.jpg',\n",
              " 'figure-42-69.jpg',\n",
              " 'figure-63-423.jpg',\n",
              " 'figure-126-1592.jpg',\n",
              " 'figure-57-256.jpg',\n",
              " 'figure-207-2876.jpg',\n",
              " 'figure-220-3091.jpg',\n",
              " 'figure-83-736.jpg',\n",
              " 'figure-103-1228.jpg',\n",
              " 'figure-291-4067.jpg',\n",
              " 'figure-82-727.jpg',\n",
              " 'figure-58-286.jpg',\n",
              " 'figure-139-1815.jpg',\n",
              " 'figure-184-2543.jpg',\n",
              " 'figure-109-1328.jpg',\n",
              " 'figure-286-3991.jpg',\n",
              " 'figure-105-1248.jpg',\n",
              " 'figure-307-4262.jpg',\n",
              " 'figure-81-708.jpg',\n",
              " 'figure-207-2880.jpg',\n",
              " 'figure-139-1818.jpg',\n",
              " 'figure-138-1795.jpg',\n",
              " 'figure-277-3874.jpg',\n",
              " 'figure-79-678.jpg',\n",
              " 'figure-299-4217.jpg',\n",
              " 'figure-249-3499.jpg',\n",
              " 'figure-75-605.jpg',\n",
              " 'figure-304-4232.jpg',\n",
              " 'figure-126-1622.jpg',\n",
              " 'figure-47-130.jpg',\n",
              " 'figure-166-2264.jpg',\n",
              " 'figure-123-1509.jpg',\n",
              " 'figure-285-3957.jpg',\n",
              " 'figure-250-3524.jpg',\n",
              " 'figure-210-2912.jpg',\n",
              " 'figure-126-1614.jpg',\n",
              " 'figure-313-4351.jpg',\n",
              " 'figure-234-3211.jpg',\n",
              " 'figure-234-3201.jpg',\n",
              " 'figure-99-1138.jpg',\n",
              " 'figure-203-2818.jpg',\n",
              " 'figure-158-2029.jpg',\n",
              " 'figure-87-822.jpg',\n",
              " 'figure-106-1282.jpg',\n",
              " 'figure-125-1585.jpg',\n",
              " 'figure-60-324.jpg',\n",
              " 'figure-247-3477.jpg',\n",
              " 'figure-165-2251.jpg',\n",
              " 'figure-97-1052.jpg',\n",
              " 'figure-85-767.jpg',\n",
              " 'figure-296-4159.jpg',\n",
              " 'figure-161-2134.jpg',\n",
              " 'figure-194-2716.jpg',\n",
              " 'figure-122-1478.jpg',\n",
              " 'figure-262-3677.jpg',\n",
              " 'figure-139-1809.jpg',\n",
              " 'figure-137-1763.jpg',\n",
              " 'figure-252-3547.jpg',\n",
              " 'figure-112-1388.jpg',\n",
              " 'figure-324-4520.jpg',\n",
              " 'figure-75-602.jpg',\n",
              " 'figure-167-2300.jpg',\n",
              " 'figure-188-2634.jpg',\n",
              " 'figure-102-1209.jpg',\n",
              " 'figure-96-1040.jpg',\n",
              " 'figure-325-4535.jpg',\n",
              " 'figure-104-1232.jpg',\n",
              " 'figure-238-3283.jpg',\n",
              " 'figure-244-3417.jpg',\n",
              " 'figure-141-1865.jpg',\n",
              " 'figure-168-2344.jpg',\n",
              " 'figure-216-3040.jpg',\n",
              " 'figure-101-1182.jpg',\n",
              " 'figure-126-1597.jpg',\n",
              " 'figure-4-3.jpg',\n",
              " 'figure-218-3083.jpg',\n",
              " 'figure-235-3227.jpg',\n",
              " 'figure-173-2440.jpg',\n",
              " 'figure-276-3860.jpg',\n",
              " 'figure-136-1728.jpg',\n",
              " 'figure-123-1507.jpg',\n",
              " 'figure-284-3925.jpg',\n",
              " 'figure-204-2859.jpg',\n",
              " 'figure-254-3570.jpg',\n",
              " 'figure-106-1281.jpg',\n",
              " 'figure-54-190.jpg',\n",
              " 'figure-281-3887.jpg',\n",
              " 'figure-240-3354.jpg',\n",
              " 'figure-153-1990.jpg',\n",
              " 'figure-141-1868.jpg',\n",
              " 'figure-208-2886.jpg',\n",
              " 'figure-204-2836.jpg',\n",
              " 'figure-165-2250.jpg',\n",
              " 'figure-269-3739.jpg',\n",
              " 'figure-46-118.jpg',\n",
              " 'figure-272-3810.jpg',\n",
              " 'figure-309-4306.jpg',\n",
              " 'figure-130-1670.jpg',\n",
              " 'figure-127-1637.jpg',\n",
              " 'figure-66-465.jpg',\n",
              " 'figure-189-2653.jpg',\n",
              " 'figure-250-3532.jpg',\n",
              " 'figure-252-3545.jpg',\n",
              " 'figure-252-3546.jpg',\n",
              " 'figure-119-1434.jpg',\n",
              " 'figure-285-3978.jpg',\n",
              " 'figure-127-1633.jpg',\n",
              " 'figure-55-214.jpg',\n",
              " 'figure-241-3368.jpg',\n",
              " 'figure-122-1501.jpg',\n",
              " 'figure-285-3953.jpg',\n",
              " 'figure-50-163.jpg',\n",
              " 'figure-89-846.jpg',\n",
              " 'figure-216-3044.jpg',\n",
              " 'figure-283-3910.jpg',\n",
              " 'figure-65-452.jpg',\n",
              " 'figure-95-1000.jpg',\n",
              " 'figure-287-4028.jpg',\n",
              " 'figure-187-2608.jpg',\n",
              " 'figure-90-875.jpg',\n",
              " 'figure-245-3437.jpg',\n",
              " 'figure-138-1798.jpg',\n",
              " 'figure-198-2749.jpg',\n",
              " 'figure-42-68.jpg',\n",
              " 'figure-90-882.jpg',\n",
              " 'figure-161-2131.jpg',\n",
              " 'figure-316-4409.jpg',\n",
              " 'figure-41-50.jpg',\n",
              " 'figure-233-3161.jpg',\n",
              " 'figure-138-1787.jpg',\n",
              " 'figure-315-4389.jpg',\n",
              " 'figure-271-3800.jpg',\n",
              " 'figure-41-55.jpg',\n",
              " 'figure-162-2158.jpg',\n",
              " 'figure-142-1911.jpg',\n",
              " 'figure-104-1239.jpg',\n",
              " 'figure-248-3495.jpg',\n",
              " 'figure-128-1656.jpg',\n",
              " 'figure-208-2884.jpg',\n",
              " 'figure-207-2875.jpg',\n",
              " 'figure-179-2500.jpg',\n",
              " 'figure-140-1830.jpg',\n",
              " 'figure-137-1776.jpg',\n",
              " 'figure-111-1366.jpg',\n",
              " 'figure-239-3309.jpg',\n",
              " 'figure-158-2038.jpg',\n",
              " 'figure-237-3266.jpg',\n",
              " 'figure-99-1136.jpg',\n",
              " 'figure-305-4254.jpg',\n",
              " 'figure-105-1260.jpg',\n",
              " 'figure-318-4450.jpg',\n",
              " 'figure-237-3255.jpg',\n",
              " 'figure-214-2967.jpg',\n",
              " 'figure-108-1309.jpg',\n",
              " 'figure-321-4476.jpg',\n",
              " 'figure-311-4328.jpg',\n",
              " 'figure-201-2802.jpg',\n",
              " 'figure-271-3787.jpg',\n",
              " 'figure-287-4029.jpg',\n",
              " 'figure-184-2567.jpg',\n",
              " 'figure-140-1854.jpg',\n",
              " 'figure-245-3433.jpg',\n",
              " 'figure-242-3391.jpg',\n",
              " 'figure-215-3026.jpg',\n",
              " 'figure-288-4050.jpg',\n",
              " 'figure-237-3274.jpg',\n",
              " 'figure-169-2397.jpg',\n",
              " 'figure-223-3109.jpg',\n",
              " 'figure-242-3394.jpg',\n",
              " 'figure-257-3601.jpg',\n",
              " 'figure-245-3435.jpg',\n",
              " 'figure-85-769.jpg',\n",
              " 'figure-98-1091.jpg',\n",
              " 'figure-212-2921.jpg',\n",
              " 'figure-81-699.jpg',\n",
              " 'figure-136-1713.jpg',\n",
              " 'figure-224-3118.jpg',\n",
              " 'figure-169-2407.jpg',\n",
              " 'figure-285-3939.jpg',\n",
              " 'figure-304-4235.jpg',\n",
              " 'figure-138-1792.jpg',\n",
              " 'figure-307-4272.jpg',\n",
              " 'figure-139-1821.jpg',\n",
              " 'figure-163-2187.jpg',\n",
              " 'figure-325-4533.jpg',\n",
              " 'figure-56-225.jpg',\n",
              " 'figure-324-4508.jpg',\n",
              " 'figure-103-1223.jpg',\n",
              " 'figure-276-3853.jpg',\n",
              " 'figure-79-681.jpg',\n",
              " 'figure-323-4486.jpg',\n",
              " 'figure-323-4490.jpg',\n",
              " 'figure-292-4080.jpg',\n",
              " 'figure-161-2148.jpg',\n",
              " 'figure-312-4338.jpg',\n",
              " 'figure-105-1255.jpg',\n",
              " 'figure-263-3691.jpg',\n",
              " 'figure-55-208.jpg',\n",
              " 'figure-159-2086.jpg',\n",
              " 'figure-169-2391.jpg',\n",
              " 'figure-160-2112.jpg',\n",
              " 'figure-48-133.jpg',\n",
              " 'figure-278-3875.jpg',\n",
              " 'figure-204-2854.jpg',\n",
              " 'figure-285-3985.jpg',\n",
              " 'figure-56-232.jpg',\n",
              " 'figure-136-1714.jpg',\n",
              " 'figure-196-2736.jpg',\n",
              " 'figure-154-1998.jpg',\n",
              " 'figure-217-3074.jpg',\n",
              " 'figure-57-266.jpg',\n",
              " 'figure-96-1020.jpg',\n",
              " 'figure-112-1378.jpg',\n",
              " 'figure-40-41.jpg',\n",
              " 'figure-93-935.jpg',\n",
              " 'figure-137-1775.jpg',\n",
              " 'figure-246-3443.jpg',\n",
              " 'figure-90-885.jpg',\n",
              " 'figure-318-4446.jpg',\n",
              " 'figure-72-580.jpg',\n",
              " 'figure-213-2952.jpg',\n",
              " 'figure-212-2920.jpg',\n",
              " 'figure-63-436.jpg',\n",
              " 'figure-34-27.jpg',\n",
              " 'figure-95-999.jpg',\n",
              " 'figure-213-2954.jpg',\n",
              " 'figure-162-2179.jpg',\n",
              " 'figure-307-4274.jpg',\n",
              " 'figure-248-3487.jpg',\n",
              " 'figure-163-2202.jpg',\n",
              " 'figure-167-2325.jpg',\n",
              " 'figure-262-3671.jpg',\n",
              " 'figure-84-754.jpg',\n",
              " 'figure-292-4096.jpg',\n",
              " 'figure-98-1107.jpg',\n",
              " 'figure-169-2411.jpg',\n",
              " 'figure-262-3685.jpg',\n",
              " 'figure-64-447.jpg',\n",
              " 'figure-143-1928.jpg',\n",
              " 'figure-158-2041.jpg',\n",
              " 'figure-98-1100.jpg',\n",
              " 'figure-137-1757.jpg',\n",
              " 'figure-100-1151.jpg',\n",
              " 'figure-313-4367.jpg',\n",
              " 'figure-58-268.jpg',\n",
              " 'figure-62-409.jpg',\n",
              " 'figure-158-2027.jpg',\n",
              " 'figure-98-1115.jpg',\n",
              " 'figure-66-470.jpg',\n",
              " 'figure-142-1909.jpg',\n",
              " 'figure-216-3043.jpg',\n",
              " 'figure-54-184.jpg',\n",
              " 'figure-308-4295.jpg',\n",
              " 'figure-60-320.jpg',\n",
              " 'figure-294-4129.jpg',\n",
              " 'figure-324-4509.jpg',\n",
              " 'figure-126-1604.jpg',\n",
              " 'figure-152-1970.jpg',\n",
              " 'figure-162-2168.jpg',\n",
              " 'figure-146-1959.jpg',\n",
              " 'figure-87-809.jpg',\n",
              " 'figure-223-3113.jpg',\n",
              " 'figure-286-3996.jpg',\n",
              " 'figure-143-1924.jpg',\n",
              " 'figure-221-3093.jpg',\n",
              " 'figure-201-2790.jpg',\n",
              " 'figure-262-3676.jpg',\n",
              " 'figure-324-4515.jpg',\n",
              " 'figure-240-3350.jpg',\n",
              " 'figure-69-526.jpg',\n",
              " 'figure-53-174.jpg',\n",
              " 'figure-191-2666.jpg',\n",
              " 'figure-162-2181.jpg',\n",
              " 'figure-181-2512.jpg',\n",
              " 'figure-324-4514.jpg',\n",
              " 'figure-165-2256.jpg',\n",
              " 'figure-317-4432.jpg',\n",
              " 'figure-236-3251.jpg',\n",
              " 'figure-262-3686.jpg',\n",
              " 'figure-59-317.jpg',\n",
              " 'figure-109-1343.jpg',\n",
              " 'figure-217-3076.jpg',\n",
              " 'figure-128-1651.jpg',\n",
              " 'figure-324-4501.jpg',\n",
              " 'figure-184-2563.jpg',\n",
              " 'figure-161-2147.jpg',\n",
              " 'figure-262-3678.jpg',\n",
              " 'figure-215-3018.jpg',\n",
              " 'figure-164-2227.jpg',\n",
              " 'figure-317-4420.jpg',\n",
              " 'figure-316-4405.jpg',\n",
              " 'figure-210-2909.jpg',\n",
              " 'figure-124-1536.jpg',\n",
              " 'figure-70-538.jpg',\n",
              " 'figure-131-1687.jpg',\n",
              " 'figure-322-4485.jpg',\n",
              " 'figure-66-472.jpg',\n",
              " 'figure-59-305.jpg',\n",
              " 'figure-60-322.jpg',\n",
              " 'figure-62-395.jpg',\n",
              " 'figure-309-4301.jpg',\n",
              " 'figure-283-3917.jpg',\n",
              " 'figure-110-1346.jpg',\n",
              " 'figure-240-3342.jpg',\n",
              " 'figure-60-344.jpg',\n",
              " 'figure-68-510.jpg',\n",
              " 'figure-89-871.jpg',\n",
              " 'figure-96-1045.jpg',\n",
              " 'figure-137-1754.jpg',\n",
              " 'figure-169-2400.jpg',\n",
              " 'figure-82-729.jpg',\n",
              " 'figure-287-4021.jpg',\n",
              " 'figure-165-2235.jpg',\n",
              " 'figure-124-1558.jpg',\n",
              " 'figure-307-4279.jpg',\n",
              " 'figure-177-2484.jpg',\n",
              " 'figure-98-1118.jpg',\n",
              " 'figure-180-2504.jpg',\n",
              " 'figure-168-2343.jpg',\n",
              " 'figure-324-4519.jpg',\n",
              " 'figure-159-2051.jpg',\n",
              " 'figure-240-3346.jpg',\n",
              " 'figure-167-2304.jpg',\n",
              " 'figure-294-4121.jpg',\n",
              " 'figure-293-4114.jpg',\n",
              " 'figure-154-2003.jpg',\n",
              " 'figure-257-3602.jpg',\n",
              " 'figure-125-1578.jpg',\n",
              " 'figure-239-3316.jpg',\n",
              " 'figure-44-92.jpg',\n",
              " 'figure-158-2021.jpg',\n",
              " 'figure-173-2442.jpg',\n",
              " 'figure-271-3792.jpg',\n",
              " 'figure-97-1059.jpg',\n",
              " 'figure-68-498.jpg',\n",
              " 'figure-271-3777.jpg',\n",
              " 'figure-154-2005.jpg',\n",
              " 'figure-58-292.jpg',\n",
              " 'figure-195-2730.jpg',\n",
              " 'figure-199-2759.jpg',\n",
              " 'figure-201-2800.jpg',\n",
              " 'figure-122-1497.jpg',\n",
              " 'figure-126-1613.jpg',\n",
              " 'figure-242-3400.jpg',\n",
              " 'figure-94-961.jpg',\n",
              " 'figure-255-3571.jpg',\n",
              " 'figure-173-2431.jpg',\n",
              " 'figure-163-2209.jpg',\n",
              " 'figure-314-4387.jpg',\n",
              " 'figure-234-3205.jpg',\n",
              " 'figure-269-3736.jpg',\n",
              " 'figure-276-3844.jpg',\n",
              " 'figure-97-1069.jpg',\n",
              " 'figure-44-101.jpg',\n",
              " 'figure-263-3705.jpg',\n",
              " 'figure-137-1751.jpg',\n",
              " 'figure-184-2545.jpg',\n",
              " 'figure-98-1113.jpg',\n",
              " 'figure-83-740.jpg',\n",
              " 'figure-241-3376.jpg',\n",
              " 'figure-285-3949.jpg',\n",
              " 'figure-124-1573.jpg',\n",
              " 'figure-53-173.jpg',\n",
              " 'figure-110-1357.jpg',\n",
              " 'figure-89-842.jpg',\n",
              " 'figure-136-1739.jpg',\n",
              " 'figure-60-325.jpg',\n",
              " 'figure-212-2926.jpg',\n",
              " 'figure-119-1443.jpg',\n",
              " 'figure-214-2963.jpg',\n",
              " 'figure-166-2265.jpg',\n",
              " 'figure-304-4231.jpg',\n",
              " 'figure-240-3327.jpg',\n",
              " 'figure-98-1095.jpg',\n",
              " 'figure-96-1050.jpg',\n",
              " 'figure-165-2257.jpg',\n",
              " 'figure-67-494.jpg',\n",
              " 'figure-136-1743.jpg',\n",
              " 'figure-63-437.jpg',\n",
              " 'figure-96-1006.jpg',\n",
              " 'figure-82-721.jpg',\n",
              " 'figure-285-3950.jpg',\n",
              " 'figure-213-2955.jpg',\n",
              " 'figure-91-890.jpg',\n",
              " 'figure-169-2374.jpg',\n",
              " 'figure-240-3326.jpg',\n",
              " 'figure-163-2205.jpg',\n",
              " 'figure-318-4434.jpg',\n",
              " 'figure-207-2877.jpg',\n",
              " 'figure-224-3126.jpg',\n",
              " 'figure-142-1910.jpg',\n",
              " 'figure-90-876.jpg',\n",
              " 'figure-168-2341.jpg',\n",
              " 'figure-246-3449.jpg',\n",
              " 'figure-291-4076.jpg',\n",
              " 'figure-166-2292.jpg',\n",
              " 'figure-142-1897.jpg',\n",
              " 'figure-106-1279.jpg',\n",
              " 'figure-136-1731.jpg',\n",
              " 'figure-203-2831.jpg',\n",
              " 'figure-262-3690.jpg',\n",
              " 'figure-100-1162.jpg',\n",
              " 'figure-67-491.jpg',\n",
              " 'figure-286-4002.jpg',\n",
              " 'figure-102-1201.jpg',\n",
              " 'figure-218-3088.jpg',\n",
              " 'figure-96-1051.jpg',\n",
              " 'figure-79-677.jpg',\n",
              " 'figure-208-2882.jpg',\n",
              " 'figure-197-2744.jpg',\n",
              " 'figure-125-1582.jpg',\n",
              " 'figure-44-106.jpg',\n",
              " 'figure-75-603.jpg',\n",
              " 'figure-240-3335.jpg',\n",
              " 'figure-87-821.jpg',\n",
              " 'figure-167-2307.jpg',\n",
              " 'figure-263-3696.jpg',\n",
              " 'figure-82-717.jpg',\n",
              " 'figure-57-254.jpg',\n",
              " 'figure-313-4355.jpg',\n",
              " 'figure-296-4171.jpg',\n",
              " 'figure-174-2450.jpg',\n",
              " 'figure-166-2266.jpg',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_elements = []\n",
        "\n",
        "for image_file in os.listdir(output_path): # The images extracted from the pdf document and saved in the figures file are drawn one by one.\n",
        "    if image_file.endswith(('.png', '.jpg', '.jpeg')):\n",
        "        image_path = os.path.join(output_path, image_file) # The file path is created for files with '.png', '.jpg', '.jpeg' extensions.\n",
        "        encoded_image = encode_image(image_path) # Images in jpg format are encoded as base64 so that they can be processed by LLMs.\n",
        "        image_elements.append(encoded_image) # Images encoded as base64 are appended to the image_elements list\n",
        "print(len(image_elements))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBAZXSbjqMA6",
        "outputId": "5444b928-9bfd-4f9a-c2b6-c94301d062a0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_elements[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "cTY1is4MpyG5",
        "outputId": "e88951f2-2016-40af-9dd3-5ef0fd426c2f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCADSAAIDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDxOiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//9k='"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.path.join(output_path, \"figure-59-305.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nYaOccMzqaOS",
        "outputId": "788d5770-5401-42ad-db8a-756652af340e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/figures/figure-59-305.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import PIL.Image\n",
        "\n",
        "image = PIL.Image.open(\"/content/figures/figure-59-305.jpg\")\n",
        "image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "42hD274Zql3q",
        "outputId": "f971b1a4-47f1-47df-f847-44cf88c8cf68"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=920x183>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5gAAAC3CAIAAAD4uGaUAACJXklEQVR4Ae3dB7gmRZU+8MVlTSuKgOScc5CcsyQlSBKQIFEJCoiAiAgIKEhQQYKi5Bwk55wzSJacMwiKcXf//n8z71r7PXfmu9x7505g5vTzTE919akT3qrb9fbp6v7G+8xnPjPJJJO8/PLLE0888Z/+9KcJJpjg3Xfftf9//+///VtthUAhUAgUAoVAIVAIFAKFwOhG4CMf+QiaOuGEE4asvv3221NNNdVbb701vqrrr79+mmmm4eFf//rXT3ziEwr/+Mc/PvrRj45un8t+IVAIFAKFQCFQCBQChUAh8H/UtJHVF198cemllx7v05/+9HvvvQehv/3tb//+7//+H//xH2Gx//znPwu2QqAQKAQKgUKgECgECoFCYLQjMN5444Wg/td//df//M//fPzjH+eSZQVDMrJKjd7+93//t9NqNLCvrRAoBAqBQqAQKAQKgUKgEBjtCCCoaKqUqy3EFYkdwlYxXPvxxx+fhKWxFhXIzn7sYx8b7R6XA4VAIVAIFAKFQCFQCBQChcDf//53WViU1WJZKwgwWpigrONPN910iG0Awl8/+clPKqupjGwNmkKgECgECoFCoBAoBAqBMQGBkFVZ17/85S+f+tSncojEjv/nP//5D3/4A5LrNS8n+JrMLbY7JvhdPhQChUAhUAgUAoVAIVAIjOMIJAWLnYasWlog/YrEji9D+9nPfhY6+GvIq+StvGy97DWOj5gKvxAoBAqBQqAQKAQKgTEEAUte5WKzcABllX61IbHjt+/FYrEhrxHqXFqA80rZvv/++1hwZHLWp7wIa4gR/+d//udghcpEeLe8MVYtjRyGPVj6R5eeoBQAQaoDsjq5x5fO9IiOGV1Oftjtdo7PxOL1xjyAGJTQ6Df4/QnRZlgaqMansq50aGtvU7Y7w17s6mtbNDRVvcgPeyr+GFpGkTBpG2WDJ5cFLgmZUbELGTidETlUz8lsTvmLzs2071XnTVOBa27Lyn1qXa1yhRk23qopBAqBQqAQGGcRaGlWM0tAGDLrdYPD5OSUL3OZgbBYM6XpxyRkgjHTmHKc9bsJdDkcRBbLEBOmZBsTpjT6w2u7ufqhqId1IlIQIBbLbQX8A6pBW41gW/lDEdeY5iRIbSGauU8I7IPop9FolOavKBzUn4l+1LOssO7vRcEfTmqGa9ptpXrsLd3N4agarnAvlQaMs/4AhcmcP0xRp7KXViN+CggJkyohCFYBJiJqgSgEJY6pV+aky4i2ljOFxbo9FruzmitkmRMMR9zD0lAIFAKFQCEwLiDQlciachAsH+gyqbz66quhqvmUwZBk7lAeZhKCkfnsj3/842CBxZBpj8LM9NHfpsbBsjLq9Ziqw6jgGX4DPTUmb6gG7cziDvGkUe/hWGMRTwo7BHUIVgAflADptOXPQd/FkD8TytXbZ6yyaws/G65dt5Xh2fmbGrCH/hL9ncYEJRxDqaNzuHYHq1KYzBmxFPLBITckWf3ZJq7mkkLorHoyYNEwy5nI47W8VZNyW+Y0WH6WnkKgECgECoGxG4GuRNasY3IyVYs/07ZCCKupSOLH3EPARG7q8qsKgwUTzaZ/CkMRsD2zYCO1g2VltOiBGLuis5nR84BVdLkfMJ2rSS4t9wmjxckPu1G/8/GrX/3KXiC5T1BAbQcxLl1mozC0TOGdd96x9+dgz5bhitTmvkVNt+2cc8455ZRTDAYCFGroL6ubcC/1xky4e2OQA9PTi4nhnuJ5eCfrNuFLsiLomDR5NQY8mTgDKzUGOScjwG3y7tnyJ5+bAQ396mDgHa7RqiwECoFCoBAoBDoRGDL1Dncz3+BViKz5Bq1EYR0qqBkyjf/7v5uq5WItkx1c1pW0Fiqw6aabmgj9iu6pp54688wzD9fJD1dlZvQAmLn80ksvvf3223//+9+bzk3k00477corr7zkkkviBOE3H64AxwRvb7rppu9973uTTTYZMJPq41Uo5mC5px9xMjwVaaNTKvGiiy7CnvVp7u6WWGIJPvjrMIwjM6xpw+BnP/sZDmecO6u71WTwDyvcSw1PGgUMdXYH+IEcuheFfTwlUpcIMQaKcFZtY10sQrP0SI2gVOoLkgBxuXAWr3UNcTY3yQ4JuMi4wmTJQR/dKLFCoBAoBAqBcRyBrkQWLvvuu68ZcZJJJsEDttpqq0w2jz/++IUXXqjGKXMVsS222CIcd1CgNDXSY1ZbaKGFzI777bffs88+O9NMMw1gjh8UfwZLSaMX5nITvHBuvvlmdEcyzy8F+xDa/fff/8wzz8wwwwwrrLBCsdgBw26UWrGKKhk8Tckg4kkzBkZho27+EHCvFVdcEZFFxa6//vonnngiw7gbi+UYVufPJ2tIompgIzx08He/+x0lCyywAM2DGGwDcNhCs5KbBMP7lVdeeeihh6677rr1119/0UUXdU1IK46B6KWXXpp66qnVCPP1118/+uij/V1rtdJKK2277baUoMUhviLKpWZYo1VTCBQChUAhUAj0QKArkcUGPPq0Bs4CWfP0Ouus4zGieVf+SfJJWuWxxx6TCpp77rmXWWYZKageekfk0KN2s+B3v/td898hhxxiyjcXZsIeEbWjt23zXwEZ4szZZ5/9xhtvmP4dmt0bMxCsPHcm9dHr84fRuqFo5Mw111x5UBB2GPY5KOFkKOpEeVk8TEFOcdVVV1199dURWX8gRq8+ZQtP7eVhhVu17bffPiOBTr2fgWHfLz+54ZnJL3/5S1xwxhln9CcZr/qlZGDCIesvv/zyjjvu+Mgjj6CnQmbdzRgiG+SFluiwWFeSKaaYQsp8lVVWUVh33XVvuOGG7bbb7rXXXttrr73yjEJzf+/RPDCvqlUhUAgUAoXAOIVAVyI78cQTm5zkV6SaPve5z8nI/va3vzVnL7zwwlIpMEJht9xyyx122MGUnIfmgwIcVcnlYLGmQLkZs1omxUHRP7qUYCphLS1LJ2/35S9/mT+4i/lb4ApmcYVisQPuptlnn33PPfcME4Jquz0YsMIeDQ1LNbijYa8cE4145UF5o6c92vY4xOHiHnkFm0J/iaxW6LIVOIhgrPdXQw+v+niYpKmxOtVUU2HPW2+99ec///nFF198ttlmc+kQiFPw4R6FUOIV8uo22JXEI4irrrrK3dpPfvITq4Q322wzeVl3IPkr6IX999G3EisECoFCoBAYdxDoSmTNzaaicMqf//znq622mlSK5GueJIZsIZqQkoV64YUX5G6dmmWWWaRvPT03p3rM+txzz3liiJwtssgi9uZaE5j5zOQn43vxxRfL8m6yySaUyE1aC6te6peY+S/L6VBYBYeyXJdddtnzzz+//PLL49CTTjqpVplNk/IJe/CmiEf2ssWW1XrSalo1O5pQM7Py0NIIs6lpVf00Qzex8A1ld8qUbKUEzbJEk08+uUIvW0hnYzbcQFK9ZmTZK5JqPjavS03Rz39nWecwDvToo49SCwHZrAceeEABaLxywzDnnHNyrBejTlESikAzfKTBPFnGq+hZbrnlVNL/1FNPAUEabM0117TuUCswhkMrEH7yySeBed9994kCnm5XnG2d+/TTTztrmSkNt956K7G1115bwo85PegOh/z888/vUP8ml8YNYQr86quvplPsmI1hIHBi0nWdeFoWLHAeGgw2Tp577rmQ+epXv8pVhVDD5vNwARGpsZch6uZKRADUxG2AkKeffvp0jbZZzfnggw8ak8K0kIMMnB9++GGhqfziF78IBF2gI3rPpMaTxlybieahkRZAQsgchllyjLDtvPPOw7YdChMm/mTgxhmdAih63nzzTYFEoZ7VlUbmRBNNpHK99dazsId14yqu8lZ3GBJ4oVic0pxpsPvjkplW0B1camn+wNvCbD3YQuhLgbfE4vCJJ57YmqgRGnN6RyXTBpUA86d6+eWXu1Bce+21YMndmiXCp59++uGHH77YYosBh1dOkY/mprYKhUAhUAgUAoXA8BGwBNaUk82cZ/vX0RB6d80115iGrd1ENzHIdoqYid8z3NRk9ZvZy+NyU7KpSP2vf/1rE1KyRA7Nl2bcOIGvIBNOaSIfc9ppp3HDpOvsF77wBZOZuTCaMUsrHDbffHMzNJobGVOjF2UI8JCwOVIZ07WglgZT7DzzzBP2tuyyy1qcR4BXZEzhasyRNtP8AQccwNU777wTY5tyyik9EsVy9thjj5h2ivKUu+2RhuaqrBi2xwHzcVJTynzG2NIcyVYIi417ArERy4ZDRxtG1c0itppTCAEapyxTHqpECf79la98BeyA8sKT+llnndUy3EjqNaQN0dR3sTjvvPMSwHvQvrvvvps2AvZf+9rXCADErQvCpBfoRNA9Dua5TcdZDJqO9jAdsA7dF2gFfA2jH+uKTHzmhgD3339/JEY3od3woRmf0wXf+c53kK1Iii6Fbvt0es5iydZzI8SM6ll7N0U5ZYQ0DRYAhMrvs88+GXLGAE6JI2r1gx/8oEl+YL83yWELVnxKtKtvgRt7zVu3dgDnoUFi777FsOlU0hzWWbvuuitYiCG7ed8Ra9cdnoSkCRmkMKw6zJIwAXsd5K/GH0VTHmzjVfxRtnGvGW3CAy64FBx33HEco5mS/N1FG6M777yzdxmNFjXN6I9+9CNjlaQmqYzAgH2ohoVAIVAIFAJjKwLmF1uLDnv8t16ILDkZFCxH4ZZbblEw/WNFKKkajwItYMUhlPE5hcxhDtsmPXnwwQdjD2rSSu7W3CyfKr+rEh/1IFLe9LbbbsPeZPvMwffee28mWqTNoVnc3C/rQz6z/pFHHqm+kZUI88fm/fFYNyNikJKCc8wxBz0J256wPJzlepjBj3/84zPPPNPELy7y5lG5Z8mhsMlGGaOwxx7Tit3Uoz7wwYquvPJKNWZlpI0Vj4+5eskll8BNvUqbKGwotddiJD7lZVXKKaKhPawM99A0H5bQzsJfICE0bhKaHmlmLE0XkOQtxqmgT3fffXf5y/SIYGVGoYTURqHQcG73ITwPvXNoSYmeQtECji6z0pF8DiXLWbfSUZgMcQ96WbjptR6HMQ0Wtzpo68knnxx/iDnLQ8p32203CjtJTGc5vvXYt9sYeIrC6IInPi3rqUNbD+p3DrBoGLsfQ53dR5111lnRZuBttNFGIiKTJkGmh60+Hg5LZDVk2l4v2/OQRWPDfRRU1XCVxRBNNxIZqzrU/cMZZ5wRhO1h69AfbIhsXNU17jPdYAjBYw2pdOlbsLh/y98mzBNOfGAum8G/0047kf9XxeD83wuR1Zvui775zW/GUutct0BGmh5ssLfC4PhUWgqBQqAQKATGFgRMkZklE1BvRDZzSeYY0mbBb3zjGyiCck6Zg6XWOpGRYQ05MOna0kpOV6osk2gIlkMM2NnM6+awAw88MNlKYnJ1ZmtnoyF5JoRMTbiggu2nP/0pSVO4edqhWdmDb/mn+JY9eWfx5qRvG0UYquCfvoSAvYHAE2fEojVsSUFiqYz8cPfcFgXNuAVCHHnMrKX0CDglSZnmTikwx4qHqtYLOuy0SBtWOlxbrbIxADW4lyb2kIc/Rq5SHzesJIm32Wab2HUKXYgeHqoMemouuOACueHkQVXajjjiCIlVynUEhbpMT/GcMOXovqUFziZkiVU9Es2cUUDO7HE1NwwK6U2Fb33rW+43FNoWQoYWO5XKjJbmc5PsLAzbNW1ky0Ojy53CSTMHE9bRKe6JkYmAb/mEpCwiqJXKYZV3auu93IPIBo3EkriGojvEtLsmd2hNG5cybBTAhdZbvxF/NIlLxsZaa631wx/+MBG1thtvvLFnJsRS4wZSIU0aLHQKlgkuufHIkgxZamKRbNpGpNCNyMaEDPRhhx2WAQwNkcLB8gl/y/h3XG1DZUTcqLaFQCFQCBQCYyUCZopMFokOi+v6gwiyVohLnllr40mlJ7/mHo87c8p0aMoho9LeoZycZJuZCSWSnlSZRW+STzQ49AARE7J58G1OzXNez/TNqRKHBJAJntFPZ3tUKtUki0a/B8HOIlskpQbZMvmx5ZSn3qZkvErK7Z577rFM1oIBKTpzpDdLvFBCocmS/njCJXZVem8ay3SKUZ5zmDP04ATx3364GyZBXs5MFJr/5je/sdRPc2SdKlAQ0JDA8ccf77NEPhlrLqdZCCpZQZtiBQFVmTJtQ24vet0gwzTwKdRWExtsKUdooIGswCo6dAQynS7DGyQ+1ZMUu/ylFDisoKQ5ZATCDShBVRMFmpUVOIyt8lxzylVqnrN33HGHpDJeqxfOP/98NwZ33XWXSrcfPiXmlMUMAqSfcuTYChN2DYCMH6fo/MUvfuGFofhMuUpO5nC4+zyaD+ejljwnI2nlQ846jBJxocvchpvA5ek1TIziUjYYaECwNLFvzYdrul+VOoh8eCdDrLCrBrzctgEhXvlbY5d7HNBregSe1pGnI5xS78/QcLKkGPJxwxjQxHjT6flrZQubdDZRgN0pJsgI1t748XzfXtnad2KRjMKRtE/vcJU/QLBPXMpigQwBZdazH0lulNpCoBAoBAqBsQyB/1ug2SMwk6IZxcSj3mRpzkNBrETce++9LY1FtkyEoa2IqUnXHIk0eGzqNRSnTLpaoRQqiWmOSaBB0pa4lOeeuIXJlXJzWNiVsgkVo0W8WDfPOcUBtAM5o9/kh8VGWG4YIfB2C/7qlMyx+dtTbIsWEERtCbeIaOCSfC2+4hT2ICtMue8fyeMSa/JcYjrkgwAEus3xQpPh463wvbpEiRQ19wQbuwQUGOWqrKR1DmussQYiIhYzN4sMwQ2PCSlhKGjznIdpHlWdezIaij2tHHISvEzDISCTRxPFAm2GvCTUbhucQiJPOumkK664gv8OQYHWBwGdxUOe0IZYpNKh3mSINk+r2dKP8ES2mNbLaCs9OCvPua0hN7gXnqq/6KeWNpvbEunADTbYQAiE4W8PB78E4QtW9LQQjBAMLHdE6ntsIAoO4YU5q8u4KnBKUpOu5AmfueesoYuR858Y/zWnis9qAChMnhMTZg+LAztkgnLQac4WD+EZn1nBrdOPkOcAMO1t/Ldg2qplnFVDL/XzFlA8h7NMrdjzRwFYesjAOSM2CgWlCdPMNQzb4KFKbt6o0NA2iPFG4bD74MwT9z+w5ZsQuEfSXkfrl1xn+K+nDJVhlVRNIVAIFAKFQCHQA4HeZgsTjPnGTGky1sw041E4evHtb3/bxGOVIT5qysFCTJAmUbMj3mAaJmyuspkgEQh7nEnB5EqJQ5MrmXCRTKgM0WAzjWFRmenJ4BbmePvIh8VSEnnOcNIhouCdJ5xScpG3NJikFShkzloFh1gsQqMQ6+xqa2blPOVDWdaQHCRtglJjoo2k8rAbB7BY2phGRwTLugITrAtNE6axWIcURhWXYshZMhieeDmpLYVDOcwQggjJYS2mhgxw6NGEqw6ZpoozocjEUHnIhDxhse4uAhTTTm244YZit3IAsPRAjMChhx7KbtgGDfGBS3Q65LyCTtfX+oJpo0Jf85wDejbJTtlo6ViPvwXu6baz9FsM7X0+mlVy2J2GFcNuPDxYl933OQttDSSrPK0Yjmn7dEov+DsVrsME4QDOCvcAwiVK9E7IdLgddshbUbi7cFb/kgeCPVcpyaIOp0RnPyhb/oggyU9quc0B+FPOaOo5n1jAnoIoEDurwGWvrfpwjzTffPPpSsu7vcwnda0tt6NEvCLVQZTbEpSCevFSCPn0o0Nng21638BgneSgBNu7EoCLIm98Chwy+UPz5IRjuoxA7xrqbCFQCBQChUAh0AOBrhlZ866ZxrTX5pvM7l7TsezSg3tfBbLc0HyJqOEi5iQzIt6goTkSy4m8aRvvQbMYJmMCJmzGMpuaPglLMimE6jmLZMhfEs5UbQI2z9FMIPMcMkQzMdlBSWKS6iX24pjpkMUQwTjmrJWIWpk7Q2iYFhfJNrsHFDldDSknYGYVuHDSJAKde2cdJkZu4Aow8e4/eRuHnRUXKKi1ltfXGAKpszzkDLJiExdnQjo1oYdOyqO/02LKCd/ZNAkyvMXSmNMdgkIW6XGI3yCUENY2DMnaVodeidOEafX06BEh8IqHmI1KPug1jgGECWo9rSaQTgn7kThXT1gSGtReVkuaXA0n9Tj9nLEsMoHrCPJeMPJcGxvzcbQ4GXmvoBlXjCYuAQYZZ7ttDQqxhBHa8xlZd0orUTilIATOC5NFp+CTvmbCQM14gI+2QHOYbu1mdwD1Ys+g0pZLCCtXWWFdpFwyEpxqgJB30+i5vyw1AK2LdVYT9bLpkrVacTXdYS8iHU1GvdsJPQJzG83EbE7ZHDKakcAN8WqbUyN7bySI0Te2LFgXJt/ij4isQnEl4ZWynhICx0a2P6W/ECgECoFCYOxAoGtG1nSSGT1zoUlUwCZ+KTRP8+XSZIxMSKhSWKyyGTcvzZinM0HKz3nTSCpUQwLmTjM3SuHQhEqGFXvKURyGTGAE1BB2yCIBGSkfGc28qyYkw4tBUsJ5GE0hK9deey3rBMyF9pSEk6GSHqZrhSSpt1FlykRSE5QawvYtD8qNTLQhakPaDLOxYqMKSsS8muNzATKsBDnf4BKOh+l+btdK3ARLQMGkTgwO5m/WmXNIIc4h8ObYMGb/N/xWzzQfNAlLU08zbfTQpswBJih3Ct1UBogmwle2dwo3khDF7J3SoSRp0DZWmIiruAjPVUJPij3ZcZXe00Jzd9llF7gxQQDaQqCfpHUXhEUabdYh+EKFLnOYvYKzTDPUKBcN8SStht1rQgb4wYohvsUKz5nWRJgCsTllMDhLv5D5rK8dRozp+MCBGEqYwxqNmH0Gp0Ks00mbTQ23OdMkQZfDkDMyjNqr1BafJimQdFBTqO+sK7BGtt0UORWv6NE2DYlRogb7d2sXZ/R4rDMk167MPSAokHTDI0y4WZZzwgkn8NZhs06mj1sGrUDyt6NVTCiwy5Ps+Zx6nQJn3+vlgCS9SwTn+WN/zDHHWMiewcCZhm0fPSmxQqAQKAQKgXEZga6ZD/lOryjhHKYiXwM1O8rk+f4oKuAznBbqWfSJooUsQtCMi0v5RoHXnizsM9f6BJLfWPJxAG9feQfIWkypOA9VTVQIrs8/yV86NOE5NG3nl+LNbV5w8R1Wc3CmQ9+08vlJeVlpPE9XefX973/fa17YLSvmUXOkl5xkZD0090tjVr56VM1trxl5ActnpFA007aslTlV8+SuTKWoLVtoDbWmZElTPEx96I4pVsNugwNJ0sSGBCj7oJXfAhCRZ/QeBPuWE4ogap+2haSPHHFVphD54wNqDiuakTxfyUWqhEAewvgu62EkwzWNlJjsbWSwGXr4icQjiLrgxhtv5JI7CsHKzPmQmcflwnTWjYd63xb1C8M4t6UF9EPeW1beeLP6WbfSIzUrbYbo6BrcwhqA5ZdfnktQolOfWt6q3rcg1GBCFNJDm88qLbfccgqQ16cStJ6DH3XUUfY6BQ5QQmX0vq+nSedbb2A8CFyPyFgDUA5S78PBYINSL+CzyIQ9nRQq+1yDvDuoHcLBuDUwZIU547NiVkQQNsaggTVapmmJ8IILLqjfyVtcwSgBrfhjKbO2PFEz7AZtRilxSk46jxqMVSGo0SmImua61UAiJjnq1UaB62VnBQ49e2d1kMFjBOp3qviQD9AaJ6AmIH1+//33+wyCEQUNsXhtEaQyshk/PAkO/kZkOt1L+Itz3+g1O38v/kAkO2lwlm9RS48hpHesZVcJB9+70L/K/dqCj1igTQOExejQ8LN4wLChDQ7+qCHAIjHd6q/AugjXDVFY+KteGt6wVENAE80VwDIAl/rlfwkXAoVAIVAIjCUI4DdIQzaziC1l7CdzFQGhmi99txwFdNb8hLWYb/bff3+MB6FJE/O0CYmw/BB5c63FqZ5ih9mYOOmkH1cj0z4nGb4ixYsiYBVmRCzNp6nMf5nbTN577bWXyV6rCFOFCrCrCdPU2muLLaHLxGgI1cC85XviNqJALC9+Nf6NCpBHaOxNqwSICYQ2ZXOz/XA3MqmHhvlbGUnyXSQUhCqem4kFLhBKohlWxJDLQBoxKBFTtvmCUl++60lh3IsD6DLSo3nmflQs9UwHrqRO85VWriL3QUkT4RPDw/Aeh3FecyCghmps8qloFuahDFXCBLBhh3lRjIAaNzZS9ZBXL3yDhyfuDdzbxB83CQrIK7qG/sYrHiYRbuF1UIowPwGb/k1Nj72BJP8dHPQvu0gS0xm0+GV61h63TtsvfelLBNpmtKhnqNWk4Pagh60eh8CxtUrQCVNb4KQjlLkRKHwFuUkq4KDOJmmq0LnhuxlIxJBO5NVHiLHq/BVEG6zyKySGH8YczWifwkEHHdTGlaiV3ZlkyTgY2x+pMnm/yuFPgMNf//rX1diiqu97gzkO6IL8rfEtBUHFW2Pbj9C2v5Q2aN2xZAEJSblYL//FLp2Rad723Z+SLAQKgUKgEBgXEBjCU//FVMVrshvPPwwsE6oqhfAqCTmn1KCJ2AZianZBTezNUmZKdErB1BXaZwpXQCbwFdkps69cbCY2s75Zk57MXn7cSOYylczhK2b9dlZNpkba2GIIPzPR4lI4rgSSBFtIbXxmznyJATQOIdtEOYelRcmgRzS0KVZ05ktq01yA5nVn1WsCipDaTplI9tjHyeyd6swhybRJGDMhrxyBaAO9YINYWJd6BZES5kmQJ6ZVWGkPow6dJR+iIHaUiMLo10o9T1pftHA0xM/grF8AYo8xI6DApM1hNERezw6XacW6fTTET+awRk2gJwo+UCghJ98ZuqwTRW3kiFSMkr7GVbLvnoZLY/PfEkmtCBDmfMBhKJUK3TYDI1DIvneOCsHSk1Npy08Oi9ShAm8VIKCmE+qMRhQt9wDDtduaE9M2YykNg4xWHAhBj4ZA6qxDFjNo06SZ4Ezyu2krk0pSghw+uLKuUfZ35I+o/bFET6dRyWYYypcDP+4lukiy1QaqP1LCyw79obvmQ38LTOusIMArmCu3P0ZnHaZbDQDK29DyiEAXCAdKXNXR/maFlh7vgUx/vSr5QqAQKAQKgbEVAURCaOFLCjhbVyLrdJtOOjmWSjOryTIzE36Z1JrZPapRGfTC9Ik2EbYp2xMwpdmINYXK2UKklElq2OZdNWa4TMZOmSmbTJv41TTeEArClgk7rMVZG69MnDQ3FqKyuRGjLahW0EqwQxX03KUtt6lttNK0zdWQJA1M2+KN85nCGVLPjWbaYawr8C1EsBuPjEyaNMca22sFAo2vBJl2GFuxrpLn6RGBOMUuB1QaIpwh5qzejEyLi/5mqzMQ9bbGqxQoyThR30jnUKn/I5E0OCUcDjDEE9bDjXrBPxFpEov4EFvaQlvzTq86yzHdwxNKbBq2fo9YL/t2G8Yiuzq0kWbmbA0rfhLIUOwMhw8IH7cVABXe3AQ6KXLGPzFK4jlv+WZEaRUcIsMNh6Br1rnnsDFLSy+yBkBHtx5UYDd/Wb2EPOypOBM9Qs5lZWj0Q3YZM1o1Q8o8j5OdfxTqW+DKbfwo11YIFAKFQCFQCHQiYH5xmBlHAZEdkpfqtpn/TDzmIbRSOWLhT+Zmc4+aJFkVzJ02BkycuIvpSqW95gr2Gpr5Oj0w6aq3sdKIi2mMvCSrvamaFaewBFsmP/Vxhgxtmqtp2S+OOWzUgQZzuRoxZ2bVhDmeqFRIFEEkPqgM90JrGlkk3GPThBuaiJGks/xBNYCAhTjFMXSBz9BjNNw0uDlMk+jkDDds2pK3BYQeFnM4BOWh1Llp0DCnqFUWsgIl4BVLkHGoEt1Rwwd7TVQGE3cjAomHQqaBJCvOioIzygoUKmuefgQsnXCIA3SK1F5PkaQfGkGSgObqVUaDhuDigBoaGCUASWWVnPFAwKlGhpR7bMQ4qYlNjBQmIxh/7MnTae8Ur9oA5ls8EWZ0co+25qFC/O9hMYfRI7REzTrnKeRMzBFr1lXSTFilwEkqqLTXJF1DGB/NgBQvMWchYOMhtPUL4ShhjsCQmP/1PiL95DUkQ5V6nQUKDbmq7Cy78RaLdRjOChZlm1YDYLF6n0Vqo5mShmdGTv4KxJXBxpAR29mhNAgNGuqDTIb0AJwZGkftCoFCoBAoBMZFBLoSWfMlFpI5yRNPs4t5OgzGFGUONvc4NDOZERtLaDIpZBqGq/nVnnAm7CBtDlZjy1mVpkOaTY0hHyZvVmyZ0Z011TnriaS9Q5vmGjpkSwE9YjqTtJmSBoRAPRMxHXPmYJX2zTEmaKOkTfD0kOllCxpahWRoGG6Hl4Ar8zeFLJKBJ3okFsIOFWhWyXPCQ4L86EfBGOtROFzTMqzq+RaOogxGdunRlhLkwCHrgZdRPJUYowyp5JLAcR2VoV9y6ppr4pS9foewAvc4H5qlibZMsBvcAEsnJTCEnj3wHUZScxuFDXanOMYHBVvaqgnO1BLmaoPdYTo38j32GZ8qAxptRiltaoDAT9bpFJc93zIMAhEZykPcmSMfPZFhVKRqetnI2AjEW1YgpsahspDjfGBUQ7L1KSt8i/L4mbJWNMT5OJAQ+G/hBBldKUyHkc89ZOgj5ax4RhFU6UnU+oJLOdTKn4+NUWWYcFIBklHYr70omOatDmVXW3iyZURFbf4KOEBSvViMK+4ZwIkuYiBSrzmv0n0JoV/OlHAhUAgUAoXAOItAVyJrijKjmF1MV5ly7DMnmaJwWZCZpfAhYrbM1mo0cWjuVMika27Lob1TGlLlrEJjwFGo0ilTLDH76FRjjqTKYeZCDZ21N/OFTcbbTNKsxG70EEOPKAxFaByCsFORNNEyQZ4DQlDPljDjpMNhN9M/zk0tyfhGBj5qxKvMnL2zxNQ4Zf4WS0LmpDL+Qazl/3ACzTkzrLlWQxsBrmorFmVNKKenxcIQ/WmijOFxw6FW4CKvjAahFKFf1l+K2tmA3zxkIv2iiYZaQSkAhk9T65Bd9TFBJuFHm0OabeDiavAhHyXgVcNV/cgEGZTaoSFnHW0To2TYjW+acCOgxTptIXY02wiEG9kHHGIpxH+RMsdVYRpawudYC2RYo2qYI5CGDpmgwZaIDKr0RdwgSbmugR7l5MXoUKUyoDTXQQZAQ8kpwnHbCCRGs+W/KiNpr9JGlT3kM374oKM1lxfXrU7RqZV6RFM9QIAW1ugsDwMFhx0OYGNaKybYtQ9uGe0ZwyoDi0NuEOYzgdjqtAt/UGSRUobfAPypJoVAIVAIFALjIAK9rZEdB+GokAuBQmAMQQAJRn8R3OYPyt54fKscvQUc3R2C+wQ3BnzjMPruFmL0elXWC4FCoBAYWxFwyRVamxo+YI3s2IpCxVUIFAJjOAJ5OOBShSnir0n3jmksFobxU0GqPkn3ZNPHcHjLvUKgECgExhoEhjzirK0QKAQKgTEKgdxz21v/EP6KJmYtxBjlp+RrHONqErHWbyi3bMEY5W05UwgUAoXA2IdA1zWyY1+oFVEhUAh8WBAIke2kg1YapHJMCwGRtfEqK4Ox7U63xzRvy59CoBAoBMYyBIrIjmUdWuEUAmMDAo0LYode/7ICtcd62TEkyCRfkVfucTVv4FkmO4a4V24UAoVAITDWI1BLC8b6Lq4AC4EPHwJ4Yb754D0qWwJQ45sPY1QwCHc+zpCvVcQ9ftb7XmNUN5UzhUAhMBYjUBnZsbhzK7RC4EOMADqYvKyPc+UTY/mo2RgYUlKwPr6Wz9UVix0D+6hcKgQKgbEVgcrIjq09W3EVAh9uBCRiEVkP66+//nr5zuWWWy6/uDFGRYVtt3fR7rrrLh/xXX755X1Yd4xyspwpBAqBQmAsRuADMrLtBYus/bIgrPNz5Q7zloP9G2+8YaZZYokllllmmbXWWkvh2WefBVwaKvgqTb6OrpxWCqmhMz9PYFawNaMEbEnGpGwVWrf1Z22BWt4IaWLRrDKeMN2sR3O+jU8/ydREoNVTZfOrAYL60pe+NPfcc6+22mqXX355XLLncJo0ze1UZ4GSHPZILPEtn+zpjLR9xEdcWrUmKTCUn1OKxc5O6bSYMkgVOpXzJMiz0nxOMqlTrPUdmfjTQlATtdnnRxBYiVpIxu1OB84777xll13WNP+tb33L2WaotW1RpMu0jbkmmZo4HLuxEgHCTuVs7PI5Ys3t1Pdln1EUydbckEiA6vPTaBGIUaf0zimnnLLFFlt88YtfXHXVVbfaaqt99tknZxNd/OndAaZjkcI2DDQRbKuPBl1GYSr5FkPQUIj/aR6jDUZ/qpqnc9M2AloFz4z8lCNAPsoVIpy9Q4aCSVrFisrOwUOsvxu7IYg+ArD//vtvv/327ftWPcKhmXB8EG/zM+O5v3Yjf+WVV6699trzzDOPa9oaa6yx+uqrd9OTZQ861zLZSy+9dL311svvRDT55g8828B2NvBGjEyCEkUCafAqZOSQaT3Y4m1KWiEK0/saRlsOm0t9LLQeJJ9fB+wcjX1UUmKFQCFQCIxsBHojshhGXrBwRXNB3Hfffb/yla9sueWWBxxwwNlnn82zvffe+7jjjnOFdRGXMvnCF76Apiy22GJ+oef2229/6623tDIPuezSQMBFX9nVkLzmzmrramuK8oNDLtNWmNmSiWmMzXzmQmxzMeWPx3bKwUXBFZwerMIpthzK4jBBjHJi+Skjs6yzmRJYZ8uWmdJPDZE0YZBUw0MCnM9PENFMlY0VPGzWWWfdcMMNr7nmmsw08YRFG1uJK74Nu9ckM1mmOuHwk0vaAkc5/nCVD2poIC8uBU1effXVFPQLQ7I+CdaEDcBhzaUG8pxXphykoqA8HaGSFTWZBf2OVHzgZ8AnBhk+MBd/FCgkpmBzSn8JwU9JUcITTSiEJLf5yZYNSprMNNNMCyywgLN33nkntQkWztpmmtQ2eGquLcnmOVfZYkINu07RT7M9VRryOaecDaOSG+Mz97RiPWj0fa9T+BZ5IadgSMRDoyK/EMuKgRQoePKd73xn2223ff3116eYYorJJ5+c2E033fT4449rnj7ij7ImUTjsXhOmM7rElQ4ixgfBilF06tkFiC6jMJV844bbLXsbJVCy15YM32ILGpNOOqko/DmoIZlOTNlZBaoyAJQpz8DTyxQSSAjZg4gJ/qSVvW5FlDXPHyN5JvSUU/3a2BUyoPSdPW3BWaVTVPVxPPfLaISFaQR+/vOfX2WVVdy7gv3mm2/upkd0+XMQKfCJ6UH7IKmQwZ8RS22rp1YvBxlK4OkQkgFTmP5M1KgXb2IXsn6nUw2x6MxQoY1mvazSlgFvyMVKBq2G/doyQow3F/lTTz01fd0vDSVcCBQChcCoQMBvgbrMZXMdtKXscpwCkiHTMP3007t04nBf//rXZSm0Qljnn3/+b3/725q4dBJ2kU3zF154wTT/5JNPuraqV+lqG232rt3qMZXUpK1yalyyTZlNuPmjJmXXaNpSJtwkO02oND2niWm1yTCtLJcZxzRJK3piPQJmkQjkUBM1UUKeMO6FyvMhAhFuVoZbaDI0NG0kh0w7Q38WlUBnOE2JSobI2CvHosOXX365ySTYdjhsIZDGh+YJthHJFnWLVz0nWx8FpVgh0xo2Q62GjGGgnqvNkEMOR1h2beaZZ1YmGbU61BbhdCtCTIAhraKZZBsVKiPc6S0BlTEdQ/a02V588cVW0/dCTLS4QMGrRJFTVLX+UhNb/mJlZJ3ij7+gTg/jdvuz6t0TdmOFHuBQTt64tY8PaU4ssNh3Dio45JAwItJpK6ooN35odirdSlWzkoJTLfymgT/KzVyctFcf8ONk5Nv4ac37VWiR/uhHP/rFL37RvIoD9s161DZvAzsEOvHvo+no7BQ+6aST3JZ01nSWyTcrZ5xxhrv9oNpwTkEsbQC7z4kGXZO2PAdgi1f90UcfPeeccxpgPbqvNVTIqTYI1aR3IhNt3OvjkEurto+T2gp/xhlnNLAfe+yxdrYKhUAhUAiMLgRcVHNdjQPo6L91I7IkXARNTuecc47beo/2Oq+SqOocc8whwbDjjju2YNqFOAk2GQVzSac9GnLYrumZe1SmbZv5Mp2E0NCPjKZJm62ffvrp2DUN51Le2qpvDZUJpC2vHLZZxzQQo43oREME6HQ2bRVs2kYP32aYYYbTTz9dZTBpMcZtksNuJFvU7awaStoELKOTWZAnQqA2CrVNjDl86aWXokFECaopHLYgHHqaCZrbzKecuLRqISgAudXzh9F2tlO/5iSb5uE6A2QyJFvDH//4x7JcnTXNn0zMLMY6cMKxAjINMEnD+GNwkrQRiCSZV155xd4EHKycdRhUFfq70dM0t7ZtEHbWcOnaa6+VhX3ooYdavQIHyMer1JPsDL9TWNmpFm87FR9E1EYvtaGMahJpE059QzWjLihFsgez6fxjoSTWIZbuyJ42enoYareITUME2Arg8eH5559vvvW9QEmz2OzyvL/jue8WI6l39BeLzahkpNvybnr4Y+Ntp4DkZYZo6wVnE1HqKW9jslPGkKaN8GmnnWZBcIRbXwDEn1uTT89GPmV91zwhlr/NVtPpYV/KmrvMcuPggw9GZB999NG+tCqZQqAQKARGKgIujLk2xgoS2/VlL9dip11Y99xzT8sJZETMphitJ4lOTTPNNBYPrL/++rjsa6+95rdunfKAjPZwCBc+Gdw8TjUxeEJKIM/IPCCzEcAgf/e737nRd62U5Z1uuuloU2/LxToP9130zYV33323SlftZZdddtFFF02SgGSeuT/88MN8wK0548mp5bl33HGHpMKKK6645JJLakUJKy73rvWsiF8gv/3tb/nGGYvhcFP15gyuCkHOlcOUk6TKaxxsSSV62igu/miebYi7Qx8g2rMChKEVPXdiv/feez0oxAzomXrqqUk4FNQTTzzBIuhWWGEFlVx65plnKOfJZJNNxhO57VtvvVWM1uYK3PIGxGjKKads75RgP8R6mhx6nEg9JWSXwt///vf8Nz9BxoPpBx98kHLMWOx6ExRc0msoNQxhxQ0AxitK9Fd6QWcZA+p14v3336+nYGjdsFY6SDJJwzPPPJMGywmsKlYgTHmm1XT0hRdeyAFuLLTQQvo0j4/5wFUwRgYst9122wMPPABAekjSo1Lv5OG+srOQF5fui7d0QlWwHgp7huB5glb92jhsIjdmAHjdddehp7zSa0sttdQss8xCFYt8yJ+JkRbEDD/1Vp7AkD80LLLIIgJxaBCCTjdBJuN/uP4kauONXQTRQ48MxYsvvliYxoMxAHmm+UYDMVgZ2PirVhzAL8W76aab8k2/sMW0QaWn7I0frh577LH6yHiDnsFg/cNUU0219NJL008zt418nUVe4lY4vPL3BXwC7lisHRKyMgc4CW37G264wQg3xuBjdBHA7Hfaaadpp512uJH2XskNY96Q0BGQtIc8Q2IBYL/Gc++GepylPFctyAjfqAOvyh5i7ZBvxgb8DWzDj7CXBODDYQBCg6SyepKCchX1Z2Jk+uN1HdAdOg7+u+22G4sZNgb8U089ZXjfc889uo9mUetKPUu5P3ZqCWcUxTcCDnnLXLqPDLsOGc1lrfncx4Lhutdee1ktxrcf/OAHlPSxYYkVAoVAITBKEeglI2ty+vWvf40XujK6UruQmTVdHO2zWV3w/e9/P2VnXfeVXXbJu3q6OudUuItyNCgggt/85jfF6Wo+33zzuQSbKb0fk+RNUkpp68LNQ5Iu+uZaTMJk5j2MnHWpVZh33nnDaXbffXfkiaT5wF6lyeOggw4i03xTNgd4HSeMgXIB0m/q5Z75wPQf5dwwkVBuSsAMEDViOIS4NDnrrLPoSWi4i7J9JzhR0vY0h06ZkK6++mpkAjFy9oorrqDWbAQKINNjDlOTzRy5+eabK5vwcBoFpvFvDZPVo6eZ6FZoKWdvf2fay6ToLRZzLQTkEaHBBDxFAYdf/epXiDJzaCi1QQ81WXzxxXEIs/KRRx6p3qyMMBEDC4pv/nbWIXJs/gvVNrm65yEccHbdddeVV14ZAu4cdBPeya4mnBELBxJXlIuUGPz1e+gj9uapq7Ppeq6efPLJwqFBaCThDFj8SdnQQrgBG/+16vuWJKiFEJoLSiwoKYcVttlmG91EJ9ZIoT8TA5IDwrdvW1iFGzACsdvL8Oh0jHxIKjZpsFl0q3c83W7jXD+SN1psCm4ecFD9CAHDY7bZZotpDw2cFQhMjjjiCI7pOPjb06/rTzzxxEMPPVSZcnB5iNwS/AKkFmnWSufqBQWU2l0BnW177rnnlC+44AIoEXBBcDPJuj9S/BViuXQYVK1JHwv+DPUdnTa+id3fWgDs13juo7kmZlwZP8JvNRYRGQDtsEdBZ/mj4CRUXW0Enux7Qs4e/lrR6aLhomQUwdPVTCtjRt+5Q9CV/r6IedilPj2oYHMtTcG9RBtLuZYCpLmqkL8dhfx10BbEBjD+6T/ssMP8bWprJHDAjXSP2OuwECgECoFRj4BLnK3ZHUIR/WvHPU67trrIuhdXnytym0JyPZWfu+WWWzTvvJ66hprgXfgkMp1ql1RTWsiBy6L31pEM2TgCoa0STqhnaFNs5UotJeDqj+LEosrDDz/cvG7ZrrZN0kVWThchpiTzt7Pmwq997WuuxcoaCifyJh5Pt3Ep9dnuu+8+73bIMGXKUclVflLo3XO0zNVc0pQbG2ywQUiz6Y1YI7LR0yh7Djv3NHtpRu6TFYwhp+IVP90wmAVVhpqwziUYzjXXXPy3SEM9ACnZZZdd1Mf5hkmnoW7l8GbpN4s4aUBfvGGdGc5eLg2qP/vZz9IcocGhLfjDRVIjfAUWPWGUzDvkkEMyksy+yijUfvvtpyshkPdjIB+LeNU666xjGAQcDNiQE+xXv/pVU76hIigIwx9HbABSZXhgTh4IQEl9tDkk9vOf/7wzTMIclmXEaEmutNJKsnfhWPL9iCD9nfJ9KdPzk5/8BDvBifGDWFf5m9/8Bq0RUftDMip46B4Mv3TKHkpCkz/jFTGtmrARqB97cSBsBvj+1nQTrISTPyWtBAVeL9GHoESPGL/xjW9If7Z7AHY32WQTA9VNUbN14403YoTuHNKVbpBwWbdnxiQP3dox14T9QWGiX/7ylzvpi3sG5NKtC/0kgwlt/ky8AKqLU69w7rnn+gPH1ZrCARTcRhoeOtfdsmR8+9uMqljv43jur3X9BcxcuywtQE970WB0AUG3koRhW/mjr/P3lbb+cPwteB0wAwDm6gE1++yztzW46RoCnuZDjw82Q8ulw1UiFJlO14c0ZyIFqoz/Aw880E2FMv9tCkEp1z2Hfd8A6+5R8lhbowj5lkWOzr4rKclCoBAoBAYdgaGXt74R2VyzJEKOOuqoXNDb1dAp82g77PQyF27zmSyCKdAE72znpKsVKoBF5aIcKpZrsbau3cxFYdoyHU8i4xKvYPZFaIgp20L+uOqhtrUQ5M15aYUCmlpiKxMhrmYyNisQaNOMUzyR88OSqTV52HvYjV11Oq/S4brrrktnf1/20jYbMhEWzgc10W8+QztiV6WoPRlnxZv+5jagJZyEYC5EOom1LghW0d9jDx819k2DB+V4Z9K6EAgIzkorbrfddp2qjjnmGKwxDSkBfsro9U9/+tNWD1Idl7sUhqyClUoUVzQ7BGzcUOm+CDvEoeNnCwEdZMtZ9cFEkq8Ra5UJgU7ZYhN/mL360AIFn/cy6ihxm+QwWMVuyir7vmH2smJxIP3CdEI2xwvh+OOPpy1/GrEi+arLQh2cgqRAcsqh+jRPk26eNFQNSNqsQQ+M5BMpPFFMh+obyJGJn9EPH809pI4hAlaA6HcUmWNgRxMJyPgGHFzZbQOyQp5abNVSk/SOw/yJOUSatcpyyZgzmNVoFT2EU5YDxj6b9RT6vs+VgbyopY0XXnjhhB887QMUo72P575bjGRDuzVE36VC22GPQvub5XAyl7naRCwA6noFKyUClFO6IOE49ctf/tLNSUPYKaG5SLqDTfPmEmx1LoFO5RqqcZXwR0G/fW4nIha1bRD2cL6XQ/chkggRcHNFs7RF/OmlVZ0qBAqBQmBkI+AKaWtWhqTGXKGGu5nz5EKIunrK3JAxeUfSqc4mZNS4VqIRbtxdmuXbCLjmpiGKg7mqMesgByeccALOJG+BEFCuib1UlinBQ2dZjR122MEVU1vXepRIQ2tGrdXjOs3SgZKahNWzyK6EkDJJRDbpUmKxxS7iiN26vsf/Sy65xGfCzMfa2nhunnDRp8SSQcsBpRIlYDiABMuucRjHMpMxISKHcqI4U6ZtYmzRLHVk7uGwMs/Vk+fbRhtthPbxnDxPVCozR0DgDim0xyFMRQkWniS1VR9nFNrGxHe/+13JQqs/JbAxWqc0aQI9CnymVqT6RcKSt9AmL89HUtQcU7B3im+c10Q9D4mBAkSaEIh7ylB1ykbeoeboIzFNGGKFV+JKCOnf5oB6rTy1jLBwxE6DsSj3KfsuDUwGwdJcskoekWNRzjGG6NehplX5LcOGZs1tupjOnXfeGR13qJV9zia6IUJ93i666CK3MRbA8FA4xlv8pADDUC8r7DOxGV0Z3twjw7d0CtOxDigOkOSe4ZFh0M0RkbII6qDtCQAADRitBEuVBSoONVdPYUaRYFXK12KiVhfIpflD0JU6naRTxCjUnG/qVUY/4QDltiFi9gS8bCQp6E+AP/pL+DrIng/+AP0RWUKQhjqLb+i1rLCuSb875W/c36NOVA4O3UIebn3c4wyL9lDllXjbeFamNgNDXN3GM+XInBCiUEFDTg7XqMoEpUAn5QoaMupQOTXGPLFcT1pvcg/hA7vLHUmGNOEhBGCuYf4cfIHBENUdvCJmwFhUk6dJDsnQ4xR51yUmaNPc+OEzbbmWcoD+/PFyIF5hw65LnqHpShrUN5le4iXDSrrJwIM2Nzxd4bylscrgSoxwUGlLLFrZ0pxYbYVAIVAIjC4Ehkz2w91cCl0QZQSlcJRd4PDazIKZaLVyfXdRc5kzzWQud8qF2MTj8pcpQd7RFChJkEnOdVBmAglDZ2lwKSRPj7LlYsQsF3PoGu1i7dJpNrXINfMxGcLmAMkPi8xcQ1lnJddWh866Iuei7NptXlGjYItOjnkhBluV1aOct3TaTCfMOYuwhgp7QqceBxV1Y7G84qF8MLWMcl7sfFOPyFrSQI/ANTTxQAwL9GjeagdiNo6pp42MOSlzBrfVi0j6SsEp2gCuHqSogCggI0xMF4bcFoukMi4uTcXnKGd0uBv51GurwBDlTOhKMyWLyrwSkW7KHKwGGgLkTLrGWZvm2sJf5skp4EPAIVfdZghHExuXBB5JTXjOigIHVGoSpqW5SojZ2yjx1FsGNDO37CZgrdFUH9M6Mf2lg3j1yCOPSJlrKHzOKOBwfJCuVtaWPN+Cj4LKfm3uYRBZ4aQtP1kP+PQgc1mhoaybOCkukaJ6miQ0XRwQHCaEOMBbMoS7+ZMeESPEGA0ghA2ADFQClNDPrkhF7ebKIwJlhoISWHSiLtZQwSF5ZQV7Pc5VoakkAyuuEjParYiFLRkP9NP7jCqIgoBhwx+rPvInzwE3nzDPzZW/SgrdbEDDmmmLOhjiD4cp7NcWbzXnsOZAhhhzGc+5qlBIjAxz3cZzG4eEjUPygtXEfrj+qNc74GVOIX+SWqVPA7uGLDobB+w9+ncf5e9UHylEwD4okeS55f5777237LJMpzs0KInF8nTPKyyzIawjdCWQ9Yu/IKpUiou3Tqlh1F5lG04JJIPc52Vyp0Egg1YrnmcktCbOdm4CdIoG8QZbF2dOSrTz2cXHqXxImHKpB2OSz4TVw4pLtOVPo1NtlQuBQqAQGGUIdJ1NXQpdB10EzWoury0r6fqV652CCcYM5+JumiHswueyy3WVLqCZUJ1S4xLvAk2GWldwc6Q1ea7CLqMuzU6ZmZxyZbSykFoTKhptdvQAV4rO92tpdg111oo0y/tyWdfcJTVzTMw5tAU+iVtum3c5QycrTtnTudlmm6GkLIZdoaEUcjWTk+YYvD3N3BCggqAoEYj1qZTQKRzIJGRvYt1www24oFMa5vouCqTNIT/th7o2hI+aEhzaMg8xai5BTAHIhHqS6JpZkP9LLbXUUNl/C8MGFPy9ESLrRlgvOBscItZjz3nO2HOMt8IUgliEr9CEmaZT11BImKQ9SNWgAtxzKGSIObSQwMAIJuRhGIIVMc7AXDn4kHSoVWuuE7nEARGp5JgNjH44QLwKYIEkDZRj3kZFKBqvSAIHYgk8MyjlWukp2uInl4QWAiE0m1Yt2L4U0AuqxCgc/UWD4SGQdDcc4oAa7omRAGxpDhQCZJE/YlFWaEYdEm6HPQqM2sSueUgqiwkzAwC3MAAozIDklafAnPFn4pEChsRnq3q09St0cZJLrBg59hnhoSDxnCEFY0khIYiXEosN3C+p8ddBALzGM88NaQJ0Bg396M/5hz/8obUEetygJaMf5cUl9nhCsjP8HvF2O0zIQZtaY4YkZPS18A0PVvwpAYpvyt3Gs57SEFY2DcXVzWKrJxOHFcA1tEOG/NgHKNTzBAJi148GZBRmvMVJ3sZtDXP10xfaGkh77LGH35Sx+lnu3N2gWBS8HmBFkxtstxAC0Qs8ERrwBUWDSs11JYVGmlb5MxQ4Bwir5CeX4gZzTnGVWNyDUouuRyGnIJOI2HWfD3yPrfyt0WCviYg8/zECPTNhlCFG+cM3OCj3UFuHhUAhUAiMMgS6zu6u+y5Y3h6QUPQk0ScFMm/lmuWq6nrtWmn+c6UzW+eSym/X2VxzXYLJqDeDKlPoaujCiht5RQyVdPUk6dLp+qgyMZNRY9Z06HmlBWReEjcvunS6aLpey1zSxqLD0JrMeTxx2WWFV67jmptTk5SN5vAJeREzhw+KoXGcMT+JSCbJRZkYGd6aJn3iyqFPCvi5XZrVczKTWQiBVkLjBlsc0FwrU0j8JK95WKyCOYwSwnwzUfFfE0CFSgZSc5WCSvpNaQrCueqqq7xXBBCtIKMLWMnCx6zf1YTpTH4MDbuxCBMC6SA9QonZi+mYACkBtmgGKYUcIBwYzc1OUesw6SWH8q+Z3gSlOeVCix41CYo8JRkkktBR0rRZOeAZKPwNAJJCYwKRzcezQOrJNRNeczGdKyBDRguZAMv/DJ5Uah5/hCYEMjQrkI9dVnqBiNiwmyf48lIYhih0DQFrH41Shli/7LLLFlxwQZXxXCGTugLroCDGIkn1AlSj4KxKvqlRHu7mlC1RwAGAnE8Uxgxn8qdBfwJ0+yQ/ig8BisL0qYcGll4oJ+qQPG0NzvzhcNJmuCYcDR0apZwk4K5MwZ0VikwgazYUjBB7wvFHdADngOW2co2WJlPiLD3i9fqXvLUPVvgeCG0Z5M72cdOzAYE8/MPFWVdv75SRFiiYI9NtPMccYZuytvDP9SGneuyFr0YgekFBjDxXzhhgN4G0fiejX5q3EGaIS/6mGMr4TFtPlhz6BRk3ybqV82QIG1f6y4Mvf/WAil19p98F1aDjUkwH/OxZT4c6RZU7DUDxoekhwJleBj/nndU8ERkkFsxsvPHGxobRzivpAEpcbawVNh5I8hCMkFFICL3o17a2QqAQKARGKgJDJp7hbqECrrneXEZnrVJ1tXJNdIU1L7qcYWPoiKdOuaK5htJjAnM9zRTrkpom5j+XPJdXAq6wvuDjKViSRvQgiOo1wZ9c3818anAm7I2GfIQyD/ddyulH7EycpgTCLrsUZrZQwJPImKVchenkobJrrkqemxhUukZ7O8TsyzdeuXxzGENVFrJsa3iYhiZm7x4p2Jw1Z5uQ5FFMRSRZp01Dew7wRCF7njBnM8fEKBOSWPTwxHTl4Z15CCz0EAMFWwFQPQENw3Q9L8ZIAEIPDbwFvhfSPYtkVHNGmbPvZWOd/6YcIAOQpIJW/FFWzyiLItId4OKYegWZV9kjCblMlvBk3XNke72T2LV1CHBtjQ0NeZUCxAAiEJpjHbNXNhL8vkYGiVY6ggYZKawRtiQpQROt67CakCoDLF0MAWUYom46It4aKuTVM8RhQUUzMcGKwiEnyfRrM04QOO8Upo+oMq/rLIa8yu03nHzHikKc3j5gwkQZLBxrIwHyKikRZvrLYS9bOkhQwgE7c7SJxQBQpsHGqDITADFslEUtUjLpSqt3pGNZgba9jgOChrqJP7SpzN+jVsr8Z87oSiAGDOTFnvvA9KZKStjyV5Z3KBmljS1/UE5Ryx8+q3SK2ypFrWBjpb9bRkJaUcgNw4DDAolC3jrbQiDjsMd45kP+EoOPthHr5oyzNiGAThPhQNUhJRzQOy4CqKdDGjigu8krC18BwiBSBqa+AAgBYuRp83ESbSHZ7kY0TCe61pGhIVG7kXNz4i8oMXIjVlyHySdkbQ059ekg108d4fZDPXl7Idh0QbxVM+wG0oyHppPbVp9jrga8b7ZIxDIhKDeZXqh1SpP8ZdEmFvuYG1Z51RQChUAhMAoQ6EpkTRUuyq6Dfmjb3iNLn3fx4nOupz4gJbUp1+KdGF6aLcyLFomiaFaXSriing4V1MiA5uOIxEwJHq75fpD31n3GUqrV5uJOM7rs6umVBQpNvdibpB3WS4+Lu0/AWEApJYDarrnmmj4Obzb19rSrsGeguIV53aQuh2qa54zLvTK1tBGW2gwZkgn2sB4RZI5XLvGsu/pbuOYK7t2szPfUeiEJh3Md921a3xAwqXjTX9R4nllK+lDsYeGmOrOFKcfeNZ1OBRusAhcfYl3BsleMzTttCAH9VuvKdmNgTqHX4HJvoBUWxR97L4J4RKsLTB54LQdMKr6YG/1asW7fbdOJBLQ15VCCL1p+ylWAoCBa6RFTpt8mQJ4sM/VyFWBFod6jatRThgbO8PRFMITba0C4PlU62jxKPzR0N26n1/AhrJRv5tSwDXg6FBpGDkBnWRe4MWBE6X1cWVzClCYHBT9BbW6GjKecLMrK841L+otRfWcYWMOnEhSA9c0HI80tEOh0utDckCjotbCKbuD0Um8WdxvjGxoGwJVXXpmFHxbOej7g7RxeWRuKOug4sVju4utXxqHxIHBuG/OeY2iol8MzAnUs9kIswEXGiBURKFB24RiiAtGJym57ZIspN+axH33EB31k/aJho488v8Zi3SoYwxAgCboQLJr55h7AHxTiol9YwbT8YWZ8QpIkMSAz5480f/4ZJLren49HIlau63qu8lB0/srceEhg6yADQJ/KWLs48NOfMEO9xBtAht2D1N+CGPlv5PDKEHXnrBLOTCPZBqG/QYAIpNt4pkEHQQaABmT+HhPjsEbVQMPeowDPPYCjN902M2RE+Xux90KVscEiMbDgtSptPNFEpbt0o52ranQQAWhz2GYUGTZ+XAZiKKnLFLeNKCvpvRsQlFzH9BeQoWek6VZ/L/rRHwsq6Z02gIiFIV7l+mPvTwb39aEJeQd/FxwjQE9uqJh22G2LcLuMJGug0uWUXVHoC51ohBs8LgX0QBICRkh0MtRNedUXAoVAITDSEUDLXI+ymc9sKbtyhQA5dCGWf3L1NA3kqucq5k0O12KXMFMXGdfuXLV57GwutaEy0g/4B0lX2yh3yd5///3d3Cc8M65LrVWJVtqZIMlEJ/Lq0kwGpbO3AIAejpm8zaBqfASKsGmbhqYqX41BMsi0K7jpxPxKWHN7k641ps7GQ21NG760b4KBgLk5Dijn9wjavOVNXlQsj9u89ODKTls0K6SVgs1kaa+mQWpiGHrmn5YrmAsZ5aH8qzyNSZoJh2YRMoBCy0Tt2w4WCif8zEn4HyJCxvQGRuEANmq77Vs/mjUzSwUrUehZrXxrCRQASb1eUAlAylFPYep0mz5l3UzvKbY52GDIt9K8v5+G6V/UJ71vQhUyPuos57/3ve9hSwx5SQs5wFwpcYoegVtGgprEnxaRAbD88ssn8HSxstkdQeRG4sUhKNGPhl+02dvUWLupK4lFbeT7vhc+Zpw7h+hE2T0u11+Qp4cPekEBe0gfEQOUcCLPJZQoYiTbSEir4XriFNqadQJRYp+vYjVg1YDRXV80+NPzp6HG5pS3rNyKONX+HjNKczPJT2zVWVyZsA1DyqHeAa/DuOcv0cBIej6Shijl7ivyR4pIabj11lu7+fQZB+NHV/IhfyzatgFPrL+bzzVkZU77C3WbFDfcWsQ0lp+ajJDhjmcuYfZuljhgMKTjenfG36wm0Uk/DfaGdOA1DqPHoHJ3p1MiQCbDm0BqaHA9MYoY1fUuHZYQ2LsWRWGAMsL1eFyKexyg3J+eixLJbK4YoHb/5lQ6KH8ChmjaUssBJnLYxjys+hI1sfyxaN7k+dZiSeH8889v3Uos15Y2sGO69oVAIVAIjDwEXHA6rzkmqfH8w8NyrWRYIdfrVqPSBct13JVOagQ7ca1Ev+Q5UBzqXLidctVG3exdnclIEtCjoUO8xNXcRZZONeTpNN8rayhLYa7Kz1bFlibauqqGHJiAMWbTmDxcDNFDMpEQZjeUyPVdw+YPMXZDqswNHHDZZcv0Y46nHyVF12iwIjZimlAS3xQI59Csb2LggENsRkQu65zRSvP4k3g5xoFocIp84s2eURAJHBk1e5l6LUEmFsfiLUOYEFYHXvkYH3vCG8zEwuSAU+TjQPasN6yUe2zRxqJ6DYWctRMO8U6xBA17+HAyOg2JRnlBpwsA5cYDt4iTwjSP6mUB2sRr0xYIrEQmmhniP0k+OGsLFAqsSPAgbdSqFBoNClQRpiejizaEng8ckKJuaoFGiS7QirBIDZiYZrRhYt7Nk1yV/drEqIvBAihKUFIsNgyeY2AJdY5OkvznfLMbJJs/xAKUQuLqxZk2zsm0cloFmbRli0WjVKW0GZyNqPb9UZikiwkDB1Yk84fgUCWH/an6y+IkzKnK6HUI5IwTPvvrEz6eKvXbwiFJoRiNTM9APOVwqGxU6xf5ReM8Y4+hFnjc7uO+BZ6/Dq2EyUlboEh0Kpnmfw4zPOwznpsSzfkc09xuf+89nAkg9PtzM2idTRQCZyVkjkXmHKZtAznjXGVn/8YfIbjQccnfu4Yunq4AfHDlMYr4Q7+z2ma4Bl64Iev2WKy/R9bVU6XjVGqloEm73rof9qGP/EVw0p+DPWe6BRv/mQap0eKQTgUeKiTYyLQYU0/t0H74SBsPEat9IVAIFAIjG4FcydsV2LXxA4jsyHao9LcpGcnonDmwBwuUUQRzmG4jZu7sMbsUeoVAIVAIFAKFQCFQCIw7CAxLZIfchdc2GhGQ2GBdZtRegk0PSWVJt8gDJacl5yFHgsXKgnQy3dHoc5kuBAqBQqAQKAQKgUJgTECgiOzo7wXPCkNVkVp5WY+wrT9GWz0V9RKP54MeIHqTY5999uFrngCOfqfLg0KgECgECoFCoBAoBEY3AkVkR3MPSLgir5ywbABzVbYyz3tvMrJe87KiUY7WZmmmUxYYjGZ3y3whUAgUAoVAIVAIFAJjDAK1RnY0d4VVBJYNNCfa+zFq2rsjOStHW+nYBlQVCoFCoBAoBAqBQmBcQ0BqT8idL3v971erxjUgxpx40xlekY5LeshrzilbOIvm2pq36b92WIVCoBAoBAqBQqAQKATGZQSKyI7m3recgAc+wWMVgdUFeG0+6uTjVlk4m3ytsw4JjGZ3y3whUAgUAoVAIVAIFAJjDAJFZEdzV7TvFfiKpI8S5MOQFs76vLmlBW1RbOo7FyGMZr/LfCFQCBQChUAhUAgUAqMbga5ENk+x81wbo+KndGDjVTmM853PvlMzgnuLQTs1WDbqUGXLR6bQHsd3Cvel3PQrdEbUl7YjScaXtjo15wPm3vrKx7mcCoUtItuJUpULgUKgECgECoFCYBxHoCuR9YzbZ6FsAMov3MgXYreNULZvmpIcRDrIomfoqHP4K5Yc9qaSRRS2sc/8mtcA+o+qkG/awhTzfH8AqqpJIVAIFAKFQCFQCBQChcDoQqC3z2/ls1Aec6Oq3kCSJsxzcPwPo1UZp3FB5LLlDkcwkhgNdWaatqQnGcVow54V8F2S+fnHAVjEvClpAfZIiA5AYTUpBAqBQqAQKAQKgUKgEBjFCHTNyIZE8gZ3tPm1+rDY1MRL/NWmPFgsNmrz2n5Ss1gsTyRQ8eZmRZlLhDt/7D5tP3CfpHKyvBSecsop55xzzge2KoFCoBAoBAqBQqAQKAQKgTENgQ/4jiyemgfxuCNCKYXp9XmMNhlZ/NKGa0pwNpY5ghGyQiH9NDNNW+enVZUZIjDU8hB2G0bbL6OSu63VPPPM46ezbr/99lDbfukp4UKgECgECoFCoBAoBAqBUYYAfsgW+heLn/vc53pbWmDlaKgkkvfKK6889NBDCN9KK6206KKLtvqsjkUrB4vIIqlxkYkTTjiB0VlnnfWtt95iYuKJJ5amDcfNuoKlllpq+eWX7y98iT/kGy/XHF8vIttfGEu+ECgECoFCoBAoBAqB0YtAb0TWylFsb6uttrr88stfffXVmWaa6emnn5522mkRWVywMws7WCw2WLSVr3fccce999574oknStNOOumkiKyvq5Jh2lKHBx98cLvttusvkUWIw1nj/6677uowi2VHb0+U9UKgECgECoFCoBAoBAqBfiHQG5H1CJ4uHxDYeeedl1lmmemmm85eCjPP5SVHPZTHCx0ObjqTWnZpPu644+RfF1tssQ022IAPrPz1r3/NWYx2ww03TLlfAYfIYrEKkr477rijcr80lHAhUAgUAoVAIVAIFAKFwJiAQFcii0FiqPbnnXceRxVkYZHXsFg1PtGvMh/qz9lBiScrYhlCoy3GZRTdlJFVI0PcmKuySmSUUdya2KOPPopzTzPNNH/4wx/kjNWrlEK+4YYbtF166aUth1WIk9dee63U8htvvOF3B4jNOeec6KzQorClbMN35WutqXjqqaeeeOKJueeem6opp5zyu9/9Lm492WSTRWFj2DmsfSFQCBQChUAhUAgUAoXAyEagK5Ed2Ya76Q8zxh1tZCwzeP/99y1dwFz9AoLPFDzzzDOWGSioxGVxTb+JdfXVV3/hC18grzkqjFijpFtuueXdd9+t8NJLL6GeN9100xRTTEHmueees17ixRdfVKZkttlmQ4KRV63UKNCJRnPA2TfffHPbbbe97LLLVM4777yHHXYYNiwZfNFFFy2xxBKMxs8s7eVPCvTUVggUAoVAIVAIFAKFQCEwUhHo+vmtkWq1F+UYJCZqI+MdLCTVK2nYoXq/gCB7uvXWWx944IHO+nLWz372M/Wo6sorr4yeysgeeeSRmiisvvrqlh+ceuqpDz/8MM46wwwzvPPOO1oRnn766S+88MLnn38eS95vv/2wz/jDHOYamfDRRx55ZL755nvsscdkZNFcC3Zlf++//357m4QxFqteQ2o1LBYbJGtfCBQChUAhUAgUAoXAKEBgjMvIer5vEzlqiCNilgiot82mmmoqX1FAcBHWFVZYQTnpVVSSJDprCa9WXghDMS2r9RLYEUccQRXu682wq666ylkEVw2ma5mBFQv0W1og14uJOiRpCyVVsFrgJz/5CbveOWNFYlhD9bPMMgt+zBCj6SFtU5C1DRXOYe0LgUKgECgECoFCoBAoBEYeAmMckUUukUUBe8RvU0BkpU6/+c1vophoYhYbYJ8OSaKt+VkET/wlRAlYXbDxxhtbw6pMAzE6kU5n84NhyuHKr7/+OmaMpIaJoqHM0RkyakHCGWecYQkBAUZjhXzW6UoMW4+LAbOL4GoY2q1QWyFQCBQChUAhUAgUAoXAKEBgjFtagHfKttpa8F6oWnvttZ999tnXXnsN9ZRMzWIAJBV3xC+lTpXRzTBgPHWfffZBVXFcSpx6+eWXsVj1VtmqCYtVmHzyydV7OSy2mFYI/ZV/JczQKqusgryygss6pI2MU4ceeugcc8wRBswNG1IbPbUvBAqBQqAQKAQKgUKgEBgFCIxxRBY1lG3FL+VHbUiqGk/z5UHDFFdcccU111wT073mmmt8USEJV0ihrTbraMkjnWoas5x66qmzqMBZZJRazWkmg906DNAhqa1MmBveAwtbtSI2XhFwCm8O8c16AxyaOZQ3zWtfCBQChUAhUAgUAoVAITCyERjjiGx+8iDcUXIUf/XsXi4W3cQdLTM46KCD/KCXU6effvpZZ52FTSK+GKQ9TumHG0hKoKq3NBZPhWBbGIDpIqPaEg4TdSgvi9SSVI6wPbu+jYCe3nbbbZzBg5Pu1dZZyrFeTSRic5gm+eSCcm2FQCFQCBQChUAhUAgUAiMbga5ENhStpTbRuJQxvDzZT0aTfzKRIXmIHdKpxurSo48++p577hmA996+wkQR01BGGui0GMAh/bipGocoIybKtE2Ns3wggHRmxWrSq/aSr3htDkM00VBNyOcsDYJSky0cmozCbrvttv/++3vZy+JaJJhjTFOo4Q9+8AMfQ6AEDnEVYgT+pab+LwQKgUKgECgECoFCoBAYuQh0fdkLW0X7PEP3wpPcJK6G2+FtPkGFF6KbDi0S9cJTaBwS2daerrbaaj56pa1fEGg/GdD3OFgJjX7ggQfkYhFWP0Zw5ZVXoo9M4Jdvv/0293xQ1uN+uVWa1Tz++ONa+XAs4nvFFVdY5IpWfvazn/UTBmRUOgxh/d3vfiezi7/y2Qe2SJ5zzjm+gYCpY6vrrbcehWwhxLvsssull15qJcMWW2yx/fbb84TdG2+8cY899qBz1llnzZoH8pg3twn0PcySLAQKgUKgECgECoFCoBAYIQQmmWQS/C8bqmdLGU9VeOWVV/KVKzbCAkMcHUpSHnvssQgiMS8/KchNYoc0rLHGGiid72HhmtHW9z1aGWFP89dZZ53E1vnIPryZdSaWXXZZYuT92EEkMWwFnNJnsxSw3l133dUveEVnvEVJk3alBBUmFgqulZ888AsIYb14rVYo8uGHH54sry/aRnivvfby2ll0WguBEKdc+0KgECgECoFCoBAoBAqBkYTAUKL6v0yVCSR2PP9QN/zMpsoen7NvGVYs0AN9zA9PxWWRvOQdW0GrNNEqlWruuuuumWaaiXKV/d2kRZmLFR5jrshiGCqai3qiy0ywFRoaVyPTMqMOsc8Q1jjQHMZQKbciVnqVvMxx89Ap+WaH6ht71lCC1qcPJJj9sK1fAnPWpmEQc1YTbjSXmsIqFAKFQCFQCBQChUAhUAgMCgLhXY12yjB2JbLsha5JW8a2Q9wue4psSY4ilzKaSCEKKD+KXDIQbietm0LfvaeWCYxZAYvFWZFOZQVWQqabttDcHIbdJrfaOKuGaDFG24OdxzHN1ds4rzlq2/h6BGSasVWc2J4YGXsR9YhLc6r46WzzrQqFQCFQCBQChUAhUAgUAoOIAF5HW6NbvRFZpE2KMcsJlNFBOVGF5DidCnUbrnPJm4byDlegL5XhrKxgjaGnWikn8Yk1YtX8IUAyh06JjYfNT03ErLJF3nKunTIaMkGVMJOR1dAhpt7EsFVbODFtXvkiwxbTCtl6ENx/Vdf/hUAhUAgUAoVAIVAIFAIjikCjc1GEyHb9agGKJvUYmqicJ/uIGjJn30ngUD1Uksb8skB7vM4Yyf663JqEvDpMAS22cQl/5Yw9zdyQRkU9cxjeya7NWXu+hbajqqGejadGhhiZmNA8wvEh4TChrZWyDGUlgwQtMYzWFhYrZNxdq/6mn/sLTskXAoVAIVAIFAKFQCFQCDQEuhJZtAx1w+3sUT0N3nrrLbzNIbqGyaF3CBymi+plJYBPBHgWH26XfOoAiB2LbGWZrAIr2aOqbPHKPgXWnVKDWyvkq1jE+GbDU+3Vx3luOxXyGtqNvOYwYqG5qCq1GLOGE088sT1PSCoTYIgwGs0rap2iSlmYGtrHlla1FQKFQCFQCBQChUAhUAiMbAR6WyM7sm2PxfrDv5PoxW7R3wFw+rEYnwqtECgECoFCoBAoBAqB/iLQmYLUtrelBf1VXfJBAG2VMEZhkVfZYpUy1gVOIVAIFAKFQCFQCBQChcCgI9B1acGgWxpHFKKt7XZBFjapWdR2HAm/wiwECoFCoBAoBAqBQmCUIVBEdvChDpHNEt6Us8Zg8C2VxkKgECgECoFCoBAoBMZhBIrIDnLne/0rawmShVWuN8AGGeJSVwgUAoVAIVAIFAKFwFAEavnmIA+EfHlXIjbf6mqf9BpkM6WuECgECoFCoBAoBAqBcR6BysgO/hA444wzzjzzTHrzspe3viopO/gol8ZCoBAoBAqBQqAQGOcRqM9vDf4QmHPOOS2K/d3vfmd1ARZrU8j3cQffWGksBAqBQqAQKAQKgUJg3ECgvU+fcH1+q7elBfKIcop+4OCFF1644oor7r///ueff36RRRbZZJNNpp56as/QCVgDSmZwH6B7TYrRLDbNa1JI4WWXXeZhvR/Q8oNbU045pa8B+IUCv1Mw++yzr7DCChxQ7xcZ/JaBIPnWyR2jMN6K3K82fPrTn6bfVwWyCUHhtddem3zyyXuMBNowUb994He8lLVtP9XLB+75lYQoiYxD5tQwxwc/xKDGPmqVNaGwWYmfYqGNZodC+OQnP9npf4RZJxNbTcypZqjprEIhUAgUAoVAIVAIFALjAgK9EVnsEE895ZRTtt9++2mmmWaBBRaYY445TjzxxMMPP3yfffbZbbfdCMAItcLDcK9BwQstyxP5/EYXncjrO++8c/LJJ+N//EEKn3zySaYnm2wyp775zW+utNJKfnVskkkmwSC10kQhDFg5XByD1IQ8NozFIoshxATCaxXCYvNrtPkl23DQKCeASuKaFCqgkkxgllyimUVMN/J77rmnsnpNbNz+05/+RF7bxmiFMMsss8CNANod9F5++eWppppKOXhyErasMMdzoaV5rDRVhEU01FTtCoFCoBAoBAqBQqAQGIcQ6Lq0QMISVXrqqafmmWeeNddc85hjjpG/TQ7y4IMPPuSQQ84666wll1xSDWpIGHUbXNiQPzqxwBBBOVoF7BC923bbbSeddNKDDjooP4eL5znFOmewTIQP1QuRVf/GG28QdjYs9t13351wwgl7uBp5dDk/S4uVRnPEcFCaO5PEBGgOleQPiwRiUcEWlwjguLZmDk1XDscNH3VIGxlNyNPmrI2SpjAFksk6yxxjrvhuZ9K6mahCIVAIFAKFQCFQCBQCYyUCyJK4wvoUeltakLfvZfvmnXfeI444gmijgLvsssu555574403SoUSa/WDAlmjd8mJRmcrO8siPoeMNq753nvvoZI2D/fVX3vttc888wwevNxyy80000zyoCGFCDcBh3SigGjrlVde+dhjj80222xzzTXXggsuePvttzu12mqrhaHixBKiarQSL6P4pZz04osvfsEFF6C8iy22mMUMVPEE12QXSjwMpyepLWIaOqscGqog6cuZBx544J577nnppZeWWWYZCyRmmGGGb33rWzvttNMUU0yB11Kot/jAz9tuu+3uu+9my7oOyyqkctXTAwrpZJLh6GpqKwQKgUKgECgECoFCYBxCwBN5hCmb1KMtZWnCFNC4VKJfapJuRLmWWmqpyCC7iFSER3yPCFKCqDVVmKJDVtTgrPbrrrvuzjvvzBMc2j7ukUH4pp9+ep037bTTorDpxcsvv1wTkvaUx398EenEOFFYwiSnm246DHKbbbZBDUnaWLR9/etfx1lxf/SRcnQZV0ZSsXnIEGNXAXUORY7RWWedNRYJMCoEjFaZ8nDohRdemHWqvBkWmr7hhhtqi90Sy+brB+4iVCLQc889N6arPPPMMz/44IMEZJrtxS6iBPW/zeq/QqAQKAQKgUKgECgExkYE0B5biwyJ/b+n3mFgbS+PSM5hHmFjYxNNNBEeqaDykUcewSYVcDh5weQgW9sRKeQxOraHnMUW7ihDmfyrRCbl6j2jR+9wxyRNVV544YVLLLHE1ltvHVr5xBNPoKHf//73ZVh//vOfk+S8Jrbf/OY3UqpWJrzyyisPP/ywN9iQToQVNKJGK9FNm7juuuuu008//aijjmKRQulba1stEZZGlSiFDDG+KWCZkqYe+nsxzgJiyLJocS2dIhIC//F+ypmzJEOA55xzjhztQw89RMl9992XYC2TFYt6exoQa3liUBDzxttzzz0n8M033xx7pooSDJsDgiJfWyFQCBQChUAhUAgUAuMUAr0RIHlWNBEDQ9TyHhJqBZ3TTjtN7tNKWbTPNrh4YXt0hhnjl9GPyaGDSCGSyh90FqtmFwXkGxpH0stnCOuOO+6IsKrEwqnaf//9PZHHRDHCOE/J66+/jneig3Qiu4RxQW9oXXfddTGHI2b5xQknnDDffPOh7EzwAR/1uYaNNtro7LPP9lYWB8IgkUsJVynbQEEhx5SjWQHv5E+4+K677kpYBld9rCt4kc5bdNdcc40mDvmsCQquHOU85JXtuOOOW2WVVWjjjOUTQUakUU6+tkKgECgECoFCoBAoBMYRBLoS2aQPgwLyF96m8qSTTvJUXVLz85//PLKF6arECDGtQYEMlQyJxFlvueUWj/utYU0iNmlIVmQ6PZRX8M4TSse93//+97KVSOett96quewmKswr6U8P8ZFUzHvZZZfFO/np7M033yzTaXVEmCiCSNuvf/1r4YRc0infOf/880v0+vLXGmusQRsZtJKHxx9/vLP8Ce+kBK10thFuVNWhnC55Orlkrwms2MWP1WulOf+dDdprrbWWNQlIsLjIO0WJU1ivQICAf3PDKSlbCx64TQ8ZNfa1FQKFQCFQCBQChUAhME4h0JXIYnINCNQqxPF73/seCovLrrfeeghWsoASt0hVCJkmGoaBDYxdYWa4Gg2YqIUBuF3emgpHzB6DbClbjnHPY3d0EFVVn9UOrHMMv8QpUUafX1hxxRWRVOQSr73jjjsIS3wi6L66JVnrY17e4lp//fUTOL6LsMrjeqdt7bXXRiK5YY9i+gaC17OkaZnAiZPEZUJWOCjxhCQ3OKNe3hTjJMm0lQk46Morr6w+8IpFQchaHXvssYlLDRyefvrpfffd14+EMURAIAAnpgmHeagL6BEmc9FW+0KgECgECoFCoBAoBMYdBLoSWaxLBhEQKBR+9uabb1p+aqXm9ddf7/0k9RhVYMJic4gmYoFIFcqFQWriharI9H0ftfb4H9O4mrRlewSvhio0Dk10imPEHOKyKKCkLHrnELdDedOWP0ihrSVQKZFk9cqUJbA047jOPvrooz5Va+2sDxSopCdJVoeWw6KzlrGKixKLZUFB0gfI8Eg1FjNY6sAZXFarUHn+xGjSvfiuRLJY1L/66qt4sxqeNEbLW6cEIkB0mQzmbbkCV/FmpglLzV511VVHHnkkNkxVNNeigr6PrpIsBAqBQqAQKAQKgbEJga5EVpAYEj6ngPNJQHpn/6abbrJINDnLZCKRPBwO5UXa8CpsjDwyh9thsY2A9h2yZHPRUNqwOvq1ZYtROuVQFfA5Cwa4Rz8x7+9LppJBnT2aJ48HJ2cZuw4Jy2s65OoOO+wgo7z66qv7FEAqw7w98d9yyy2ZxhETiF9bkBL20StJWU2YtqGep5566hZbbKGV5oJFQJOaTUaWLeyWLZLR7yUw7nkDjLBTaDFD8S0NqdVKDQrLAYUsP0Bb8VcyTPDcUgdvp3nVjJg7Da2CSUsGR2ftC4FCoBAoBAqBQqAQGBcQ6LqwFYkMi7Ue1Fv2nrl7ZQqLxcPU77HHHn4lAXnFYvPQHPmDV2ioFasSmZ6Mh5z1C0fKk4v1tS8P8f00LhqNEarHYqlCXrNGVto1RonxkC3vn3FJthLtIy+pSRjbQ3yRv7iB//nkgpWvKCCWiYyqJ+xQdGixXK8arex9Gvbiiy9GTzlAkpgNobSHA26KmBJridWYw0QJAFATgJC0eiFvp5G3XGHvvfe2ZldDkhxWIM/tQw89VFKZtmgQb5wJq+a5tQebbrop+eBsT4aGEGj1tRUChUAhUAgUAoVAITDuINCVyMr5YUj77bcf4oWz+n0vyVHvSNm8XO9xPBaFwkIKn7NHCjG8ELvNNtvMw3crENT0F0p6sEmtOIAX+prVxhtv/Ktf/QoZVSmXueqqq3KMS9YMoJv4Yrig3x7zAwdSpxYMYLo27sloesd/6aWX9kO78ZYS8r/85S+//OUv+1Crl6jkcbHY888/37dpPc0PKcQa0UerJo4++mi28HKsFHXmhnSsVbaStVkFgYCioRa/sn7nnXdaekEnMspV73V5X83agIAADYT4sMMOo3a77baz4NhSDW05oNL7ZN/5znfoiYeouTUMfoqCNjX0WGBwwAEH/OIXv3DICpdgpS2dSeLGSu0LgUKgECgECoFCoBAYVxDo5QcREK8VVlgBoYQFzuRhfUBJjZ/1wmVtCC6SJ++I56Xmi1/8IklLC9DQ1PRrjxpGHk/1hN23BdBKdI1OplHqiy66KAKMppAfa8ALeYXgJkWKYZNHEC+55JIkRwlr4ksInuxbYEAhHpmFvKJTySIZ5NgeO/TbXX4OFxWmM7Hba+4jX0h2TNvLm26//fZNIIWgpGwVhIjQYpLWGNjjzT7CFcYcNqzsu7bOcq9p9lJd8riUiMjtAajvv//+6IdJcyBut8MqFAKFQCFQCBQChUAhMPYhgPB0ch4kdjz/ZPXCjQSsEGqVGntnUT2n0K9wL+Umo7IRXML4YpKy1gNYror+YmBNVV8K8qYIaPbInwUDEqjopo+/0olZUsJiO+VsLGKTVhQ4i4Ba20DeB8I4TFVWOIicHgI33HCDbLEFCSRRTE/5qcU7s1AhTlregCnKg/qFMEsOxEuh5/4qHeKXGKfoCJNMClmNw1R2QoRkd37VgQznKRQmP3F930MgwH994Wza8haYPLcoQhrYImBh9kBbstYPkomivyCzUlshUAgUAoVAIVAIFAIfLgRwJA43Foqg9kZkMTPMj3RIG8KEpUnThi82zoqWIVLOhi+GzzETSjcwgJrp0EQp0hilM0xcIWS3UcYcxtXspTbDTflMHnFsnC+sN22bGCtJo2K3WY/L+ZwVI2FU0t4WQswT9agttamxZzrJY0ZZDIAxh3NrC6tQavt0BiTjXkJQSSydRD9U6STc2Co9TEQm7jX5gaFdrQqBQqAQKAQKgUKgEBjzEUB4ONlXIjvmx1MeFgKFQCFQCBQChUAhUAiMIwgMS2S7vuw1jiBSYRYChUAhUAgUAoVAIVAIfEgRKCL7Ie24crsQKAQKgUKgECgECoFxHYEisuP6CKj4C4FCoBAoBAqBQqAQ+JAiUET2Q9px5XYhUAgUAoVAIVAIFALjOgJFZMf1EVDxFwKFQCFQCBQChUAh8CFFoIjsh7Tjyu1CoBAoBAqBQqAQKATGdQSKyI7rI6DiLwQKgUKgECgECoFC4EOKQFci6yP8vvPfGZWP8+fQB/w7C/mmV6fkCJab3aa5FXz/n3K/CJBfE1BwSL4JxHTqlYkJxKbQKdNiccovF6SVvYa2HsLq/VSBrWlohdawCoVAIVAIFAKFQCFQCBQCoxiB8bvZ81tTtqEk8H9wuE996lP52SqVfuDq/fff91NbCu33vbrpGUA9Q5iin20IrfSLVsrIpUJ+PIyAjeb8hpZ6XDY/88Bhv6eVegIRaz5Qm42ed999l6Sf/vLTXATY0qo1VPMv2X8KmWSUEGNITdNZhUKgECgECoFCoBAoBAqB0YJA14wsb/LLrqgqFou54osIHELplBqFN998U2UkB9H75EQpZI5+5FIhlBSJxKpT9uu1vCJMUg2ZSKrHQdXbEFwbdquGmOaN2k444YT5AVsBvvPOO8KJmIImrDgkH1VNAxMqCQxivKWqECgECoFCoBAoBAqBQmAACHTNyNKFrslWTjDBBMqYq8fxWF045XvvvfeZz3zmc5/7nFN//vOfZWcHYHu4TRhlBVl0FukM72S65WJRTKfI4JTccyr8NW6kUnOFHvpJ4qxrr722goZHH330bLPNRkZe2daEW4ypiR7sdlivWpMqFAKFQCFQCBQChUAhUAiMegR6sr1OD9BTnA9zvfzyy++777677757iimmWGihhbbbbjss9u2338ZxUcOQzs6GI1KmLbRSPhV99ExfYviJJ5649NJLpUUlaJMo5ZsC1jvvvPMutdRSPGluYJza8iGSbVUAATXLLLPMa6+99stf/lI6lljoafYijedhz8pMc0ZDNVFrP4Rcf+QjzdyIBFttC4FCoBAoBAqBQqAQKAQGjMB4k0wyiRUCaY+lKYSiedo+0UQTSX8uvvjijz/+OP76+c9//sEHH0Rnccdbbrml0yTGidt11gy43FQhnfyREPWC12OPPbbVVluhlSotCbj//vslg6eZZppXXnnlgAMOWG211SaffHIWW9tmHRGnJIsiUknDc889N/PMM7/66quf/vSns7qgydMQ7h7669Cp0OJGiNXQWUS2gVaFQqAQKAQKgUKgECgERgECnUyVOWywa0YWi3355ZdlXnfeeee11lpL5nXSSSfFAnHHWWedVUZz2223jcd//OMfMcLB8h4hDk1s+VFcc4YZZrj33ntTg2huuumm00033YEHHohcWhGbxQ98Q7utGej0JAsSKERGNU9uNTLW1yLxEcaVnVLPuj1VqQ9eobDKocUOm2+dtqpcCBQChUAhUAgUAoVAITAqEehKZDkx2WSTSYhijRKTqCqaKLWJzu66667nn3/+Jpts4vk+gjiILDaR44vs2nBHnFIBq84pSVlEE6t2CqHkUlvegLM6hdc+9NBDDzzwwBtvvDH77LNLJM8000xIarQJREMLEhxaM2D//PPPX3/99b5gYL2sNO2MM86IpArKwgmbejHOMsssNKP1N9xww+uvvz7nnHNSm/XBo7KrylYhUAgUAoVAIVAIFAKFQCcCXYkstoe25tm6PCVWN/HEE6vEFz2aRw0xPIlM6VLU0CHJrC5APVvCEiVNTrTT5AeWNUcxiYXFhn2GXyY5yrR8KgEe2nMjTFeeeJ999rFSQqpVyvaQQw5xdsMNNzzqqKM04XwYrT2qigTvu+++++2339xzz40fP/3004jsZZddhsvy2arc5ZZbLqlZa2q32Wab0047DaMV0QsvvIAcW1xhPUOCjZjwaWaxtkKgECgECoFCoBAoBAqBUYBA14Wt4YjImQ1Rw2JxOA49+uijZ5999oorrqict7KctaGzcTeMk7DKAbBYSsJiFZi2p4dOdFZ9iLUPFEQzoxhtWCxKutdee2222WY8lDe9+eabLfOVOVZYaaWVwrMFhWtitLLICyywwK233uqsDK4m1gEjxD/96U/jv5XBFuYirxyQgtXEhxpIPvzww0899ZR0rxfgrKlAiAnYtKKZqzmsfSFQCBQChUAhUAgUAoXAyEagK5FlGC2TtlRA4+wPPvjgb3/723PNNdf222+/2267OYVoymXil2icLWlO5FINoqlmYN4jnTZtKeRDKHVqQm2x5FSy5btgJC+55JJDDz2UV7KwFglwWFs8e9VVV5WOxTtPPPFEbbFeyxJklzXZcccdzzvvPF88UJbo1WqxxRbDUx0mIvlXeVyLCk499VSMFvfljAy0dCxJa3Yx1/DpJGI5EydpqK0QKAQKgUKgECgECoFCYGQj0JXIhrzmUT66Jsd51llnHXnkkZ6n33bbbb5d4BRCiedhhxHG//BLfM42In5jwNFwwQUX+OwXVRgk5ayEHGOooYwYpEWxBCwMwDst3lVGVQnjoDKmzuKyPrmFhSOyWK+2MqxOrbvuuripbxeErFuQIAtLP2pOieys6OhxuOSSSypkHQUBFNnqYVnh9pJZPORS6LXD2gqBQqAQKAQKgUKgECgERjYCXYksToa0IXN4aqjbddddh19edNFFyOvmm2/+zDPPJB+JHRImyVfMEl9UtmGftoEFkMzr4YcfLtVKQ/hrKCYfOBDqTCxs0oN+L2CFR6KqSZHiqTm7xhproKHe3NLqD0M3Da2jRXl9GRcjp9PLW5YioKeUx6LoyDikR41crKBokND1CTCYqNRQjLTlUE1thUAhUAgUAoVAIVAIFAKjBoGuRBal4wEyh8mFIEpDqvEI/qabbkLgPHB3GLFI2stKInaopIbYZwio+n5tlKOMmqCM0R9CHHqqjNFyKZXEkMuQUQLKQ0j03/+elC0ZyVfc1QpX7FOrz372s1YakJRk5SR5GqJZ+jYZaDUknXrppZfCnunhFXn1iPtbb72V0FhpTaJE29oKgUKgECgECoFCoBAoBEYBAl2JrEfzzEs0Nn4mqYnASU9ib1NPPbV0pofyxBTC9tDH8E4CCuoHFoDmMTrVVFNJo1ISzpr0Kpd8NwArxSyjH+Ncdtllr7nmGmwV0cQsMc6coofDXuqaf/751SRviolSiLY6G82hvziutKt6miVZqWJFpUwtSRsG7KyQ1ePBFDamTt4GgditfSFQCBQChUAhUAgUAoXAyEagK5GNYXQNP0MQHSJ5+QUB31695557UEOUEZeVf5W1xT4J45q43ZNPPnnllVf6mOsAvMcsacBEGZVJvfrqq9FWhsIR7QlIlKKnIbL2GOfGG29skevuu+/u0IaYcpt1+9/+9rcnnXSSz3LhxDx0NrEgpsqcR7hpYFFBjRCcwtqJoeN88FUEepB139B1lozKlqlFjm3N3ABCriaFQCFQCBQChUAhUAgUAgNAoDciiw760tYOO+xw7rnn+jkAiUnvVJ1wwgnSn4suuqg399lDAe3D5PA/TRyut956XqVaYYUV8n2AfrllSW7kkVff/MJiF1xwQZ8XSGr29ttvt+AV6cRcI4ZQ8sFS1yOOOOLiiy9eeOGFm7feSNtggw222GKLnXbaibfSqOip7CwlCs769paFB7isL21dddVVzz77rE/k3njjjb5IIFgrKMLFnaIZWcet1Uj9ykyzix/7xGy4dVtp0K9gS7gQKAQKgUKgECgECoFCYOAISLJKMWaTVbWljJ8pYHUo4zTTTIOksiEpi/b5/JacqLNYID6nIHMpUZqyQ0QWuZx22mmlZh32d+MDoqmVxKdPJfhoQKxzwDpdVPWMM86ITqxXwa/mxu1rr712tdVW87O6gYOrCPcpp5wSYaSTh05JrEbA3g8fOHvssccm95y2Pkb74IMPRgZpdkp9lhyIPYsunMWzfbkWiacBFNyIodoXAoVAIVAIFAKFQCFQCAw6AkN46r+YKuVI7Hj+eSgf0qZKobHGVGapK5ZmQ+n8jgBuh70RSxrS6gLZU8KtOYUSqJaiSpRGSd/3lMQBLJYGDZP0lS71qSxfzpIW9atdWYHQqVYyWFuecNhvFqDCxLiq0jpXkiLPAgCaU0MGNc/SAmJoLgE8FS8PW1UpOkqEI6EbJVlNEaIfEIhRDpwA0ulVlQuBQqAQKAQKgUKgECgEBgUBjIuexlR9cqorkQ3htQ+P1CyEVWIyL10hjk4RQOk8dqc6ekMxMV3ErlHGfnlPIe4YNhmjGKf0amioLCy2yq4akg5lRnvo55sNSe2s55hD+9DueOjQRjNvmzDaSrPsLyUt/HY27rXDzgKv4mRnZZULgUKgECgECoFCoBAoBEYcgX4Q2WYMz8PnsDeUrpMa5jE9CksS3QxxxDiTyxwYhaWKuc68ZpKm6jmg3hP8/CBtcy8FDmCiSKT0KoaaVC6XHPLQKQKCx4/Jq8SS0V+k3CFejoAS1lyM1vWGGbOImttU8kqTfFBWE7FTS496OjUMLS4Wm+6ofSFQCBQChUAhUAgUAoOOQL+JbMuzxhWpSrzNi1AWJIQU4oKYayNwjQUidiqRv6Rv+xUJo5gicil1muxpSwNHD/qoBq1sdsNE41JkENMkU2lztq2CcDY6FcI+FaR1cfSwYaxXvZqsJVBAi5tkW2PQGDb9/ElCF89uZDdu1L4QKAQKgUKgECgECoFCYFAQQLrokWSMtg9YWhCaKO9ow1bDC5sfnTlOrFH+MnlQnI+Bzqxqa9KXQqOMSZE2PVnAioMixzFEWwiuU3hk3OMq6xht2LC8aaLlrXCI8dNZyulBVXFQX4e1lldDaxXobAxVUjnZZUaJURi22mS4aotdhU4a3ZdIS6YQKAQKgUKgECgECoFCoO8I9IPIEm2JxmZAehIvDL1zNiwThwvDC0dMPUkkstHQpqEvheiRRkU9G79MQ15xACtFLsNiU5OzLQvbfFNPIKlWzqS+82z0Z6kA4bBYHFc5YUZzrER/Vk0oU9hSwhFrUOSw9oVAIVAIFAKFQCFQCBQCg4UAPkZVcpQKvWVkB8vkmKYHi4VCmDcyyr0emeYxzeHypxAoBAqBQqAQKAQKgUIAAsMS2d5+EGFshSwsFqPF6LFYqV+Z1LE12IqrECgECoFCoBAoBAqBsRWBcY7Ioq36MgsDkppuCdqxtY8rrkKgECgECoFCoBAoBMZKBMY5IpuFBElNW4yrU1MeK3u3gioECoFCoBAoBAqBQmAsRmCcI7L6sr2SleyslQZe9hqL+7hCKwQKgUKgECgECoFCYKxEYFwksu1rAz7jlXSsDyyMlb1bQRUChUAhUAgUAoVAITAWIzDOEVnp2CyN9bKXfj3zzDPPOOOMsbiDK7RCoBAoBAqBQqAQKATGVgTG8xtdb775ZsJLerJ9nWusjDmfpxWpzVdg55tvPi9+Pfroo2NlsBVUIVAIFAKFQCFQCBQCYw0CyJtYGlP1Hdnxe4ntvffe+8xnPkPADwT4dQCE78knn7zgggtmnXXW9dZbr/2IAIH8QkEvqvp7Kj8Me+yxx7722msTTzwx0365wGJWAeyxxx7tVxL44OUt9fEwJJUwsfwGGLfbzx+kVX7CwA8f5PcO/KjsZz/72ayabVGw7me62mth8Ip+asnDhDxbTHTG1eN3dDtPVbkQKAQKgUKgECgECoFCYNAR6EpksTqMDZdF6XC+t956S+52rbXW8rtWK6ywwpe+9KWsK0UTUcMelG5EvHz33Xf9cqzVq7jmQw89dOutt/Lhueeem2uuuZBIHHrXXXf1W7LO4q98yAoBZUbxS9snP/nJOBCqmh/0omSiiSZSLy4c1CmSePCee+6pkObN7fYTuDgxOpvYnUVkYcKigiYa0sZcKHKz2/RUoRAoBAqBQqAQKAQKgUJg5CHwwUsLkDZEDWXcaaed7r777qmmmgpjO/nkk/mUFGZL8A6Wlwgizop3ooxI5+WXX77++usjuHxIkrhlZC2KkFV+/fXXJ5tsMtZ9Tgu/RE8x3UZtOYnLouPqQ76bn6lJFKlMTjdl1nmioUqMNhqoRW3zI7pNT/OnU1U7W4VCoBAoBAqBQqAQKAQKgRFHACmlpDHPD1hakOf7pPG5G2644bjjjnvssceOOeaYp556iiL1uB11yogd7jji/tEQLpjsKQLKNN7MVjiohCgZOWD1OGXyo43Fxp8nnnjizjvvTGZ3tdVWm3766fFd6WStQmqpPe+88+add17ZZc7zfKaZZgr3JYO2Um6vhiRz1hWce+65hFmcY445Fl98cesrZG0pV0M4gVM1iJnp6Kx9IVAIFAKFQCFQCBQChUA3BLouLZCtxNVCK3E16di9994b4UMK3377bYSvaQzna4cjWMAF0WL6JUFRWOlPZTXyo1xyiK2SydKCFIi98cYblrpefPHFX/nKV5zip0qEe/vtt99xxx2PPPJIXoXmKrz44os77LCDBQM8R5EXWmghxJdyrFQNc6yoJ6nSSoZzzjnnnXfemXnmmblkzS6CO8UUU6y88soWWhADUUg2vtvWJIwgCNW8ECgECoFCoBAoBAqBQuADEfg/PtpDFKtTE1qJxaJxu+++O5436aSTSuQ6JQGJ52GNSVv2aD4ihxgnKomJUuKpfdKozKGJTlldYOFBzjqlxp5Xhx566EYbbfSNb3wD5X3ggQfuuecevp166qknnnjinHPOyVViWaswzTTT3HLLLRbgWnr74x//GKNlKGQUVVUOiyV81VVXnX766UcddZS2Er0S0l53O/zww1966SXp6ohxNcECJ4XaFwKFQCFQCBQChUAhUAiMAgS6ZmQbP/NU/brrrvPSFWorWykdi1zyTJq2LTntsWZ0BP3GWZHC8GOElVGckolUZnUBE3gqFotNYtu33XbbPvvs86Mf/Wi33XZDRr3LpR5D3WSTTRZeeOEFF1zw0ksvXWeddeh0ln68PE5++tOfbmsD1LBlT0b4uO+9997r+1zrrruu2FXKxU499dTo8tlnny0pCwoK44OGVEVn7QuBQqAQKAQKgUKgECgERgECXTOycpCo2wsvvIAgHnjggVaaInO4HdYYKpkEJBleDmJSllocNArxV8rxTnvclOmQztQrE8MgNbniiiukXbFYiVIc1CmuWheL7M4wwwybb775QQcdpJXNWTld0QXcl19+GTcln0BC33NWAphFidvLLruMQE4pqDz++OO32GIL3wVDXrF5btBMIbYdtbUvBAqBQqAQKAQKgUKgEBjZCHTNyKJruJ1FBdabWgxqbShCqdJqUYtBJSPRR2sMQg1RT2ftuas+1HNg7FZz3NQ3WdmS7KQwixwU8MuYoBmhtHhgxRVXxFxJ/v73v19sscU4g33mywZ4p8yxhC7J5Zdf3gqB5k/7Dhd5ZFdETISJss5QrAjf+lpJ2bXXXptar3nZY66WMSyzzDJrrrkmSa4KttFcNbUVAoVAIVAIFAKFQCFQCIwaBLoSWeQMS7vooovspR7jDXYoM+qDAB6sy3Ref/31U045JfInE4kySn8qY3v5HFW+jdXfMEJbJ5hggjTEMvHXVCKsqWSoldnFO31SIJlRwhLGYbdWApDnDOatSSOymC790UDGFu7bFkjQqUzeF22trODDjTfeeN9991GF71osu/XWW8vyHnLIIYIVcpK1bIV5x8naFwKFQCFQCBQChUAhUAiMVAS6ElnMj2EP1rE0pBal8xEAH7raZZddPLX3BYPZZpsNiyWD6Upe5net8mw91FC+lhKksF8BUIV3MoeS4poKWcaqElUNvVaP2oY+5sUvP9CAVmqrUrYY825kl7CPEsih+rIB/koJ9slnXpFXiZ5msYS20qsaIusoKf5qTYWvEyyyyCKSsn7MDGe1Me0dsk033fQXv/gFshtVIcGQob9f8ZZwIVAIFAKFQCFQCBQChcDAEOi6RhZ9tM0999z4qF/VQvKWXXbZ2WefXSLWbyKstNJKSCHOJ/eJwyGCIaxZVOBBv0Tm008/PYAMJUPIq2AQUNpkWxFNh8glyugsHmlTI+Nrj1mS2XDDDX1JwHcVlEmqb6bPOuuso48+2ne4LAmgExXGPrUiE23J1KK/9Kd5LFqBcPXVV/ukF39YVEnAhq3axzRtmjRzxWJBUVshUAgUAoVAIVAIFAKjBoGuGdkkFz2yz4pSX2nF6mRJ0VNfA5DjlIJFZ3kZ8ooOtnzkZpttdtddd2GN2KH0Z38jkSil03bHHXdgij6VhZX6EpayUzglWwp+ZgzFRFuxUiT1kksu+eIXv/jggw/uscceyLfvElhEa2msHyFT48cLOExYBveVV17x4S06UfDHH3+cKj8eJkwf7eLwBhtsoF6keDyLSPDzzz/v1S7BOrRW2IKKH/7wh14sE2AIsfUGXEoKub/BlnwhUAgUAoVAIVAIFAKFwMAQ6PoTtSha8o70onfWFayyyirWkjqUyET+cEev/KOV8rLWEqhHZJMN9aAfrcQmvSnlo6398kziM9o82ff4/pprrkEQWUEi6cEX4xUf1DhlyYFksOwsJsrJcGgcFGEl78cOME7fz1KmJJ54hcuqgJRDZ5t+GWgUvCVWfbeLBkTW18fkntPE+mAK991337Z0OD4TGABrj87aFwKFQCFQCBQChUAhUAj0joAsJ4E8DFdANbsS2SiSvJT1xBTxWs0Qxzyyd0gXWpnXpBrrbTRUZnTWWWclgHH27tOwZ3FTOU5kNPlX+dFQ20g6dBadxT5tKskn1apJ0qKsI7K+IItcopuaxO0s5H311VdVyqeqtxdXwyWSKLJAsFJJaKspfJ+LzOuvvy6Pi+M6TJaaGwIXo7O2YQOpmkKgECgECoFCoBAoBAqBwUKgEbYo7I3IomhIYUu1InAyr1kIqx49Rd0QTYU8WG9k0WG0Y3gj4jcHqGo8WBl5ZRTLDH+VAGZCPd+SDbXmwee0BKkyMhzIggeV4Zrxk/LU0NlWRKh06JsMuLuGOUzUWpGHhr2NWHMjIEShcifnHpHwq20hUAgUAoVAIVAIFAKFQCcCOJjDlj3sjch2NqtyIVAIFAKFQCFQCBQChUAhMHoRGJbIdv1qweh1tKwXAoVAIVAIFAKFQCFQCBQCvSNQRLZ3fOpsIVAIFAKFQCFQCBQChcAYikAR2TG0Y8qtQqAQKAQKgUKgECgECoHeESgi2zs+dbYQKAQKgUKgECgECoFCYAxFoIjsGNox5VYhUAgUAoVAIVAIFAKFQO8IFJHtHZ86WwgUAoVAIVAIFAKFQCEwhiJQRHYM7ZhyqxAoBAqBQqAQKAQKgUKgdwTG9yX/fNLfp7nyOwL2fmggX+rqvXGdLQQKgUKgECgECoFCoBAoBEY2An4EIQTVL1LlR1XRVyR2fD/r6tezlHiQX0pQzm9ojWyfSn8hUAgUAoVAIVAIFAKFQCHwgQhIsCKoobDKNvQViR1PSwctC4vLth9f/UClJVAIFAKFQCFQCBQChUAhUAiMAgRCULHWlp1VGH/aaadl+09/+tOnPvUpdPZjH/vY+OOP/49//MN+FPhUJgqBQqAQKAQKgUKgECgECoHeEcBiP/rRj5LBUeVl33///QknnBCJHQ9h/fvf/47S2v72t799/OMfJ5QEbe8a62whUAgUAoVAIVAIFAKFQCEwChBo1DRkdcjagn/+c0j6daaZZnrrrbcmnXRSTlh88Je//OUTn/iEsgajwK0yUQgUAoVAIVAIFAKFQCFQCHwgApjrX//614hJv7755ptI7JCM7Oyzz/7MM89grhNNNFHStlYa1NKCDwS0BAqBQqAQKAQKgUKgECgERgEClhZMMMEEDFla8M4771hdMOOMMz7++OP/H9LfWgngRXeqAAAAAElFTkSuQmCC\n",
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarize Original Text, Tables and Images."
      ],
      "metadata": {
        "id": "1adhusrLqtqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema.messages import HumanMessage, AIMessage, SystemMessage\n",
        "\n",
        "chain_gpt_4o_mini = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1024)\n",
        "#chain_gpt_4o = ChatOpenAI(model=\"gpt-4o\", max_tokens=1024)\n",
        "\n",
        "# Function for text summaries\n",
        "def summarize_text(text_element):\n",
        "    prompt = f\"Summarize the following text:\\n\\ntext:{text_element}\\n\\nSummary:\"\n",
        "    response = chain_gpt_4o_mini.invoke([HumanMessage(content=prompt)])\n",
        "    return response.content\n",
        "\n",
        "# Function for table summaries\n",
        "def summarize_table(table_element):\n",
        "    prompt = f\"Summarize the following table:\\n\\ntable:{table_element}\\n\\nSummary:\"\n",
        "    response = chain_gpt_4o_mini.invoke([HumanMessage(content=prompt)])\n",
        "    return response.content\n",
        "\n",
        "# Function for image summaries\n",
        "def summarize_image(encoded_image):\n",
        "    prompt = [\n",
        "        SystemMessage(content=\"You are a bot that is good at analyzing images.\"),\n",
        "        HumanMessage(content=[\n",
        "            {\"type\": \"text\", \"text\": \"Describe the contents of this image.\"},\n",
        "            {\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\n",
        "                    \"url\": f\"data:image/jpeg;base64,{encoded_image}\" # data:[MIME type];base64,[Base64 encoded data]\n",
        "                },\n",
        "            },\n",
        "        ])\n",
        "    ]\n",
        "    response = chain_gpt_4o_mini.invoke(prompt)\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "QkfYtwUJqov4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mp29j49mq-9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "UJmFJHZkg2GS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0baf5a8-824d-46bb-b3fa-789c754fe35e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1th element of tables processed.\n",
            "2th element of tables processed.\n",
            "3th element of tables processed.\n",
            "4th element of tables processed.\n",
            "5th element of tables processed.\n",
            "6th element of tables processed.\n",
            "7th element of tables processed.\n",
            "8th element of tables processed.\n",
            "9th element of tables processed.\n",
            "10th element of tables processed.\n",
            "11th element of tables processed.\n",
            "12th element of tables processed.\n",
            "13th element of tables processed.\n",
            "14th element of tables processed.\n",
            "15th element of tables processed.\n",
            "16th element of tables processed.\n",
            "17th element of tables processed.\n",
            "18th element of tables processed.\n",
            "19th element of tables processed.\n",
            "20th element of tables processed.\n",
            "21th element of tables processed.\n",
            "22th element of tables processed.\n",
            "23th element of tables processed.\n",
            "24th element of tables processed.\n",
            "25th element of tables processed.\n",
            "26th element of tables processed.\n",
            "27th element of tables processed.\n",
            "28th element of tables processed.\n",
            "29th element of tables processed.\n",
            "30th element of tables processed.\n",
            "31th element of tables processed.\n",
            "32th element of tables processed.\n",
            "33th element of tables processed.\n",
            "34th element of tables processed.\n",
            "35th element of tables processed.\n",
            "36th element of tables processed.\n"
          ]
        }
      ],
      "source": [
        "# Processing table elements with feedback\n",
        "table_summaries = []\n",
        "for i, te in enumerate(table_elements):\n",
        "    summary = summarize_table(te)\n",
        "    table_summaries.append(summary)\n",
        "    print(f\"{i + 1}th element of tables processed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JXwIckq1g2GS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a562c349-fc49-4432-d491-2123357b555a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1th element of texts processed.\n",
            "2th element of texts processed.\n",
            "3th element of texts processed.\n",
            "4th element of texts processed.\n",
            "5th element of texts processed.\n",
            "6th element of texts processed.\n",
            "7th element of texts processed.\n",
            "8th element of texts processed.\n",
            "9th element of texts processed.\n",
            "10th element of texts processed.\n",
            "11th element of texts processed.\n",
            "12th element of texts processed.\n",
            "13th element of texts processed.\n",
            "14th element of texts processed.\n",
            "15th element of texts processed.\n",
            "16th element of texts processed.\n",
            "17th element of texts processed.\n",
            "18th element of texts processed.\n",
            "19th element of texts processed.\n",
            "20th element of texts processed.\n",
            "21th element of texts processed.\n",
            "22th element of texts processed.\n",
            "23th element of texts processed.\n",
            "24th element of texts processed.\n",
            "25th element of texts processed.\n",
            "26th element of texts processed.\n",
            "27th element of texts processed.\n",
            "28th element of texts processed.\n",
            "29th element of texts processed.\n",
            "30th element of texts processed.\n",
            "31th element of texts processed.\n",
            "32th element of texts processed.\n",
            "33th element of texts processed.\n",
            "34th element of texts processed.\n",
            "35th element of texts processed.\n",
            "36th element of texts processed.\n",
            "37th element of texts processed.\n",
            "38th element of texts processed.\n",
            "39th element of texts processed.\n",
            "40th element of texts processed.\n",
            "41th element of texts processed.\n",
            "42th element of texts processed.\n",
            "43th element of texts processed.\n",
            "44th element of texts processed.\n",
            "45th element of texts processed.\n",
            "46th element of texts processed.\n",
            "47th element of texts processed.\n",
            "48th element of texts processed.\n",
            "49th element of texts processed.\n",
            "50th element of texts processed.\n",
            "51th element of texts processed.\n",
            "52th element of texts processed.\n",
            "53th element of texts processed.\n",
            "54th element of texts processed.\n",
            "55th element of texts processed.\n",
            "56th element of texts processed.\n",
            "57th element of texts processed.\n",
            "58th element of texts processed.\n",
            "59th element of texts processed.\n",
            "60th element of texts processed.\n",
            "61th element of texts processed.\n",
            "62th element of texts processed.\n",
            "63th element of texts processed.\n",
            "64th element of texts processed.\n",
            "65th element of texts processed.\n",
            "66th element of texts processed.\n",
            "67th element of texts processed.\n",
            "68th element of texts processed.\n",
            "69th element of texts processed.\n",
            "70th element of texts processed.\n",
            "71th element of texts processed.\n",
            "72th element of texts processed.\n",
            "73th element of texts processed.\n",
            "74th element of texts processed.\n",
            "75th element of texts processed.\n",
            "76th element of texts processed.\n",
            "77th element of texts processed.\n",
            "78th element of texts processed.\n",
            "79th element of texts processed.\n",
            "80th element of texts processed.\n",
            "81th element of texts processed.\n",
            "82th element of texts processed.\n",
            "83th element of texts processed.\n",
            "84th element of texts processed.\n",
            "85th element of texts processed.\n",
            "86th element of texts processed.\n",
            "87th element of texts processed.\n",
            "88th element of texts processed.\n",
            "89th element of texts processed.\n",
            "90th element of texts processed.\n",
            "91th element of texts processed.\n",
            "92th element of texts processed.\n",
            "93th element of texts processed.\n",
            "94th element of texts processed.\n",
            "95th element of texts processed.\n",
            "96th element of texts processed.\n",
            "97th element of texts processed.\n",
            "98th element of texts processed.\n",
            "99th element of texts processed.\n",
            "100th element of texts processed.\n",
            "101th element of texts processed.\n",
            "102th element of texts processed.\n",
            "103th element of texts processed.\n",
            "104th element of texts processed.\n",
            "105th element of texts processed.\n",
            "106th element of texts processed.\n",
            "107th element of texts processed.\n",
            "108th element of texts processed.\n",
            "109th element of texts processed.\n",
            "110th element of texts processed.\n",
            "111th element of texts processed.\n",
            "112th element of texts processed.\n",
            "113th element of texts processed.\n",
            "114th element of texts processed.\n",
            "115th element of texts processed.\n",
            "116th element of texts processed.\n",
            "117th element of texts processed.\n",
            "118th element of texts processed.\n",
            "119th element of texts processed.\n",
            "120th element of texts processed.\n",
            "121th element of texts processed.\n",
            "122th element of texts processed.\n",
            "123th element of texts processed.\n",
            "124th element of texts processed.\n",
            "125th element of texts processed.\n",
            "126th element of texts processed.\n",
            "127th element of texts processed.\n",
            "128th element of texts processed.\n",
            "129th element of texts processed.\n",
            "130th element of texts processed.\n",
            "131th element of texts processed.\n",
            "132th element of texts processed.\n",
            "133th element of texts processed.\n",
            "134th element of texts processed.\n",
            "135th element of texts processed.\n",
            "136th element of texts processed.\n",
            "137th element of texts processed.\n",
            "138th element of texts processed.\n",
            "139th element of texts processed.\n",
            "140th element of texts processed.\n",
            "141th element of texts processed.\n",
            "142th element of texts processed.\n",
            "143th element of texts processed.\n",
            "144th element of texts processed.\n",
            "145th element of texts processed.\n",
            "146th element of texts processed.\n",
            "147th element of texts processed.\n",
            "148th element of texts processed.\n",
            "149th element of texts processed.\n",
            "150th element of texts processed.\n",
            "151th element of texts processed.\n",
            "152th element of texts processed.\n",
            "153th element of texts processed.\n",
            "154th element of texts processed.\n",
            "155th element of texts processed.\n",
            "156th element of texts processed.\n",
            "157th element of texts processed.\n",
            "158th element of texts processed.\n",
            "159th element of texts processed.\n",
            "160th element of texts processed.\n",
            "161th element of texts processed.\n",
            "162th element of texts processed.\n",
            "163th element of texts processed.\n",
            "164th element of texts processed.\n",
            "165th element of texts processed.\n",
            "166th element of texts processed.\n",
            "167th element of texts processed.\n",
            "168th element of texts processed.\n",
            "169th element of texts processed.\n",
            "170th element of texts processed.\n",
            "171th element of texts processed.\n",
            "172th element of texts processed.\n",
            "173th element of texts processed.\n",
            "174th element of texts processed.\n",
            "175th element of texts processed.\n",
            "176th element of texts processed.\n",
            "177th element of texts processed.\n",
            "178th element of texts processed.\n",
            "179th element of texts processed.\n",
            "180th element of texts processed.\n",
            "181th element of texts processed.\n",
            "182th element of texts processed.\n",
            "183th element of texts processed.\n",
            "184th element of texts processed.\n",
            "185th element of texts processed.\n",
            "186th element of texts processed.\n",
            "187th element of texts processed.\n",
            "188th element of texts processed.\n",
            "189th element of texts processed.\n",
            "190th element of texts processed.\n",
            "191th element of texts processed.\n",
            "192th element of texts processed.\n"
          ]
        }
      ],
      "source": [
        "# Processing text elements with feedback\n",
        "text_summaries = []\n",
        "for i, te in enumerate(text_elements):\n",
        "    summary = summarize_text(te)\n",
        "    text_summaries.append(summary)\n",
        "    print(f\"{i + 1}th element of texts processed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "nwCRdxAog2GT"
      },
      "outputs": [],
      "source": [
        "# Processing image elements with feedback\n",
        "image_summaries = []\n",
        "for i, ie in enumerate(image_elements):\n",
        "    summary = summarize_image(ie)\n",
        "    image_summaries.append(summary)\n",
        "    print(f\"{i + 1}th element of images processed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-vector retriever\n"
      ],
      "metadata": {
        "id": "joZqYRM6rV3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.schema.document import Document\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Initialize the vector store and storage layer\n",
        "vectorstore = Chroma(#collection_name=\"summaries\",\n",
        "                     embedding_function=OpenAIEmbeddings(model=\"text-embedding-3-large\",\n",
        "                                                         dimensions=3072),\n",
        "                     persist_directory=\"/content/drive/MyDrive/GENAI-LLM/Projects/Multimodel RAG chatbot with Artificial Intelligence with Python\") # /content/drive/MyDrive/vectorstore\n",
        "store = InMemoryStore()\n",
        "id_key = \"doc_id\"\n",
        "\n",
        "# Initialize the retriever\n",
        "retriever = MultiVectorRetriever(vectorstore=vectorstore,\n",
        "                                 docstore=store,\n",
        "                                 id_key=id_key)"
      ],
      "metadata": {
        "id": "QFSqhJQErWUo"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading Summary Documents to Vectorstore and Original Documents to Docstore"
      ],
      "metadata": {
        "id": "_rTfU1JfsMmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid #Universally Unique Identifier\n",
        "\n",
        "def add_documents_to_retriever(summaries, original_contents):\n",
        "    doc_ids = [str(uuid.uuid4()) for _ in summaries]\n",
        "    summary_docs = [\n",
        "        Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "        for i, s in enumerate(summaries)\n",
        "    ]\n",
        "\n",
        "    original_docs = [\n",
        "        Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "        for i, s in enumerate(original_contents)\n",
        "    ]\n",
        "\n",
        "    # The original data corresponding to each summary was indexed with the same doc_id.\n",
        "    retriever.vectorstore.add_documents(summary_docs)          # The summaries are sent to vectorstore.\n",
        "    retriever.docstore.mset(list(zip(doc_ids, original_docs)))"
      ],
      "metadata": {
        "id": "O7VoxxC3sOQY"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add text summaries to vectorstore and the original text corresponding to each summary is sent to docstore\n",
        "add_documents_to_retriever(text_summaries, text_elements)\n",
        "\n",
        "# Add table summaries to vectorstore and the original table corresponding to each summary is sent to docstore\n",
        "add_documents_to_retriever(table_summaries, table_elements)\n",
        "\n",
        "# Add image summaries to vectorstore and the original image corresponding to each summary is sent to docstore\n",
        "#add_documents_to_retriever(image_summaries, image_elements)"
      ],
      "metadata": {
        "id": "9I1-faCPsbnn"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and Load Docstore and Vectorstore"
      ],
      "metadata": {
        "id": "zQvYrSOJsh6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get docstore\n",
        "docstore_store= retriever.docstore\n",
        "docstore_store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk0QtlXksiXx",
        "outputId": "bb6ba154-34a7-40b6-ed8d-329459071f37"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_core.stores.InMemoryStore at 0x78e444fc9f30>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the docstore to colab drive/google drive\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/GENAI-LLM/Projects/Multimodel RAG chatbot with Artificial Intelligence with Python/docstore.pickle', 'wb') as f: #/content/drive/MyDrive/docstore.pickle'\n",
        "    pickle.dump(docstore_store, f)"
      ],
      "metadata": {
        "id": "u8WMhcYIspPR"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the docstore from colab drive/google drive\n",
        "\n",
        "with open('/content/drive/MyDrive/GENAI-LLM/Projects/Multimodel RAG chatbot with Artificial Intelligence with Python/docstore.pickle', 'rb') as f: #/content/drive/MyDrive/docstore.pickle'\n",
        "    loaded_docstore = pickle.load(f)\n",
        "loaded_docstore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4X0j7JcsyHf",
        "outputId": "3368476d-be30-4843-f407-1a23ac591530"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_core.stores.InMemoryStore at 0x78e46c5ea3e0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_vecstore=Chroma(persist_directory=\"/content/drive/MyDrive/GENAI-LLM/Projects/Multimodel RAG chatbot with Artificial Intelligence with Python/vectorstore\", ## /content/drive/MyDrive/vectorstore\n",
        "                       embedding_function=OpenAIEmbeddings(model=\"text-embedding-3-large\",\n",
        "                                                           dimensions=3072))"
      ],
      "metadata": {
        "id": "QfGglDXIsyfJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_retriever = MultiVectorRetriever(vectorstore=loaded_vecstore,\n",
        "                                     docstore=loaded_docstore,\n",
        "                                     id_key=\"doc_id\")"
      ],
      "metadata": {
        "id": "eyNujLcSs8IX"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval\n",
        "\n",
        "Get relevant documents for question:"
      ],
      "metadata": {
        "id": "aJ-U6pVdtG2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#retriever.search_kwargs[\"k\"]=5\n",
        "\n",
        "# By default, MultiVectorRetriever returns the 4 most relevant/similar contents to the query, but you can change this number as above."
      ],
      "metadata": {
        "id": "CTmJuWIes9fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"When Do You Need Regression? explain\"\n",
        "\n",
        "retriever.vectorstore.similarity_search(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmt3Rq8XtLkb",
        "outputId": "6d0ac738-7795-4599-c1c3-8f155dfee84e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'doc_id': '3cb19bb5-57b0-47e9-ba74-6d6c5c50ba4b'}, page_content='The text outlines key learning outcomes related to regression analysis, including the application of regression models using sklearn and keras, data processing techniques, feature selection, and data cleaning to improve model accuracy, specifically RMSE. It explains the concept of regression, which examines relationships among variables, using examples like employee salaries based on features such as experience and education, or house prices based on various characteristics. The text distinguishes between dependent and independent variables, emphasizing that regression is used to analyze how phenomena influence one another and to forecast outcomes based on new predictors. It also notes that regression does not imply causation and warns against confusing correlation with causation. Lastly, it provides a link to a dataset for practical application.'),\n",
              " Document(metadata={'doc_id': '0b0adc52-b3e3-4fa5-86a9-21a5922c5d2d'}, page_content='Logistic regression is preferred over linear regression for classification problems, such as distinguishing between malignant and benign tumors based on size. Linear regression can misclassify data, especially in the presence of outliers, by fitting a line that does not accurately separate the classes. For example, a threshold of 0.5 may not effectively differentiate between classes if outliers are present. In contrast, logistic regression employs the sigmoid function to produce outputs between 0 and 1, representing probabilities. This method is more robust against outliers and provides a clearer decision boundary for classification tasks. The logistic function is defined mathematically and results in an S-shaped curve, effectively handling classification challenges by mapping linear outputs into the appropriate probability range.'),\n",
              " Document(metadata={'doc_id': '7c6bba50-ecf1-44c9-bf07-a45cc703c845'}, page_content=\"Decision tree regression involves making predictions by sequentially asking True/False questions based on the data, with the model determining both the order and content of the questions. The model's accuracy is influenced by how it splits data, using mean squared error (MSE) as the criterion for regression trees. To create a decision tree, the model evaluates various variables and their values to identify splits that yield the best reduction in MSE. Training continues until a stopping condition is met, such as reaching a maximum depth or when leaf nodes contain only a single sample. The text also includes a code snippet for training a decision tree regressor and calculating the root mean squared error (RMSE) for both training and test sets, demonstrating the model's performance with specific RMSE values for each set.\"),\n",
              " Document(metadata={'doc_id': '8b5c318b-17dd-45c9-95f9-417b0af8671c'}, page_content='The text discusses two main topics: data wrangling using the `pd.melt` function in pandas and regression analysis in statistics. The `pd.melt` function is used to reshape data from a wide format to a long format, specifying which columns to use as value variables. In regression analysis, various techniques are described for estimating relationships between dependent and independent variables. Linear regression is highlighted as a foundational method using historical data, while Decision Tree regression utilizes conditional rules for predictions. Random Forests enhance this by aggregating multiple decision trees through majority voting. Neural Networks mimic brain function by learning from data and adjusting weights to minimize prediction errors. Effective data processing techniques like ranking feature importance and outlier removal can enhance model predictions.')]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_search= retriever.invoke(query) #retriever_new\n",
        "doc_search\n",
        "\n",
        "# get four most relevant/similar documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UsGFWVHtdi1",
        "outputId": "82c8ab34-bc60-4101-a954-729744302e86"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'doc_id': '3cb19bb5-57b0-47e9-ba74-6d6c5c50ba4b'}, page_content='Learning outcomes:\\n\\nLearn and apply basic models for regression tasks using sklearn and keras. • Learn data processing techniques to achieve better results. • Learn how to use simple feature selection techniques to improve our model. • Data cleaning to help improve our model’s RMSE\\n\\nRegression looks for relationships among variables. For example, you can observe several employees of some company and try to understand how their salaries depend on the features, such as experience, level of education, role, city they work in, and so on.\\n\\nThis is a regression problem where data related to each employee represent one observation. The presumption is that the experience, education, role, and city are the independent features, and the salary of the employee depends on them.\\n\\nSimilarly, you can try to establish a mathematical dependence of the prices of houses on their areas, numbers of bedrooms, distances to the city center, and so on. Generally, in regression analysis, you usually consider some phenomenon of interest and have a number of observations. Each observation has two or more features. Following the assumption that (at least) one of the features depends on the others, you try to establish a relation among them.\\n\\nThe dependent features are called the dependent variables, outputs, or responses.\\n\\n© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022 T. T. Teoh, Z. Rong, Artiﬁcial Intelligence with Python, Machine Learning: Foundations, Methodologies, and Applications, https://doi.org/10.1007/978-981-16-8615-3_10\\n\\n163\\n\\n164\\n\\n10 Regression\\n\\nThe independent features are called the independent variables, inputs, or predic- tors.\\n\\nRegression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, brand, and so on.\\n\\nIt is a common practice to denote the outputs with x and inputs with y. If there are two or more independent variables, they can be represented as the vector x = (x1, . . . , xr ), where r is the number of inputs.\\n\\nWhen Do You Need Regression? Typically, you need regression to answer whether and how some phenomenon inﬂuences the other or how several variables are related. For example, you can use it to determine if and to what extent the experience or gender impacts salaries.\\n\\nRegression is also useful when you want to forecast a response using a new set of predictors. For example, you could try to predict electricity consumption of a household for the next hour given the outdoor temperature, time of day, and number of residents in that household.\\n\\nRegression is used in many different ﬁelds: economy, computer science, social sciences, and so on. Its importance rises every day with the availability of large amounts of data and increased awareness of the practical value of data.\\n\\nIt is important to note is that regression does not imply causation. It is easy to ﬁnd examples of non-related data that, after a regression calculation, do pass all sorts of statistical tests. The following is a popular example that illustrates the concept of data-driven “causality.”\\n\\nMl Number of people who drowned by falling into a pool Films Nicolas Cage appeared in 999 2000 200 3002 2003 2008 2008 2006 2007 2008 2009 -@ Nicholas Cage -* Swimming pool drownings\\n\\nIt is often said that correlation does not imply causation, although, inadvertently, we sometimes make the mistake of supposing that there is a causal link between two variables that follow a certain common pattern\\n\\n10 Regression\\n\\nDataset: “Alumni Giving Regression (Edited).csv” You can obtain the dataset from this link:\\n\\nhttps: //www.dropbox.com/s/veak3ugc4wj 91luz/Alumni%20Giving + %20Regression%20%28Edited%29.csv?d1=0.\\n\\nAlso, you may run the following code in order to download the dataset in google colab:'),\n",
              " Document(metadata={'doc_id': '0b0adc52-b3e3-4fa5-86a9-21a5922c5d2d'}, page_content='11.1 Logistic Regression\\n\\nWhy not use linear regression?\\n\\nSuppose we have a data of tumor size vs. its malignancy. As it is a classiﬁcation problem, if we plot, we can see, all the values will lie on 0 and 1. And if we ﬁt best found regression line, by assuming the threshold at 0.5, we can do line pretty reasonable job.\\n\\n(Yes) 1\\n\\nMalignant\\n\\n(No) 0\\n\\nTumor Size \\n\\nWe can decide the point on the x axis from where all the values lying to its left side are considered as negative class and all the values lying to its right side are positive class.\\n\\nBut what if there is an outlier in the data. Things would get pretty messy. For example, for 0.5 threshold,\\n\\nIf we ﬁt best found regression line, it still will not be enough to decide any point by which we can differentiate classes. It will put some positive class examples into negative class. The green dotted line (Decision Boundary) is dividing malignant tumors from benign tumors, but the line should have been at a yellow line that is clearly dividing the positive and negative examples. So just a single outlier\\n\\n11.1 Logistic Regression\\n\\n189\\n\\nThreshold : 0.5  Tumor Size \\n\\n(Yes) 1  Malignant ?  (No) 0  Threshold : 0.5  Tumor Size  Negative Class Positive Class  (Yes) 1  (No) 0  Tumor Size  Negative Class Positive Class \\n\\nTumor Size \\n\\nMalignant ?\\n\\nis disturbing the whole linear regression predictions. And that is where logistic regression comes into a picture.\\n\\nAs discussed earlier, to deal with outliers, Logistic Regression uses Sigmoid function. An explanation of logistic regression can begin with an explanation of the standard logistic function. The logistic function is a Sigmoid function, which takes any real value between zero and one. It is deﬁned as\\n\\nσ (t) = et et + 1 = 1 1 + e−t\\n\\nAnd if we plot it, the graph will be S curve. Now, when logistic regression model come across an outlier, it will take care\\n\\nof it.\\n\\nAnother way of looking at logistic regression: Consider the case where we are looking at a classiﬁcation problem and our output is probability. Our output is from 0 to 1, which represents the probability that the event has occurred. Using linear regression would result in output from 1 to inﬁnity, which when mapped to a sigmoid function goes very well into 0 to 1 depending on the output of linear regression.\\n\\n#https://scikit-learn.org/stable/modules/generated/sklearn. ~linear_model.LogisticRegression. html linear_classifier linear_model .LogisticRegression (random_ state=123) linear_classifier.fit(scaled_X_train, y_train)\\n\\n(continues on next page)\\n\\n190\\n\\n1.0\\n\\n0.8\\n\\n= 0.6 Ul > x 2 a 0.4\\n\\n0.2\\n\\n0.0\\n\\n11 Classiﬁcation\\n\\n. 2 8 Cott oe 0 cm 0 eaERS CREEPERS Oo SS comecmaocamem ¢ «¢ .\\n\\n2\\n\\n4,'),\n",
              " Document(metadata={'doc_id': '7c6bba50-ecf1-44c9-bf07-a45cc703c845'}, page_content='10.2 Decision Tree Regression\\n\\nA decision tree is arriving at an estimate by asking a series of questions to the data, each question narrowing our possible values until the model gets conﬁdent enough to make a single prediction. The order of the question and their content are being determined by the model. In addition, the questions asked are all in a True/False form.\\n\\nThis is a little tough to grasp because it is not how humans naturally think, and perhaps the best way to show this difference is to create a real decision tree from. In the above problem x1, x2 are two features that allow us to make predictions for the target variable y by asking True/False questions.\\n\\nThe decision of making strategic splits heavily affects a tree’s accuracy. The decision criteria are different for classiﬁcation and regression trees. Decision trees regression normally use mean squared error (MSE) to decide to split a node into two or more sub-nodes. Suppose we are doing a binary tree; the algorithm will ﬁrst pick a value and split the data into two subsets. For each subset, it will calculate the MSE separately. The tree chooses the value with results in smallest MSE value.\\n\\nLet us examine how is Splitting Decided for Decision Trees Regressor in more detail. The ﬁrst step to create a tree is to create the ﬁrst binary decision. How are you going to do it?\\n\\n169\\n\\n170\\n\\n10 Regression\\n\\nBranch/ Sub-Tree Splitting oars cor \\\\ Decision Node ( Decision Node ]\\n\\n1. We need to pick a variable and the value to split on such that the two groups are as different from each other as possible.\\n\\n2. For each variable, for each possible value of the possible value of that variable see whether it is better.\\n\\n3. Take weighted average of two new nodes (mse*num_samples).\\n\\nTo sum up, we now have:\\n\\n• A single number that represents how good a split is, which is the weighted average of the mean squared errors of the two groups that create.\\n\\n• A way to ﬁnd the best split, which is to try every variable and to try every possible value of that variable and see which variable and which value gives us a split with the best score.\\n\\nTraining of a decision tree regressor will stop when some stopping condition is met:\\n\\n1. When you hit a limit that was requested (for example: max_depth). 2. When your leaf nodes only have one thing in them (no further split is possible, MSE for the train will be zero but will overﬁt for any other set—not a useful model).\\n\\n#Model 2 decision tree model2 = tree.DecisionTreeRegressor() model2.fit(X_train, y_train) print(\"Decision Tree\") print(\"================================\") y_pred_train2 = model2.predict(X_train) RMSE_train2 = mean_squared_error(y_train,y_pred_train2) print(\"Decision Tree Train set: RMSE {}\".format(RMSE_train2)) print(\"================================\") \\n\\n(continues on next page)\\n\\n10.3 Random Forests\\n\\n(continued from previous page)\\n\\ny_pred_test2 = model2.predict(X_test) RMSE_test2 = mean_squared_error(y_test,y_pred_test2) print(\"Decision Tree Test set: RMSE {}\".format(RMSE_test2)) print(\"================================\") \\n\\nDecision Tree ================================ Decision Tree Train set: RMSE 1.4739259778473743e-36 ================================ Decision Tree Test set: RMSE 0.008496 ================================ '),\n",
              " Document(metadata={'doc_id': '8b5c318b-17dd-45c9-95f9-417b0af8671c'}, page_content=\"pd.melt(df, value_vars=['A', 'B', 'C']) pd.melt(df, value_vars=['key', 'A', 'B']) \\n\\n161\\n\\n162\\n\\n9 Data Wrangling\\n\\nvariable value foo key bar key baz key 1 A 2 A 3 A 4 B 5 B 6 B  0 1 2 3 4 5 6 7 8 \\n\\nvariable value foo key bar key baz key 1 A 2 A 3 A 4 B 5 B 6 B\\n\\n0 1 2 3 4 5 6 7 8\\n\\nChapter 10 Regression\\n\\n®) cra\\n\\nAbstract Regression estimates the relationship between dependent variables and independent variables. Linear regression is an easily understood, popular basic technique, which uses historical data to produce an output variable. Decision Tree regression arrives at an estimate by applying conditional rules on the data, narrowing possible values until a single prediction is made. Random Forests are clusters of individual decision trees that produce a prediction by selecting a vote by majority voting. Neural Networks are a representation of the brain and learns from the data through adjusting weights to minimize the error of prediction. Proper Data Processing techniques can further improve a model’s prediction such as ranking feature importance and outlier removal.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_search[0].page_content"
      ],
      "metadata": {
        "id": "TF27GrDpt1SI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "bd02960f-6300-4f37-cb9b-d9ded930ae19"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Learning outcomes:\\n\\nLearn and apply basic models for regression tasks using sklearn and keras. • Learn data processing techniques to achieve better results. • Learn how to use simple feature selection techniques to improve our model. • Data cleaning to help improve our model’s RMSE\\n\\nRegression looks for relationships among variables. For example, you can observe several employees of some company and try to understand how their salaries depend on the features, such as experience, level of education, role, city they work in, and so on.\\n\\nThis is a regression problem where data related to each employee represent one observation. The presumption is that the experience, education, role, and city are the independent features, and the salary of the employee depends on them.\\n\\nSimilarly, you can try to establish a mathematical dependence of the prices of houses on their areas, numbers of bedrooms, distances to the city center, and so on. Generally, in regression analysis, you usually consider some phenomenon of interest and have a number of observations. Each observation has two or more features. Following the assumption that (at least) one of the features depends on the others, you try to establish a relation among them.\\n\\nThe dependent features are called the dependent variables, outputs, or responses.\\n\\n© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022 T. T. Teoh, Z. Rong, Artiﬁcial Intelligence with Python, Machine Learning: Foundations, Methodologies, and Applications, https://doi.org/10.1007/978-981-16-8615-3_10\\n\\n163\\n\\n164\\n\\n10 Regression\\n\\nThe independent features are called the independent variables, inputs, or predic- tors.\\n\\nRegression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, brand, and so on.\\n\\nIt is a common practice to denote the outputs with x and inputs with y. If there are two or more independent variables, they can be represented as the vector x = (x1, . . . , xr ), where r is the number of inputs.\\n\\nWhen Do You Need Regression? Typically, you need regression to answer whether and how some phenomenon inﬂuences the other or how several variables are related. For example, you can use it to determine if and to what extent the experience or gender impacts salaries.\\n\\nRegression is also useful when you want to forecast a response using a new set of predictors. For example, you could try to predict electricity consumption of a household for the next hour given the outdoor temperature, time of day, and number of residents in that household.\\n\\nRegression is used in many different ﬁelds: economy, computer science, social sciences, and so on. Its importance rises every day with the availability of large amounts of data and increased awareness of the practical value of data.\\n\\nIt is important to note is that regression does not imply causation. It is easy to ﬁnd examples of non-related data that, after a regression calculation, do pass all sorts of statistical tests. The following is a popular example that illustrates the concept of data-driven “causality.”\\n\\nMl Number of people who drowned by falling into a pool Films Nicolas Cage appeared in 999 2000 200 3002 2003 2008 2008 2006 2007 2008 2009 -@ Nicholas Cage -* Swimming pool drownings\\n\\nIt is often said that correlation does not imply causation, although, inadvertently, we sometimes make the mistake of supposing that there is a causal link between two variables that follow a certain common pattern\\n\\n10 Regression\\n\\nDataset: “Alumni Giving Regression (Edited).csv” You can obtain the dataset from this link:\\n\\nhttps: //www.dropbox.com/s/veak3ugc4wj 91luz/Alumni%20Giving + %20Regression%20%28Edited%29.csv?d1=0.\\n\\nAlso, you may run the following code in order to download the dataset in google colab:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"What is CNN and How to use? explain\"\n",
        "\n",
        "doc_search= retriever.invoke(query) #retriever_new\n",
        "doc_search[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "ka0pNcGt7El0",
        "outputId": "41858aad-9f1f-48d3-ea6f-f5db488c0e14"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 16 Convolutional Neural Networks\\n\\n® a\\n\\nAbstract Convolutional Neural Networks are neural networks with convolution layers which perform operations similar to image processing ﬁlters. Convolutional Neural Networks are applied in a variety of tasks related to images such as image classiﬁcation, object detection, and semantic segmentation. Popular Network architectures include ResNet, GoogleNet, and VGG. These networks are often trained on very large datasets, can be downloaded in Keras and Tensorﬂow, and can be later used for ﬁnetuning on other tasks.\\n\\nLearning outcomes:\\n\\nUnderstand how convolution, pooling, and ﬂattening operations are performed. • Perform an image classiﬁcation task using Convolutional Neural Networks. • Familiarize with notable Convolution Neural Network Architectures. • Understand Transfer Learning and Finetuning. • Perform an image classiﬁcation task through ﬁnetuning a Convolutional Neural Network\\n\\npreviously trained on a separate task.\\n\\n• Exposure to various applications of Convolutional Neural Networks.\\n\\nA fully connected neural network consists of a series of fully connected layers that connect every neuron in one layer to every neuron in the other layer. The main problem with fully connected neural networks is that the number of weights required is very large for certain types of data. For example, an image of 224x224x3 would require 150528 weights in just the ﬁrst hidden layer and will grow quickly for even bigger images. You can imagine how computationally intensive things would become once the images reach dimensions as large as 8K resolution images (7680×4320), training such a network would require a lot of time and resources.\\n\\nHowever, for image data, repeating patterns can occur in different places. Hence we can train many smaller detectors, capable of sliding across an image, to take advantage of the repeating patterns. This would reduce the number of weights required to be trained.\\n\\nA Convolutional Neural Network is a neural network with some convolutional layers (and some other layers). A convolutional layer has a number of ﬁlters that does the convolutional operation.\\n\\n© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022 T. T. Teoh, Z. Rong, Artiﬁcial Intelligence with Python, Machine Learning: Foundations, Methodologies, and Applications, https://doi.org/10.1007/978-981-16-8615-3_16\\n\\n262\\n\\n16 Convolutional Neural Networks\\n\\nThey can be compressed to the same parameters.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separating The Elements Returned by Multivector Retriver Into Text and Images"
      ],
      "metadata": {
        "id": "LVDx8uPFt9A_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We parse the first four contents that are most similar to the question/query we asked above as base64 image and text data.\n",
        "# because we need to give text and image data to the model separately.\n",
        "from base64 import b64decode, b64encode\n",
        "def split_image_text_types(docs):\n",
        "\n",
        "    ''' Split base64-encoded images and texts '''\n",
        "    b64 = []  # an empty list to hold base64 encoded images\n",
        "    text = [] # An empty list to hold tables with plain text\n",
        "    for doc in docs:\n",
        "        try:\n",
        "            b64decode(doc.page_content, validate=True)   # Try to parse the string expression in Base64 form as binary data\n",
        "            b64.append(doc.page_content)  # If successful, add to list 'b64' (It will be successful if the document is in base64 format)\n",
        "        except Exception as e:            # # If it fails/returns error, an exception is thrown (If the document is in text format, an error will occur)\n",
        "            text.append(doc.page_content) # Treat it as text and add it to the 'text' list\n",
        "    return {\n",
        "        \"images\": b64,                    # Returns output as a dict\n",
        "        \"texts\": text\n",
        "    }\n",
        "docs_by_type = split_image_text_types(doc_search)"
      ],
      "metadata": {
        "id": "VnuW4YOH2s2F"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_by_type[\"texts\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwG6XbGit9EQ",
        "outputId": "211c2cbd-e479-4c2c-afee-eb3b004f058d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Chapter 16 Convolutional Neural Networks\\n\\n® a\\n\\nAbstract Convolutional Neural Networks are neural networks with convolution layers which perform operations similar to image processing ﬁlters. Convolutional Neural Networks are applied in a variety of tasks related to images such as image classiﬁcation, object detection, and semantic segmentation. Popular Network architectures include ResNet, GoogleNet, and VGG. These networks are often trained on very large datasets, can be downloaded in Keras and Tensorﬂow, and can be later used for ﬁnetuning on other tasks.\\n\\nLearning outcomes:\\n\\nUnderstand how convolution, pooling, and ﬂattening operations are performed. • Perform an image classiﬁcation task using Convolutional Neural Networks. • Familiarize with notable Convolution Neural Network Architectures. • Understand Transfer Learning and Finetuning. • Perform an image classiﬁcation task through ﬁnetuning a Convolutional Neural Network\\n\\npreviously trained on a separate task.\\n\\n• Exposure to various applications of Convolutional Neural Networks.\\n\\nA fully connected neural network consists of a series of fully connected layers that connect every neuron in one layer to every neuron in the other layer. The main problem with fully connected neural networks is that the number of weights required is very large for certain types of data. For example, an image of 224x224x3 would require 150528 weights in just the ﬁrst hidden layer and will grow quickly for even bigger images. You can imagine how computationally intensive things would become once the images reach dimensions as large as 8K resolution images (7680×4320), training such a network would require a lot of time and resources.\\n\\nHowever, for image data, repeating patterns can occur in different places. Hence we can train many smaller detectors, capable of sliding across an image, to take advantage of the repeating patterns. This would reduce the number of weights required to be trained.\\n\\nA Convolutional Neural Network is a neural network with some convolutional layers (and some other layers). A convolutional layer has a number of ﬁlters that does the convolutional operation.\\n\\n© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022 T. T. Teoh, Z. Rong, Artiﬁcial Intelligence with Python, Machine Learning: Foundations, Methodologies, and Applications, https://doi.org/10.1007/978-981-16-8615-3_16\\n\\n262\\n\\n16 Convolutional Neural Networks\\n\\nThey can be compressed to the same parameters.',\n",
              " '16.2 Pooling\\n\\nNowadays, a CNN always exploits extensive weight-sharing to reduce the degrees of the freedom of models. A pooling layer helps reduce computation time and gradually build up spatial and conﬁgural invariance. For image understanding, pooling layer helps extract more semantic meaning. The max pooling layer simply returns the maximum value over the values that the kernel operation is applied on. The example below illustrates the outputs of a max pooling and average pooling operation, respectively, given a kernel of size 2 and stride 2.\\n\\nmax pooling x fs [12] 2 [0 37| 4 25 average pooling\\n\\n16.3 Flattening\\n\\nAdding a Fully Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolu- tional layer. The Fully Connected layer is learning a possibly non-linear function in that space.\\n\\n16.4 Exercise\\n\\n265\\n\\nBy ﬂattening the image into a column vector, we have converted our input image into a suitable form for our Multi-Level Perceptron. The ﬂattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them using the Softmax Classiﬁcation technique.\\n\\nReLU Activation Fn. Volume-28x28x3  Output Volume 14x14x3  Output Volume 588x1  Output Volume 20x1  Output Nodes 5x1  Class 1  Class 2  Class 3  Class 4  Convolution layer Stride 1  Class 5  Max Pool layer Stride 2  Soft-max Layer  Flatten layer  Fully connected Layer ReLU Activation Fn.  Soft-Max Activation Fn \\n\\nInuput Volume 32x32x1\\n\\n16.4 Exercise\\n\\nWe will build a small CNN using Convolution layers, Max Pooling layers, and Dropout layers in order to predict the type of fruit in a picture.\\n\\nThe dataset we will use is the fruits 360 dataset. You can obtain the dataset from this link: https://www.kaggle.com/moltean/fruits\\n\\nimport numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd. read_csv) import os from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, Flatten from tensorflow.keras.layers import Conv2D, MaxPooling2D from tensorflow.keras import optimizers import numpy as np import pandas as pd\\n\\n(continues on next page)\\n\\n266\\n\\n16 Convolutional Neural Networks\\n\\n(continued from previous page)\\n\\nfrom tensorflow.keras.preprocessing.image import,, ~ImageDataGenerator import matplotlib.pyplot as plt import matplotlib.image as mpimg import pathlib\\n\\ntrain_root -=pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Training =) test_root = pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Test\")\\n\\ntrain_root -=pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Training =) test_root = pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Test\")\\n\\nbatch_size = 10 ',\n",
              " '15 Image Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 239 Load the Dependencies . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 239 15.1 Load Image from urls . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 240 15.2 Image Analysis .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 243 15.3 Image Histogram .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 244 15.4 Contour .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 247 15.5 Grayscale Transformation . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 249 15.6 Histogram Equalization.. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 250 15.7 Fourier Transformation . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 252 15.8 High pass Filtering in FFT . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 253 15.9 15.10 Pattern Recognition .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 254 15.11 Sample Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 258\\n\\n16 Convolutional Neural Networks . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 261 The Convolution Operation.. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 262 Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 264 Flattening .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 264 Exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 265 CNN Architectures.. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.1 VGG16.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.2 Inception Net . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.3 ResNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 271 Finetuning .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 272 Other Tasks That Use CNNs . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 273 16.7.1 Object Detection . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 274 16.7.2 Semantic Segmentation . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 274',\n",
              " 'Model: \"sequential_1\"\\n\\n(continues on next page)\\n\\n16.7 Other Tasks That Use CNNs\\n\\n(continued from previous page)\\n\\nLayer (type) resnet50 (Model) (None, Param # 23587712 o global_average_pooling2d_1 (| (None, o dense 1 (Dense) (None, Total params: 23,856,131 Trainable params: 268,419 Non-trainable params: 23,587,712 268419\\n\\nLayer (type) resnet50 (Model) (None, Param # 23587712 o global_average_pooling2d_1 (| (None, o dense 1 (Dense) (None, Total params: 23,856,131 Trainable params: 268,419 Non-trainable params: 23,587,712 268419\\n\\nOutput Shape\\n\\nParam #\\n\\n(None, 4, 4, 2048)\\n\\n0\\n\\n(None, 131)\\n\\n268419\\n\\nmodel.fit(train_data, epochs=1) \\n\\n6770/6770 [= loss: 0.1312 - accuracy: 0.9728 - 388s 57ms/step -\\n\\n<tensorflow.python.keras.callbacks.History at 0x15420d63490> \\n\\n<tensorflow.python.keras.callbacks.History at 0x15420d63490>\\n\\nscore = model.evaluate(train_data) print(score) score = model.evaluate(test_data) print(score) \\n\\n6770/6770 [== loss: 0.0214 - accuracy: 0.9927 [0.021364932879805565, 0.9926578998565674 2269/2269 [== loss: 0.3093 - accuracy: 0.9347 [0.3093399107456207, 0.9346791505813599] - 387s 57ms/step - - 132s 58ms/step -\\n\\n16.7 Other Tasks That Use CNNs\\n\\nCNNs are used in many other tasks apart from Image classiﬁcation.\\n\\n273\\n\\n274\\n\\n16 Convolutional Neural Networks\\n\\n16.7.1 Object Detection\\n\\nClassiﬁcation tasks only tell us what is in the image and not where the object is. Object detection is the task of localizing objects within an image. CNNs, such as ResNets, are usually used as the feature extractor for object detection networks.\\n\\nGroundtruth: tv or monitor tv or monitor (2) tv or monitor (3) person remote control remote control (2)\\n\\n16.7.2 Semantic Segmentation\\n\\nUsing Fully Convolutional Nets, we can generate output maps which tell us which pixel belongs to which classes. This task is called Semantic Segmentation.\\n\\n16.7 Other Tasks That Use CNNs\\n\\n275\\n\\nChapter 17 Chatbot, Speech, and NLP\\n\\n®) ces\\n\\nAbstract Chatbots are programs that are capable of conversing with people trained for their speciﬁc tasks, such as providing parents with information about a school. This chapter will provide the skills required to create a basic chatbot that can converse through speech. Speech to text tools will be used to convert speech data into text data. An encoder-decoder architecture model will be trained using Long- Short Term Memory units for a question and answer task for conversation.']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eCQgTnLxt9kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Images Returned by Multivector Retriver"
      ],
      "metadata": {
        "id": "PxXqupIvu9OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# With this function, we first convert the string expression in base64 form to image and then check its resolution.\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "def plt_img_base64(img_base64):\n",
        "    image = Image.open(io.BytesIO(b64decode(img_base64))) # It first analyzes the Base64 string, which is the input parameter,\n",
        "                                                          # as binary data and then converts this binary data into image data in bytes.\n",
        "\n",
        "    width, height=image.size\n",
        "    print(f\"image shape: ({width}, {height})\") #get image shape\n",
        "\n",
        "    # Display the image\n",
        "    return image\n",
        "\n",
        "plt_img_base64(docs_by_type[\"images\"][0])"
      ],
      "metadata": {
        "id": "ltwWintG2_Ez"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resizing Images Returned by Multivector Retriver"
      ],
      "metadata": {
        "id": "c1BlBi7OuVbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_base64_image(base64_string, size=(128,128)):\n",
        "  \"\"\"\n",
        "  Resize an image encoded as a Base64 string\n",
        "  \"\"\"\n",
        "  from base64 import b64encode\n",
        "  # Decode the base64 string\n",
        "  img_data = b64decode(base64_string)\n",
        "  img = Image.open(io.BytesIO(img_data))\n",
        "\n",
        "  # Resize the image\n",
        "  resized_img = img.resize(size, Image.LANCZOS) # The image is edited to the desired size.\n",
        "                                                # Image.LANCZOS is the resizing algorithm. Generally provides high quality.\n",
        "\n",
        "  # Save the resized image to a bytes buffer\n",
        "  buffered = io.BytesIO() # we create temporary memory.\n",
        "  resized_img.save(buffered, format=img.format) #We save the resized image into temporary memory.\n",
        "\n",
        "  # Encode the resized image to Base64\n",
        "  return b64encode(buffered.getvalue()).decode(\"utf-8\") # We convert the new size image we saved into temporary memory back to base64."
      ],
      "metadata": {
        "id": "gcvgvPKBuL3g"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_by_type[\"images\"]"
      ],
      "metadata": {
        "id": "C3m8H679u1z_"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_by_type[\"images\"]=[resize_base64_image(i, size=(256,256)) for i in docs_by_type[\"images\"]]"
      ],
      "metadata": {
        "id": "TbjvsUB7u2Oa"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt_img_base64(docs_by_type[\"images\"][0])"
      ],
      "metadata": {
        "id": "mKB9I9N1u5kq"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VSk2tSrZu78y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fde6f17-d244-4270-b759-68e1858d399f"
      },
      "source": [
        "## Retrieval Answer for Questions From Models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The function below returns an answer by taking all these contents into consideration, depending on whether the contents most similar to\n",
        "# the question we ask are text/table or image.\n",
        "def prompt_func(dict, question):\n",
        "    format_texts = \"\\n\".join(dict[\"texts\"])\n",
        "\n",
        "    messages = [\n",
        "        {\"type\": \"text\", \"text\": f\"\"\"Answer the question based only on the following context, which can include text, tables and the below image(s) if available:\n",
        "    Question: {question}\n",
        "\n",
        "    Text and tables:\n",
        "    {format_texts}\n",
        "\n",
        "    \"\"\"}\n",
        "    ]\n",
        "\n",
        "    # Iterate through the images\n",
        "    for image_data in dict['images']:\n",
        "        messages.append(\n",
        "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}}\n",
        "        )\n",
        "\n",
        "    return HumanMessage(content=messages)"
      ],
      "metadata": {
        "id": "I--CShWuOEYw"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "BUUBqkBfvOYn",
        "outputId": "171663d0-80fe-4f3d-c4c4-69b7cdee3bd8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is CNN and How to use? explain'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_by_type"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuW2-DuhvPB5",
        "outputId": "95ced1ad-00ee-4865-d6bf-2f3db56d34d0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'images': [],\n",
              " 'texts': ['Chapter 16 Convolutional Neural Networks\\n\\n® a\\n\\nAbstract Convolutional Neural Networks are neural networks with convolution layers which perform operations similar to image processing ﬁlters. Convolutional Neural Networks are applied in a variety of tasks related to images such as image classiﬁcation, object detection, and semantic segmentation. Popular Network architectures include ResNet, GoogleNet, and VGG. These networks are often trained on very large datasets, can be downloaded in Keras and Tensorﬂow, and can be later used for ﬁnetuning on other tasks.\\n\\nLearning outcomes:\\n\\nUnderstand how convolution, pooling, and ﬂattening operations are performed. • Perform an image classiﬁcation task using Convolutional Neural Networks. • Familiarize with notable Convolution Neural Network Architectures. • Understand Transfer Learning and Finetuning. • Perform an image classiﬁcation task through ﬁnetuning a Convolutional Neural Network\\n\\npreviously trained on a separate task.\\n\\n• Exposure to various applications of Convolutional Neural Networks.\\n\\nA fully connected neural network consists of a series of fully connected layers that connect every neuron in one layer to every neuron in the other layer. The main problem with fully connected neural networks is that the number of weights required is very large for certain types of data. For example, an image of 224x224x3 would require 150528 weights in just the ﬁrst hidden layer and will grow quickly for even bigger images. You can imagine how computationally intensive things would become once the images reach dimensions as large as 8K resolution images (7680×4320), training such a network would require a lot of time and resources.\\n\\nHowever, for image data, repeating patterns can occur in different places. Hence we can train many smaller detectors, capable of sliding across an image, to take advantage of the repeating patterns. This would reduce the number of weights required to be trained.\\n\\nA Convolutional Neural Network is a neural network with some convolutional layers (and some other layers). A convolutional layer has a number of ﬁlters that does the convolutional operation.\\n\\n© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022 T. T. Teoh, Z. Rong, Artiﬁcial Intelligence with Python, Machine Learning: Foundations, Methodologies, and Applications, https://doi.org/10.1007/978-981-16-8615-3_16\\n\\n262\\n\\n16 Convolutional Neural Networks\\n\\nThey can be compressed to the same parameters.',\n",
              "  '16.2 Pooling\\n\\nNowadays, a CNN always exploits extensive weight-sharing to reduce the degrees of the freedom of models. A pooling layer helps reduce computation time and gradually build up spatial and conﬁgural invariance. For image understanding, pooling layer helps extract more semantic meaning. The max pooling layer simply returns the maximum value over the values that the kernel operation is applied on. The example below illustrates the outputs of a max pooling and average pooling operation, respectively, given a kernel of size 2 and stride 2.\\n\\nmax pooling x fs [12] 2 [0 37| 4 25 average pooling\\n\\n16.3 Flattening\\n\\nAdding a Fully Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolu- tional layer. The Fully Connected layer is learning a possibly non-linear function in that space.\\n\\n16.4 Exercise\\n\\n265\\n\\nBy ﬂattening the image into a column vector, we have converted our input image into a suitable form for our Multi-Level Perceptron. The ﬂattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them using the Softmax Classiﬁcation technique.\\n\\nReLU Activation Fn. Volume-28x28x3  Output Volume 14x14x3  Output Volume 588x1  Output Volume 20x1  Output Nodes 5x1  Class 1  Class 2  Class 3  Class 4  Convolution layer Stride 1  Class 5  Max Pool layer Stride 2  Soft-max Layer  Flatten layer  Fully connected Layer ReLU Activation Fn.  Soft-Max Activation Fn \\n\\nInuput Volume 32x32x1\\n\\n16.4 Exercise\\n\\nWe will build a small CNN using Convolution layers, Max Pooling layers, and Dropout layers in order to predict the type of fruit in a picture.\\n\\nThe dataset we will use is the fruits 360 dataset. You can obtain the dataset from this link: https://www.kaggle.com/moltean/fruits\\n\\nimport numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd. read_csv) import os from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, Flatten from tensorflow.keras.layers import Conv2D, MaxPooling2D from tensorflow.keras import optimizers import numpy as np import pandas as pd\\n\\n(continues on next page)\\n\\n266\\n\\n16 Convolutional Neural Networks\\n\\n(continued from previous page)\\n\\nfrom tensorflow.keras.preprocessing.image import,, ~ImageDataGenerator import matplotlib.pyplot as plt import matplotlib.image as mpimg import pathlib\\n\\ntrain_root -=pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Training =) test_root = pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Test\")\\n\\ntrain_root -=pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Training =) test_root = pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Test\")\\n\\nbatch_size = 10 ',\n",
              "  '15 Image Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 239 Load the Dependencies . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 239 15.1 Load Image from urls . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 240 15.2 Image Analysis .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 243 15.3 Image Histogram .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 244 15.4 Contour .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 247 15.5 Grayscale Transformation . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 249 15.6 Histogram Equalization.. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 250 15.7 Fourier Transformation . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 252 15.8 High pass Filtering in FFT . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 253 15.9 15.10 Pattern Recognition .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 254 15.11 Sample Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 258\\n\\n16 Convolutional Neural Networks . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 261 The Convolution Operation.. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 262 Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 264 Flattening .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 264 Exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 265 CNN Architectures.. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.1 VGG16.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.2 Inception Net . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.3 ResNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 271 Finetuning .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 272 Other Tasks That Use CNNs . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 273 16.7.1 Object Detection . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 274 16.7.2 Semantic Segmentation . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 274',\n",
              "  'Model: \"sequential_1\"\\n\\n(continues on next page)\\n\\n16.7 Other Tasks That Use CNNs\\n\\n(continued from previous page)\\n\\nLayer (type) resnet50 (Model) (None, Param # 23587712 o global_average_pooling2d_1 (| (None, o dense 1 (Dense) (None, Total params: 23,856,131 Trainable params: 268,419 Non-trainable params: 23,587,712 268419\\n\\nLayer (type) resnet50 (Model) (None, Param # 23587712 o global_average_pooling2d_1 (| (None, o dense 1 (Dense) (None, Total params: 23,856,131 Trainable params: 268,419 Non-trainable params: 23,587,712 268419\\n\\nOutput Shape\\n\\nParam #\\n\\n(None, 4, 4, 2048)\\n\\n0\\n\\n(None, 131)\\n\\n268419\\n\\nmodel.fit(train_data, epochs=1) \\n\\n6770/6770 [= loss: 0.1312 - accuracy: 0.9728 - 388s 57ms/step -\\n\\n<tensorflow.python.keras.callbacks.History at 0x15420d63490> \\n\\n<tensorflow.python.keras.callbacks.History at 0x15420d63490>\\n\\nscore = model.evaluate(train_data) print(score) score = model.evaluate(test_data) print(score) \\n\\n6770/6770 [== loss: 0.0214 - accuracy: 0.9927 [0.021364932879805565, 0.9926578998565674 2269/2269 [== loss: 0.3093 - accuracy: 0.9347 [0.3093399107456207, 0.9346791505813599] - 387s 57ms/step - - 132s 58ms/step -\\n\\n16.7 Other Tasks That Use CNNs\\n\\nCNNs are used in many other tasks apart from Image classiﬁcation.\\n\\n273\\n\\n274\\n\\n16 Convolutional Neural Networks\\n\\n16.7.1 Object Detection\\n\\nClassiﬁcation tasks only tell us what is in the image and not where the object is. Object detection is the task of localizing objects within an image. CNNs, such as ResNets, are usually used as the feature extractor for object detection networks.\\n\\nGroundtruth: tv or monitor tv or monitor (2) tv or monitor (3) person remote control remote control (2)\\n\\n16.7.2 Semantic Segmentation\\n\\nUsing Fully Convolutional Nets, we can generate output maps which tell us which pixel belongs to which classes. This task is called Semantic Segmentation.\\n\\n16.7 Other Tasks That Use CNNs\\n\\n275\\n\\nChapter 17 Chatbot, Speech, and NLP\\n\\n®) ces\\n\\nAbstract Chatbots are programs that are capable of conversing with people trained for their speciﬁc tasks, such as providing parents with information about a school. This chapter will provide the skills required to create a basic chatbot that can converse through speech. Speech to text tools will be used to convert speech data into text data. An encoder-decoder architecture model will be trained using Long- Short Term Memory units for a question and answer task for conversation.']}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input= prompt_func(docs_by_type, query)\n",
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8sMECTNvRKB",
        "outputId": "f3826a68-a8a3-4138-fb7a-fcaf094e1238"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HumanMessage(content=[{'type': 'text', 'text': 'Answer the question based only on the following context, which can include text, tables and the below image(s) if available:\\n    Question: What is CNN and How to use? explain\\n\\n    Text and tables:\\n    Chapter 16 Convolutional Neural Networks\\n\\n® a\\n\\nAbstract Convolutional Neural Networks are neural networks with convolution layers which perform operations similar to image processing ﬁlters. Convolutional Neural Networks are applied in a variety of tasks related to images such as image classiﬁcation, object detection, and semantic segmentation. Popular Network architectures include ResNet, GoogleNet, and VGG. These networks are often trained on very large datasets, can be downloaded in Keras and Tensorﬂow, and can be later used for ﬁnetuning on other tasks.\\n\\nLearning outcomes:\\n\\nUnderstand how convolution, pooling, and ﬂattening operations are performed. • Perform an image classiﬁcation task using Convolutional Neural Networks. • Familiarize with notable Convolution Neural Network Architectures. • Understand Transfer Learning and Finetuning. • Perform an image classiﬁcation task through ﬁnetuning a Convolutional Neural Network\\n\\npreviously trained on a separate task.\\n\\n• Exposure to various applications of Convolutional Neural Networks.\\n\\nA fully connected neural network consists of a series of fully connected layers that connect every neuron in one layer to every neuron in the other layer. The main problem with fully connected neural networks is that the number of weights required is very large for certain types of data. For example, an image of 224x224x3 would require 150528 weights in just the ﬁrst hidden layer and will grow quickly for even bigger images. You can imagine how computationally intensive things would become once the images reach dimensions as large as 8K resolution images (7680×4320), training such a network would require a lot of time and resources.\\n\\nHowever, for image data, repeating patterns can occur in different places. Hence we can train many smaller detectors, capable of sliding across an image, to take advantage of the repeating patterns. This would reduce the number of weights required to be trained.\\n\\nA Convolutional Neural Network is a neural network with some convolutional layers (and some other layers). A convolutional layer has a number of ﬁlters that does the convolutional operation.\\n\\n© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022 T. T. Teoh, Z. Rong, Artiﬁcial Intelligence with Python, Machine Learning: Foundations, Methodologies, and Applications, https://doi.org/10.1007/978-981-16-8615-3_16\\n\\n262\\n\\n16 Convolutional Neural Networks\\n\\nThey can be compressed to the same parameters.\\n16.2 Pooling\\n\\nNowadays, a CNN always exploits extensive weight-sharing to reduce the degrees of the freedom of models. A pooling layer helps reduce computation time and gradually build up spatial and conﬁgural invariance. For image understanding, pooling layer helps extract more semantic meaning. The max pooling layer simply returns the maximum value over the values that the kernel operation is applied on. The example below illustrates the outputs of a max pooling and average pooling operation, respectively, given a kernel of size 2 and stride 2.\\n\\nmax pooling x fs [12] 2 [0 37| 4 25 average pooling\\n\\n16.3 Flattening\\n\\nAdding a Fully Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolu- tional layer. The Fully Connected layer is learning a possibly non-linear function in that space.\\n\\n16.4 Exercise\\n\\n265\\n\\nBy ﬂattening the image into a column vector, we have converted our input image into a suitable form for our Multi-Level Perceptron. The ﬂattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them using the Softmax Classiﬁcation technique.\\n\\nReLU Activation Fn. Volume-28x28x3  Output Volume 14x14x3  Output Volume 588x1  Output Volume 20x1  Output Nodes 5x1  Class 1  Class 2  Class 3  Class 4  Convolution layer Stride 1  Class 5  Max Pool layer Stride 2  Soft-max Layer  Flatten layer  Fully connected Layer ReLU Activation Fn.  Soft-Max Activation Fn \\n\\nInuput Volume 32x32x1\\n\\n16.4 Exercise\\n\\nWe will build a small CNN using Convolution layers, Max Pooling layers, and Dropout layers in order to predict the type of fruit in a picture.\\n\\nThe dataset we will use is the fruits 360 dataset. You can obtain the dataset from this link: https://www.kaggle.com/moltean/fruits\\n\\nimport numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd. read_csv) import os from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, Flatten from tensorflow.keras.layers import Conv2D, MaxPooling2D from tensorflow.keras import optimizers import numpy as np import pandas as pd\\n\\n(continues on next page)\\n\\n266\\n\\n16 Convolutional Neural Networks\\n\\n(continued from previous page)\\n\\nfrom tensorflow.keras.preprocessing.image import,, ~ImageDataGenerator import matplotlib.pyplot as plt import matplotlib.image as mpimg import pathlib\\n\\ntrain_root -=pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Training =) test_root = pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Test\")\\n\\ntrain_root -=pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Training =) test_root = pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Test\")\\n\\nbatch_size = 10 \\n15 Image Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 239 Load the Dependencies . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 239 15.1 Load Image from urls . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 240 15.2 Image Analysis .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 243 15.3 Image Histogram .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 244 15.4 Contour .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 247 15.5 Grayscale Transformation . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 249 15.6 Histogram Equalization.. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 250 15.7 Fourier Transformation . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 252 15.8 High pass Filtering in FFT . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 253 15.9 15.10 Pattern Recognition .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 254 15.11 Sample Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 258\\n\\n16 Convolutional Neural Networks . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 261 The Convolution Operation.. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 262 Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 264 Flattening .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 264 Exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 265 CNN Architectures.. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.1 VGG16.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.2 Inception Net . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.3 ResNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 271 Finetuning .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 272 Other Tasks That Use CNNs . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 273 16.7.1 Object Detection . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 274 16.7.2 Semantic Segmentation . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 274\\nModel: \"sequential_1\"\\n\\n(continues on next page)\\n\\n16.7 Other Tasks That Use CNNs\\n\\n(continued from previous page)\\n\\nLayer (type) resnet50 (Model) (None, Param # 23587712 o global_average_pooling2d_1 (| (None, o dense 1 (Dense) (None, Total params: 23,856,131 Trainable params: 268,419 Non-trainable params: 23,587,712 268419\\n\\nLayer (type) resnet50 (Model) (None, Param # 23587712 o global_average_pooling2d_1 (| (None, o dense 1 (Dense) (None, Total params: 23,856,131 Trainable params: 268,419 Non-trainable params: 23,587,712 268419\\n\\nOutput Shape\\n\\nParam #\\n\\n(None, 4, 4, 2048)\\n\\n0\\n\\n(None, 131)\\n\\n268419\\n\\nmodel.fit(train_data, epochs=1) \\n\\n6770/6770 [= loss: 0.1312 - accuracy: 0.9728 - 388s 57ms/step -\\n\\n<tensorflow.python.keras.callbacks.History at 0x15420d63490> \\n\\n<tensorflow.python.keras.callbacks.History at 0x15420d63490>\\n\\nscore = model.evaluate(train_data) print(score) score = model.evaluate(test_data) print(score) \\n\\n6770/6770 [== loss: 0.0214 - accuracy: 0.9927 [0.021364932879805565, 0.9926578998565674 2269/2269 [== loss: 0.3093 - accuracy: 0.9347 [0.3093399107456207, 0.9346791505813599] - 387s 57ms/step - - 132s 58ms/step -\\n\\n16.7 Other Tasks That Use CNNs\\n\\nCNNs are used in many other tasks apart from Image classiﬁcation.\\n\\n273\\n\\n274\\n\\n16 Convolutional Neural Networks\\n\\n16.7.1 Object Detection\\n\\nClassiﬁcation tasks only tell us what is in the image and not where the object is. Object detection is the task of localizing objects within an image. CNNs, such as ResNets, are usually used as the feature extractor for object detection networks.\\n\\nGroundtruth: tv or monitor tv or monitor (2) tv or monitor (3) person remote control remote control (2)\\n\\n16.7.2 Semantic Segmentation\\n\\nUsing Fully Convolutional Nets, we can generate output maps which tell us which pixel belongs to which classes. This task is called Semantic Segmentation.\\n\\n16.7 Other Tasks That Use CNNs\\n\\n275\\n\\nChapter 17 Chatbot, Speech, and NLP\\n\\n®) ces\\n\\nAbstract Chatbots are programs that are capable of conversing with people trained for their speciﬁc tasks, such as providing parents with information about a school. This chapter will provide the skills required to create a basic chatbot that can converse through speech. Speech to text tools will be used to convert speech data into text data. An encoder-decoder architecture model will be trained using Long- Short Term Memory units for a question and answer task for conversation.\\n\\n    '}])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(temperature=0.0,\n",
        "                   model=\"gpt-4o-mini\",\n",
        "                   max_tokens=1024,\n",
        "                   top_p=1.0) # vision models can work on both text and images\n",
        "\n",
        "output=model.invoke([input])\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vk4qerbpvfYh",
        "outputId": "d01e83aa-a34a-42de-ab49-943d3eafdd8a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"**What is CNN and How to use?**\\n\\n**Convolutional Neural Networks (CNNs)** are a type of neural network specifically designed for processing structured grid data, such as images. They consist of multiple layers, including convolutional layers, pooling layers, and fully connected layers, which work together to extract features from images and perform tasks like image classification, object detection, and semantic segmentation.\\n\\n### Key Components of CNNs:\\n1. **Convolutional Layers**: These layers apply filters to the input image to create feature maps. The filters slide over the image, detecting patterns such as edges, textures, and shapes.\\n\\n2. **Pooling Layers**: Pooling reduces the spatial dimensions of the feature maps, which helps decrease the computational load and extract dominant features. Max pooling is a common technique that selects the maximum value from a set of values in the feature map.\\n\\n3. **Flattening**: After the convolutional and pooling layers, the multi-dimensional output is flattened into a one-dimensional vector to be fed into fully connected layers.\\n\\n4. **Fully Connected Layers**: These layers learn non-linear combinations of the features extracted by the convolutional layers and are typically used at the end of the network for classification tasks.\\n\\n### How to Use CNNs:\\n1. **Dataset Preparation**: Obtain a dataset suitable for the task (e.g., images for classification). For example, the Fruits 360 dataset can be used for fruit classification tasks.\\n\\n2. **Model Building**: Use libraries like TensorFlow or Keras to build a CNN model. This involves defining the architecture with convolutional layers, pooling layers, and fully connected layers.\\n\\n3. **Training the Model**: Train the model using the training dataset. This involves feeding the images through the network, calculating the loss, and updating the weights using backpropagation.\\n\\n4. **Evaluation**: After training, evaluate the model's performance on a separate test dataset to check its accuracy and loss.\\n\\n5. **Fine-tuning**: If necessary, fine-tune the model by adjusting hyperparameters or using transfer learning, where a pre-trained model is adapted for a new task.\\n\\n### Example Code Snippet:\\nHere’s a simplified example of how to build and train a CNN using Keras:\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\\n\\n# Define the model\\nmodel = Sequential()\\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(Flatten())\\nmodel.add(Dense(128, activation='relu'))\\nmodel.add(Dense(num_classes, activation='softmax'))\\n\\n# Compile the model\\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\\n\\n# Train the model\\nmodel.fit(train_data, epochs=10)\\n\\n# Evaluate the model\\nscore = model.evaluate(test_data)\\nprint(score)\\n```\\n\\nThis example demonstrates the basic structure of a CNN and how to train it on image data. CNNs are powerful tools for image-related tasks and can be adapted for various applications beyond simple classification.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 677, 'prompt_tokens': 3403, 'total_tokens': 4080}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-42e9acfa-023e-43e6-b585-e07e735a23b6-0', usage_metadata={'input_tokens': 3403, 'output_tokens': 677, 'total_tokens': 4080})"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(output.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yGR94tlUv1qv",
        "outputId": "e00f9999-bf9d-402b-b1fe-c58791031aa4"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **What is CNN and How to use?**\n> \n> **Convolutional Neural Networks (CNNs)** are a type of neural network specifically designed for processing structured grid data, such as images. They consist of multiple layers, including convolutional layers, pooling layers, and fully connected layers, which work together to extract features from images and perform tasks like image classification, object detection, and semantic segmentation.\n> \n> ### Key Components of CNNs:\n> 1. **Convolutional Layers**: These layers apply filters to the input image to create feature maps. The filters slide over the image, detecting patterns such as edges, textures, and shapes.\n> \n> 2. **Pooling Layers**: Pooling reduces the spatial dimensions of the feature maps, which helps decrease the computational load and extract dominant features. Max pooling is a common technique that selects the maximum value from a set of values in the feature map.\n> \n> 3. **Flattening**: After the convolutional and pooling layers, the multi-dimensional output is flattened into a one-dimensional vector to be fed into fully connected layers.\n> \n> 4. **Fully Connected Layers**: These layers learn non-linear combinations of the features extracted by the convolutional layers and are typically used at the end of the network for classification tasks.\n> \n> ### How to Use CNNs:\n> 1. **Dataset Preparation**: Obtain a dataset suitable for the task (e.g., images for classification). For example, the Fruits 360 dataset can be used for fruit classification tasks.\n> \n> 2. **Model Building**: Use libraries like TensorFlow or Keras to build a CNN model. This involves defining the architecture with convolutional layers, pooling layers, and fully connected layers.\n> \n> 3. **Training the Model**: Train the model using the training dataset. This involves feeding the images through the network, calculating the loss, and updating the weights using backpropagation.\n> \n> 4. **Evaluation**: After training, evaluate the model's performance on a separate test dataset to check its accuracy and loss.\n> \n> 5. **Fine-tuning**: If necessary, fine-tune the model by adjusting hyperparameters or using transfer learning, where a pre-trained model is adapted for a new task.\n> \n> ### Example Code Snippet:\n> Here’s a simplified example of how to build and train a CNN using Keras:\n> \n> ```python\n> import numpy as np\n> import pandas as pd\n> from tensorflow.keras.models import Sequential\n> from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n> from tensorflow.keras.preprocessing.image import ImageDataGenerator\n> \n> # Define the model\n> model = Sequential()\n> model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n> model.add(MaxPooling2D(pool_size=(2, 2)))\n> model.add(Flatten())\n> model.add(Dense(128, activation='relu'))\n> model.add(Dense(num_classes, activation='softmax'))\n> \n> # Compile the model\n> model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n> \n> # Train the model\n> model.fit(train_data, epochs=10)\n> \n> # Evaluate the model\n> score = model.evaluate(test_data)\n> print(score)\n> ```\n> \n> This example demonstrates the basic structure of a CNN and how to train it on image data. CNNs are powerful tools for image-related tasks and can be adapted for various applications beyond simple classification."
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking whether documents are related to the question"
      ],
      "metadata": {
        "id": "PwFq0cEJRKsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.search_kwargs[\"k\"]=5"
      ],
      "metadata": {
        "id": "4rtjluudxlqR"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"What is CNN and How to use? explain\"\n",
        "\n",
        "index= retriever.vectorstore.similarity_search(query, k=5)\n",
        "index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063a8be3-ce8b-4689-9cd6-f0e4005cc4d9",
        "id": "OunpIqsFxlqS"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'doc_id': '29a90db4-28ca-45e4-8fbb-d007eabd41e0'}, page_content='Chapter 16 focuses on Convolutional Neural Networks (CNNs), which are specialized neural networks that utilize convolution layers to process images similarly to traditional image processing filters. CNNs are widely used for tasks like image classification, object detection, and semantic segmentation, with prominent architectures including ResNet, GoogleNet, and VGG. These networks are typically trained on large datasets and can be adapted for other tasks through fine-tuning in frameworks like Keras and TensorFlow.\\n\\nKey learning outcomes include understanding convolution, pooling, and flattening operations, performing image classification with CNNs, familiarizing oneself with notable architectures, and grasping concepts like transfer learning and fine-tuning. The chapter contrasts CNNs with fully connected neural networks, highlighting the inefficiency of the latter for image data due to the vast number of weights required. CNNs address this by employing smaller filters that slide over images, leveraging the presence of repeating patterns and significantly reducing computational load.'),\n",
              " Document(metadata={'doc_id': '6973319e-0bea-49f8-a859-bf939ea89375'}, page_content='The text discusses various components and processes involved in Convolutional Neural Networks (CNNs). It begins by explaining the role of pooling layers, particularly max pooling, which reduces computation time and enhances the extraction of semantic meaning from images through weight-sharing. It then describes flattening, which transforms the image into a column vector for input into a Fully Connected layer, allowing the model to learn non-linear combinations of high-level features. The process involves feeding this flattened output into a feed-forward neural network, where backpropagation is applied for training, ultimately enabling the classification of images using the Softmax technique.\\n\\nFurthermore, the text outlines a practical exercise to build a small CNN to predict fruit types using the Fruits 360 dataset, providing instructions on importing necessary libraries and setting up data paths for training and testing.'),\n",
              " Document(metadata={'doc_id': '87821439-f0d2-450c-9fdf-f271e854e87f'}, page_content='The text outlines the contents of two chapters focused on image processing and convolutional neural networks (CNNs). Chapter 15 details various aspects of image processing, including loading images from URLs, image analysis, histograms, contour detection, grayscale transformation, histogram equalization, Fourier transformation, high pass filtering in FFT, pattern recognition, and includes sample code. Chapter 16 introduces CNNs, covering the convolution operation, pooling, flattening, different CNN architectures (such as VGG16, Inception Net, and ResNet), finetuning, and additional tasks that utilize CNNs, including object detection and semantic segmentation.'),\n",
              " Document(metadata={'doc_id': 'cfcaaf5f-2228-4851-b541-41b160073659'}, page_content='The text discusses various applications of Convolutional Neural Networks (CNNs) beyond image classification, specifically focusing on object detection and semantic segmentation. Object detection involves localizing objects within images, while semantic segmentation assigns class labels to individual pixels in an image. The ResNet architecture is highlighted as a feature extractor for these tasks. Additionally, the text briefly mentions chatbots, emphasizing their ability to converse and provide information, and outlines the skills needed to create a basic chatbot using speech-to-text tools and an encoder-decoder model with Long Short-Term Memory units for conversation tasks.'),\n",
              " Document(metadata={'doc_id': '34866d27-38e2-4190-8ec0-ce2b795a1b39'}, page_content=\"Neural networks are modeled after the brain's interconnected neurons, allowing for the processing of information. They take inputs, such as images, and provide outputs, like the probability of identifying an object (e.g., a cat). Initially, the network cannot accurately predict outputs, so it learns through a labeled training phase. If the output doesn't match the label, the network adjusts its weights through a process called backpropagation. The text also includes a Python code snippet to create a simple neural network using layers of neurons, demonstrating how to compile and fit the model using the TensorFlow library, and how to evaluate its performance using Root Mean Square Error (RMSE) on training and test sets.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_search= retriever.invoke(query) #retriever_new\n",
        "doc_search\n",
        "\n",
        "# get four most relevant/similar documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed362da-4fc1-4b28-8df7-16ad3dbd88ac",
        "id": "-SrXJvsRxlqS"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'doc_id': '29a90db4-28ca-45e4-8fbb-d007eabd41e0'}, page_content='Chapter 16 Convolutional Neural Networks\\n\\n® a\\n\\nAbstract Convolutional Neural Networks are neural networks with convolution layers which perform operations similar to image processing ﬁlters. Convolutional Neural Networks are applied in a variety of tasks related to images such as image classiﬁcation, object detection, and semantic segmentation. Popular Network architectures include ResNet, GoogleNet, and VGG. These networks are often trained on very large datasets, can be downloaded in Keras and Tensorﬂow, and can be later used for ﬁnetuning on other tasks.\\n\\nLearning outcomes:\\n\\nUnderstand how convolution, pooling, and ﬂattening operations are performed. • Perform an image classiﬁcation task using Convolutional Neural Networks. • Familiarize with notable Convolution Neural Network Architectures. • Understand Transfer Learning and Finetuning. • Perform an image classiﬁcation task through ﬁnetuning a Convolutional Neural Network\\n\\npreviously trained on a separate task.\\n\\n• Exposure to various applications of Convolutional Neural Networks.\\n\\nA fully connected neural network consists of a series of fully connected layers that connect every neuron in one layer to every neuron in the other layer. The main problem with fully connected neural networks is that the number of weights required is very large for certain types of data. For example, an image of 224x224x3 would require 150528 weights in just the ﬁrst hidden layer and will grow quickly for even bigger images. You can imagine how computationally intensive things would become once the images reach dimensions as large as 8K resolution images (7680×4320), training such a network would require a lot of time and resources.\\n\\nHowever, for image data, repeating patterns can occur in different places. Hence we can train many smaller detectors, capable of sliding across an image, to take advantage of the repeating patterns. This would reduce the number of weights required to be trained.\\n\\nA Convolutional Neural Network is a neural network with some convolutional layers (and some other layers). A convolutional layer has a number of ﬁlters that does the convolutional operation.\\n\\n© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022 T. T. Teoh, Z. Rong, Artiﬁcial Intelligence with Python, Machine Learning: Foundations, Methodologies, and Applications, https://doi.org/10.1007/978-981-16-8615-3_16\\n\\n262\\n\\n16 Convolutional Neural Networks\\n\\nThey can be compressed to the same parameters.'),\n",
              " Document(metadata={'doc_id': '6973319e-0bea-49f8-a859-bf939ea89375'}, page_content='16.2 Pooling\\n\\nNowadays, a CNN always exploits extensive weight-sharing to reduce the degrees of the freedom of models. A pooling layer helps reduce computation time and gradually build up spatial and conﬁgural invariance. For image understanding, pooling layer helps extract more semantic meaning. The max pooling layer simply returns the maximum value over the values that the kernel operation is applied on. The example below illustrates the outputs of a max pooling and average pooling operation, respectively, given a kernel of size 2 and stride 2.\\n\\nmax pooling x fs [12] 2 [0 37| 4 25 average pooling\\n\\n16.3 Flattening\\n\\nAdding a Fully Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolu- tional layer. The Fully Connected layer is learning a possibly non-linear function in that space.\\n\\n16.4 Exercise\\n\\n265\\n\\nBy ﬂattening the image into a column vector, we have converted our input image into a suitable form for our Multi-Level Perceptron. The ﬂattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them using the Softmax Classiﬁcation technique.\\n\\nReLU Activation Fn. Volume-28x28x3  Output Volume 14x14x3  Output Volume 588x1  Output Volume 20x1  Output Nodes 5x1  Class 1  Class 2  Class 3  Class 4  Convolution layer Stride 1  Class 5  Max Pool layer Stride 2  Soft-max Layer  Flatten layer  Fully connected Layer ReLU Activation Fn.  Soft-Max Activation Fn \\n\\nInuput Volume 32x32x1\\n\\n16.4 Exercise\\n\\nWe will build a small CNN using Convolution layers, Max Pooling layers, and Dropout layers in order to predict the type of fruit in a picture.\\n\\nThe dataset we will use is the fruits 360 dataset. You can obtain the dataset from this link: https://www.kaggle.com/moltean/fruits\\n\\nimport numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd. read_csv) import os from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, Flatten from tensorflow.keras.layers import Conv2D, MaxPooling2D from tensorflow.keras import optimizers import numpy as np import pandas as pd\\n\\n(continues on next page)\\n\\n266\\n\\n16 Convolutional Neural Networks\\n\\n(continued from previous page)\\n\\nfrom tensorflow.keras.preprocessing.image import,, ~ImageDataGenerator import matplotlib.pyplot as plt import matplotlib.image as mpimg import pathlib\\n\\ntrain_root -=pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Training =) test_root = pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Test\")\\n\\ntrain_root -=pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Training =) test_root = pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Test\")\\n\\nbatch_size = 10 '),\n",
              " Document(metadata={'doc_id': '87821439-f0d2-450c-9fdf-f271e854e87f'}, page_content='15 Image Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 239 Load the Dependencies . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 239 15.1 Load Image from urls . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 240 15.2 Image Analysis .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 243 15.3 Image Histogram .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 244 15.4 Contour .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 247 15.5 Grayscale Transformation . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 249 15.6 Histogram Equalization.. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 250 15.7 Fourier Transformation . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 252 15.8 High pass Filtering in FFT . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 253 15.9 15.10 Pattern Recognition .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 254 15.11 Sample Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 258\\n\\n16 Convolutional Neural Networks . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 261 The Convolution Operation.. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 262 Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 264 Flattening .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 264 Exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 265 CNN Architectures.. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.1 VGG16.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.2 Inception Net . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.3 ResNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 271 Finetuning .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 272 Other Tasks That Use CNNs . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 273 16.7.1 Object Detection . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 274 16.7.2 Semantic Segmentation . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 274'),\n",
              " Document(metadata={'doc_id': 'cfcaaf5f-2228-4851-b541-41b160073659'}, page_content='Model: \"sequential_1\"\\n\\n(continues on next page)\\n\\n16.7 Other Tasks That Use CNNs\\n\\n(continued from previous page)\\n\\nLayer (type) resnet50 (Model) (None, Param # 23587712 o global_average_pooling2d_1 (| (None, o dense 1 (Dense) (None, Total params: 23,856,131 Trainable params: 268,419 Non-trainable params: 23,587,712 268419\\n\\nLayer (type) resnet50 (Model) (None, Param # 23587712 o global_average_pooling2d_1 (| (None, o dense 1 (Dense) (None, Total params: 23,856,131 Trainable params: 268,419 Non-trainable params: 23,587,712 268419\\n\\nOutput Shape\\n\\nParam #\\n\\n(None, 4, 4, 2048)\\n\\n0\\n\\n(None, 131)\\n\\n268419\\n\\nmodel.fit(train_data, epochs=1) \\n\\n6770/6770 [= loss: 0.1312 - accuracy: 0.9728 - 388s 57ms/step -\\n\\n<tensorflow.python.keras.callbacks.History at 0x15420d63490> \\n\\n<tensorflow.python.keras.callbacks.History at 0x15420d63490>\\n\\nscore = model.evaluate(train_data) print(score) score = model.evaluate(test_data) print(score) \\n\\n6770/6770 [== loss: 0.0214 - accuracy: 0.9927 [0.021364932879805565, 0.9926578998565674 2269/2269 [== loss: 0.3093 - accuracy: 0.9347 [0.3093399107456207, 0.9346791505813599] - 387s 57ms/step - - 132s 58ms/step -\\n\\n16.7 Other Tasks That Use CNNs\\n\\nCNNs are used in many other tasks apart from Image classiﬁcation.\\n\\n273\\n\\n274\\n\\n16 Convolutional Neural Networks\\n\\n16.7.1 Object Detection\\n\\nClassiﬁcation tasks only tell us what is in the image and not where the object is. Object detection is the task of localizing objects within an image. CNNs, such as ResNets, are usually used as the feature extractor for object detection networks.\\n\\nGroundtruth: tv or monitor tv or monitor (2) tv or monitor (3) person remote control remote control (2)\\n\\n16.7.2 Semantic Segmentation\\n\\nUsing Fully Convolutional Nets, we can generate output maps which tell us which pixel belongs to which classes. This task is called Semantic Segmentation.\\n\\n16.7 Other Tasks That Use CNNs\\n\\n275\\n\\nChapter 17 Chatbot, Speech, and NLP\\n\\n®) ces\\n\\nAbstract Chatbots are programs that are capable of conversing with people trained for their speciﬁc tasks, such as providing parents with information about a school. This chapter will provide the skills required to create a basic chatbot that can converse through speech. Speech to text tools will be used to convert speech data into text data. An encoder-decoder architecture model will be trained using Long- Short Term Memory units for a question and answer task for conversation.'),\n",
              " Document(metadata={'doc_id': '34866d27-38e2-4190-8ec0-ce2b795a1b39'}, page_content='10.4 Neural Network\\n\\nNeural networks are the representation we make of the brain: neurons intercon- nected to other neurons, which forms a network. A simple information transits in a lot of them before becoming an actual thing, like “move the hand to pick up this pencil.”\\n\\n10.4 Neural Network\\n\\n173\\n\\nThe operation of a complete neural network is straightforward : one enters variables as inputs (for example, an image if the neural network is supposed to tell what is on an image), and after some calculations, an output is returned (probability of whether an image is a cat).\\n\\nx1  x2  x3  +1  Layer L4  +1  +1  Layer L3 \\n\\nLayer L,\\n\\nLayer L,\\n\\nWhen an input is given to the neural network, it returns an output. On the ﬁrst try, it cannot get the right output by its own (except with luck) and that is why, during the learning phase, every input comes with its label, explaining what output the neural network should have guessed. If the choice is the good one, actual parameters are kept and the next input is given. However, if the obtained output does not match the label, weights are changed. Those are the only variables that can be changed during the learning phase. This process may be imagined as multiple buttons that are turned into different possibilities every time an input is not guessed correctly. To determine which weight is better to modify, a particular process, called “backpropagation” is done.\\n\\nBelow is the code to create a simple neural network in python: The following code is telling python to add a layer of 64 neurons into the neural network. We can stack the models by adding more layers of neuron. Or we can simply increase the number of neurons. This can be thought of as to increase the number of “neurons” in one’s brain and thereby improving one’s learning ability.\\n\\n#Model 5: neural network print(\"Neural Network\") print(\"================================\") model = Sequential() model.add(Dense(64, input_dim=Y_POSITION, activation=\\'relu\\')) model.add(Dense(64, activation=\\'relu\\')) model.add(Dropout(0.2)) model.add(Dense(1, activation=\\'relu\\')) # Compile mode \\n\\n(continues on next page)\\n\\n174\\n\\n10 Regression\\n\\n(continued from previous page)\\n\\n# https: //www.tensorflow.org/guide/keras/train_and_evaluate model.compile(loss=\\'MSE\\', optimizer=\\'Adamax\\', metrics=[ = \\'accuracy\\']) # Fit the model model.fit(X_train, y train, epochs=300, batch_size=5,,, —-verbose=0) # evaluate the model predictions5 = model.predict (X_train) RMSE_train5 = mean_squared_error(y_train,predictions5) print (\"Neural Network TrainSet: RMSE {}\".format (RMSE_train5) ) predictions5 model .predict (X_test) RMSE_test5 = mean_squared_error(y_test,predictions5) print (\"Neural Network TestSet: RMSE {}\".format (RMSE_test5) ) =\")\\n\\nNeural Network ================================ Neural Network TrainSet: RMSE 0.02496122448979592 ================================== Neural Network TestSet: RMSE 0.032824 ================================ ')]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from typing import Literal\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "def retrival_grader_and_get_relevant_retrival(query, retriever=retriever, k=7):\n",
        "\n",
        "\n",
        "    class GradeDocuments(BaseModel):\n",
        "        \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "        binary_score: str = Field(\n",
        "            description=\"Documents are relevant to the question, 'yes' or 'no'\",\n",
        "            enum=[\"yes\", \"no\"]\n",
        "        )\n",
        "\n",
        "    # Prompt\n",
        "    system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "        It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
        "\n",
        "        yes: relevant\n",
        "        no: no relevant\"\"\"\n",
        "\n",
        "    grade_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system),\n",
        "            (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # LLM with function call\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                    temperature=0,\n",
        "                    top_p=1.0)\n",
        "\n",
        "\n",
        "    retrieval_grader = grade_prompt | llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "    retriever.search_kwargs[\"k\"]=k\n",
        "\n",
        "    matching_results=retriever.vectorstore.similarity_search(query, k=k)\n",
        "\n",
        "    doc_search=retriever.invoke(query)\n",
        "\n",
        "    new_docs=[]\n",
        "    binary_score_list=[]\n",
        "\n",
        "    for doc in matching_results:\n",
        "      doc_txt=doc.page_content\n",
        "      binary_score=retrieval_grader.invoke({\"question\": query, \"document\": doc_txt})\n",
        "\n",
        "      binary_score_list.append(binary_score.binary_score)\n",
        "      if binary_score.binary_score==\"yes\":\n",
        "        new_docs.append(doc)\n",
        "      else:\n",
        "        continue\n",
        "    return new_docs, binary_score_list, doc_search"
      ],
      "metadata": {
        "id": "SpSs0oMLRhCH"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is CNN and How to use? explain\"\n",
        "new_docs, binary_score_list, doc_search=retrival_grader_and_get_relevant_retrival(query)\n",
        "display(binary_score_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8C7c8Bfd2b1Y",
        "outputId": "dc3a030e-f8a1-44b2-9326-be666827c3f8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['yes', 'yes', 'yes', 'yes', 'no', 'yes', 'yes']"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in new_docs:\n",
        "  print(i.metadata[\"doc_id\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMMn8fSH2s0l",
        "outputId": "8392a6c8-cbd6-4e8c-cb20-d2efded6b08d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29a90db4-28ca-45e4-8fbb-d007eabd41e0\n",
            "6973319e-0bea-49f8-a859-bf939ea89375\n",
            "87821439-f0d2-450c-9fdf-f271e854e87f\n",
            "cfcaaf5f-2228-4851-b541-41b160073659\n",
            "2d149c3f-c403-45ca-8d74-ddd6c5e9b4b2\n",
            "84055c64-b57e-4797-a8de-69249860a30f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_doc_search=[]\n",
        "for i in new_docs:\n",
        "  for j in doc_search:\n",
        "    if i.metadata[\"doc_id\"]==j.metadata[\"doc_id\"]:\n",
        "      new_doc_search.append(j)"
      ],
      "metadata": {
        "id": "spxCgsXX2tUB"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[j for i in new_docs for j in doc_search if i.metadata[\"doc_id\"]==j.metadata[\"doc_id\"]]"
      ],
      "metadata": {
        "id": "ORelUoNG8BiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in new_doc_search:\n",
        "  print(i.metadata[\"doc_id\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owG1eMU53lRy",
        "outputId": "66253ca0-ec3d-47de-ddd9-6287845a4dc1"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29a90db4-28ca-45e4-8fbb-d007eabd41e0\n",
            "6973319e-0bea-49f8-a859-bf939ea89375\n",
            "87821439-f0d2-450c-9fdf-f271e854e87f\n",
            "cfcaaf5f-2228-4851-b541-41b160073659\n",
            "2d149c3f-c403-45ca-8d74-ddd6c5e9b4b2\n",
            "84055c64-b57e-4797-a8de-69249860a30f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_doc_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3Aiie-b44DY",
        "outputId": "c73ed556-60ad-4de7-ffa0-9169714232e6"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'doc_id': '29a90db4-28ca-45e4-8fbb-d007eabd41e0'}, page_content='Chapter 16 Convolutional Neural Networks\\n\\n® a\\n\\nAbstract Convolutional Neural Networks are neural networks with convolution layers which perform operations similar to image processing ﬁlters. Convolutional Neural Networks are applied in a variety of tasks related to images such as image classiﬁcation, object detection, and semantic segmentation. Popular Network architectures include ResNet, GoogleNet, and VGG. These networks are often trained on very large datasets, can be downloaded in Keras and Tensorﬂow, and can be later used for ﬁnetuning on other tasks.\\n\\nLearning outcomes:\\n\\nUnderstand how convolution, pooling, and ﬂattening operations are performed. • Perform an image classiﬁcation task using Convolutional Neural Networks. • Familiarize with notable Convolution Neural Network Architectures. • Understand Transfer Learning and Finetuning. • Perform an image classiﬁcation task through ﬁnetuning a Convolutional Neural Network\\n\\npreviously trained on a separate task.\\n\\n• Exposure to various applications of Convolutional Neural Networks.\\n\\nA fully connected neural network consists of a series of fully connected layers that connect every neuron in one layer to every neuron in the other layer. The main problem with fully connected neural networks is that the number of weights required is very large for certain types of data. For example, an image of 224x224x3 would require 150528 weights in just the ﬁrst hidden layer and will grow quickly for even bigger images. You can imagine how computationally intensive things would become once the images reach dimensions as large as 8K resolution images (7680×4320), training such a network would require a lot of time and resources.\\n\\nHowever, for image data, repeating patterns can occur in different places. Hence we can train many smaller detectors, capable of sliding across an image, to take advantage of the repeating patterns. This would reduce the number of weights required to be trained.\\n\\nA Convolutional Neural Network is a neural network with some convolutional layers (and some other layers). A convolutional layer has a number of ﬁlters that does the convolutional operation.\\n\\n© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022 T. T. Teoh, Z. Rong, Artiﬁcial Intelligence with Python, Machine Learning: Foundations, Methodologies, and Applications, https://doi.org/10.1007/978-981-16-8615-3_16\\n\\n262\\n\\n16 Convolutional Neural Networks\\n\\nThey can be compressed to the same parameters.'),\n",
              " Document(metadata={'doc_id': '6973319e-0bea-49f8-a859-bf939ea89375'}, page_content='16.2 Pooling\\n\\nNowadays, a CNN always exploits extensive weight-sharing to reduce the degrees of the freedom of models. A pooling layer helps reduce computation time and gradually build up spatial and conﬁgural invariance. For image understanding, pooling layer helps extract more semantic meaning. The max pooling layer simply returns the maximum value over the values that the kernel operation is applied on. The example below illustrates the outputs of a max pooling and average pooling operation, respectively, given a kernel of size 2 and stride 2.\\n\\nmax pooling x fs [12] 2 [0 37| 4 25 average pooling\\n\\n16.3 Flattening\\n\\nAdding a Fully Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolu- tional layer. The Fully Connected layer is learning a possibly non-linear function in that space.\\n\\n16.4 Exercise\\n\\n265\\n\\nBy ﬂattening the image into a column vector, we have converted our input image into a suitable form for our Multi-Level Perceptron. The ﬂattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them using the Softmax Classiﬁcation technique.\\n\\nReLU Activation Fn. Volume-28x28x3  Output Volume 14x14x3  Output Volume 588x1  Output Volume 20x1  Output Nodes 5x1  Class 1  Class 2  Class 3  Class 4  Convolution layer Stride 1  Class 5  Max Pool layer Stride 2  Soft-max Layer  Flatten layer  Fully connected Layer ReLU Activation Fn.  Soft-Max Activation Fn \\n\\nInuput Volume 32x32x1\\n\\n16.4 Exercise\\n\\nWe will build a small CNN using Convolution layers, Max Pooling layers, and Dropout layers in order to predict the type of fruit in a picture.\\n\\nThe dataset we will use is the fruits 360 dataset. You can obtain the dataset from this link: https://www.kaggle.com/moltean/fruits\\n\\nimport numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd. read_csv) import os from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, Flatten from tensorflow.keras.layers import Conv2D, MaxPooling2D from tensorflow.keras import optimizers import numpy as np import pandas as pd\\n\\n(continues on next page)\\n\\n266\\n\\n16 Convolutional Neural Networks\\n\\n(continued from previous page)\\n\\nfrom tensorflow.keras.preprocessing.image import,, ~ImageDataGenerator import matplotlib.pyplot as plt import matplotlib.image as mpimg import pathlib\\n\\ntrain_root -=pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Training =) test_root = pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Test\")\\n\\ntrain_root -=pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Training =) test_root = pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Test\")\\n\\nbatch_size = 10 '),\n",
              " Document(metadata={'doc_id': '87821439-f0d2-450c-9fdf-f271e854e87f'}, page_content='15 Image Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 239 Load the Dependencies . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 239 15.1 Load Image from urls . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 240 15.2 Image Analysis .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 243 15.3 Image Histogram .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 244 15.4 Contour .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 247 15.5 Grayscale Transformation . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 249 15.6 Histogram Equalization.. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 250 15.7 Fourier Transformation . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 252 15.8 High pass Filtering in FFT . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 253 15.9 15.10 Pattern Recognition .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 254 15.11 Sample Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 258\\n\\n16 Convolutional Neural Networks . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 261 The Convolution Operation.. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 262 Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 264 Flattening .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 264 Exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 265 CNN Architectures.. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.1 VGG16.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.2 Inception Net . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.3 ResNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 271 Finetuning .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 272 Other Tasks That Use CNNs . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 273 16.7.1 Object Detection . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 274 16.7.2 Semantic Segmentation . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 274'),\n",
              " Document(metadata={'doc_id': 'cfcaaf5f-2228-4851-b541-41b160073659'}, page_content='Model: \"sequential_1\"\\n\\n(continues on next page)\\n\\n16.7 Other Tasks That Use CNNs\\n\\n(continued from previous page)\\n\\nLayer (type) resnet50 (Model) (None, Param # 23587712 o global_average_pooling2d_1 (| (None, o dense 1 (Dense) (None, Total params: 23,856,131 Trainable params: 268,419 Non-trainable params: 23,587,712 268419\\n\\nLayer (type) resnet50 (Model) (None, Param # 23587712 o global_average_pooling2d_1 (| (None, o dense 1 (Dense) (None, Total params: 23,856,131 Trainable params: 268,419 Non-trainable params: 23,587,712 268419\\n\\nOutput Shape\\n\\nParam #\\n\\n(None, 4, 4, 2048)\\n\\n0\\n\\n(None, 131)\\n\\n268419\\n\\nmodel.fit(train_data, epochs=1) \\n\\n6770/6770 [= loss: 0.1312 - accuracy: 0.9728 - 388s 57ms/step -\\n\\n<tensorflow.python.keras.callbacks.History at 0x15420d63490> \\n\\n<tensorflow.python.keras.callbacks.History at 0x15420d63490>\\n\\nscore = model.evaluate(train_data) print(score) score = model.evaluate(test_data) print(score) \\n\\n6770/6770 [== loss: 0.0214 - accuracy: 0.9927 [0.021364932879805565, 0.9926578998565674 2269/2269 [== loss: 0.3093 - accuracy: 0.9347 [0.3093399107456207, 0.9346791505813599] - 387s 57ms/step - - 132s 58ms/step -\\n\\n16.7 Other Tasks That Use CNNs\\n\\nCNNs are used in many other tasks apart from Image classiﬁcation.\\n\\n273\\n\\n274\\n\\n16 Convolutional Neural Networks\\n\\n16.7.1 Object Detection\\n\\nClassiﬁcation tasks only tell us what is in the image and not where the object is. Object detection is the task of localizing objects within an image. CNNs, such as ResNets, are usually used as the feature extractor for object detection networks.\\n\\nGroundtruth: tv or monitor tv or monitor (2) tv or monitor (3) person remote control remote control (2)\\n\\n16.7.2 Semantic Segmentation\\n\\nUsing Fully Convolutional Nets, we can generate output maps which tell us which pixel belongs to which classes. This task is called Semantic Segmentation.\\n\\n16.7 Other Tasks That Use CNNs\\n\\n275\\n\\nChapter 17 Chatbot, Speech, and NLP\\n\\n®) ces\\n\\nAbstract Chatbots are programs that are capable of conversing with people trained for their speciﬁc tasks, such as providing parents with information about a school. This chapter will provide the skills required to create a basic chatbot that can converse through speech. Speech to text tools will be used to convert speech data into text data. An encoder-decoder architecture model will be trained using Long- Short Term Memory units for a question and answer task for conversation.'),\n",
              " Document(metadata={'doc_id': '2d149c3f-c403-45ca-8d74-ddd6c5e9b4b2'}, page_content='19.4.1 Intermediate Layers for Style and Content\\n\\nSo why do these intermediate outputs within our pretrained image classiﬁcation network allow us to deﬁne style and content representations?\\n\\nAt a high level, in order for a network to perform image classiﬁcation (which this network has been trained to do), it must understand the image. This requires taking the raw image as input pixels and building an internal representation that converts the raw image pixels into a complex understanding of the features present within the image.\\n\\nThis is also a reason why convolutional neural networks are able to generalize well: they are able to capture the invariances and deﬁning features within classes (e.g. cats vs. dogs) that are agnostic to background noise and other nuisances. Thus, somewhere between where the raw image is fed into the model and the output classiﬁcation label, the model serves as a complex feature extractor. By accessing intermediate layers of the model, you are able to describe the content and style of input images.\\n\\n19.5 Build the Model\\n\\n309\\n\\n19.5 Build the Model\\n\\nThe networks in tf.keras.applications are designed so you can easily extract the intermediate layer values using the Keras functional API.\\n\\nTo deﬁne a model using the functional API, specify the inputs and outputs: model = Model(inputs, outputs) This following function builds a VGG19 model that returns a list of intermediate\\n\\nlayer outputs:\\n\\ndef vgg_layers(layer_names) : \"\"\" Creates a vgg model that returns a list of intermediate,, soutput values.\"\"\" # Load our model. Load pretrained VGG, trained on imagenet,, data vgg = tf.keras.applications.VGG19(include_top=False, weights= —\\'imagenet\\') vgg.trainable = False outputs = [vgg.get_layer(name) .output for name in layer_ names] model = tf.keras.Model([vgg.input], outputs) return model\\n\\ndef vgg_layers(layer_names):\\n\\nAnd to create the model:\\n\\nstyle_extractor = vgg_layers(style_layers) style_outputs = style_extractor(style_image*255)  #Look at the statistics of each layer\\'s output for name, output in zip(style_layers, style_outputs):  print(name) print(\" print(\" print(\" print(\" print()  shape: \", output.numpy().shape) min: \", output.numpy().min()) max: \", output.numpy().max()) mean: \", output.numpy().mean()) \\n\\nblock1_conv1  shape: min: max: mean:  (1, 336, 512, 64)  0.0 835.5255  33.97525  block2_conv1  shape: min: max: mean:  (1, 168, 256, 128)  0.0 4625.8867  199.82687 \\n\\n(continues on next page)\\n\\n310\\n\\n310\\n\\n19 Neural Style Transfer\\n\\n(continued from previous page)\\n\\nblock3_conv1  shape: min: max: mean:  (1, 84, 128, 256)  0.0 8789.24  230.78099  block4_conv1  shape: min: max: mean:  (1, 42, 64, 512)  0.0 21566.133  791.24005  block5_conv1  (1, 21, 32, 512)  shape: min: max: mean:  0.0 3189.2532  59.179478 '),\n",
              " Document(metadata={'doc_id': '84055c64-b57e-4797-a8de-69249860a30f'}, page_content='batch_size = 10\\n\\nfrom skimage import io image = io.imread(\"D:/Programming Stuff/Teoh\\'s Slides//book-ai- potato (docs) /fruits-360_dataset/fruits-360/Training/Apple,, “Braeburn/101_100.jpg\") print (image. shape) print (image) io. imshow (image)\\n\\n(100, 100, 3) [[[253 255 250] [255 255 251] [255 254 255] ... [255 255 255] [255 255 255] [255 255 255]]  [[251 255 252] [253 255 252] [255 254 255] ... [255 255 255] [255 255 255] [255 255 255]]  [[249 255 253] [251 255 254] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]  ... \\n\\n(100, 100, 3) [[[253 255 250] [255 255 251] [255 254 255] ... [255 255 255] [255 255 255] [255 255 255]]\\n\\n[[251 255 252] [253 255 252] [255 254 255] ... [255 255 255] [255 255 255] [255 255 255]]\\n\\n[[249 255 253] [251 255 254] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]\\n\\n...\\n\\n(continues on next page)\\n\\n16.4 Exercise\\n\\n(continued from previous page)\\n\\n[[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]  [[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]  [[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]] \\n\\n[[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]\\n\\n[[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]\\n\\n[[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]]\\n\\n<matplotlib.image.AxesImage at 0x1543232f070> \\n\\n<matplotlib.image.AxesImage at 0x1543232f070>\\n\\n0\\n\\nGenerator = ImageDataGenerator () train_data = Generator.flow_from_directory(train_root, (100,,, 100), batch_size=batch_size) test_data = Generator.flow_from_directory(test_root, (100,,, 100), batch_size=batch_size)\\n\\n267\\n\\n268\\n\\n16 Convolutional Neural Networks\\n\\nFound 67692 images belonging to 131 classes. Found 22688 images belonging to 131 classes. \\n\\nnum_classes = len([i for i in os.listdir(train_root)]) print(num_classes) \\n\\n131 \\n\\n131\\n\\nmodel = Sequential () model.add(Conv2D(16, (5, 5), input_shape=(100, 100, 3), sactivation=\\'relu\\')) model .add(MaxPooling2D(pool_size=(2, 2), strides=2)) model .add (Dropout (0.05) ) model .add(Conv2D(32, (5, 5), activation=\\'relu\\')) model .add(MaxPooling2D(pool_size=(2, 2), strides=2)) model .add (Dropout (0.05) ) model.add(Conv2D(64, (5, 5),activation=\\'relu\\')) model .add(MaxPooling2D(pool_size=(2, 2), strides=2)) model .add (Dropout (0.05) ) model .add(Conv2D(128, (5, 5), activation=\\'relu\\')) model .add(MaxPooling2D(pool_size=(2, 2), strides=2)) model .add (Dropout (0.05) ) model .add (Flatten () ) model.add(Dense(1024, activation=\\'relu\\')) model .add (Dropout (0.05) ) model.add(Dense (256, activation=\\'relu\\')) model .add (Dropout (0.05) ) model.add(Dense(num_classes, activation=\"softmax\") )\\n\\nmodel .compile(loss=keras.losses.categorical_crossentropy,,, —optimizer=optimizers.Adam(), metrics=[\\'accuracy\\']) model.fit(train_data, batch_size = batch_size, epochs=2)\\n\\nEpoch 1/2 6770/6770 [== - 160s 24ms/step - loss: 1. - accuracy: Epoch 2/2 6770/6770 - 129s 19ms/step - loss: 0.5038 - accuracy:\\n\\n16.5 CNN Architectures\\n\\n<tensorflow.python.keras.callbacks.History at 0x154323500a0> \\n\\nscore = model.evaluate(train_data) print(score) score = model.evaluate(test_data) print(score) \\n\\n6770/6770 [== - 105s 15ms/step -,, loss: 0.2151 - accuracy: [0.21505890786647797, 0.93660 2269/2269 [ ] - 34s 15ms/step -,, loss: 0.8411 - accuracy: [0.8410834670066833, 0.8113980889320374]')]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating an Answer Based on Similar Embeddings Vectors"
      ],
      "metadata": {
        "id": "b6RYFS13wjNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Qm7eIKQM6GkK",
        "outputId": "a53272b7-0f46-4faf-b70a-38bf6510c01b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is CNN and How to use? explain'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_by_type = split_image_text_types(new_doc_search)\n",
        "docs_by_type"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9BJqOrjHdTX",
        "outputId": "e5fb151f-981f-406c-d32a-a767d5a4cca8"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'images': [],\n",
              " 'texts': ['Chapter 16 Convolutional Neural Networks\\n\\n® a\\n\\nAbstract Convolutional Neural Networks are neural networks with convolution layers which perform operations similar to image processing ﬁlters. Convolutional Neural Networks are applied in a variety of tasks related to images such as image classiﬁcation, object detection, and semantic segmentation. Popular Network architectures include ResNet, GoogleNet, and VGG. These networks are often trained on very large datasets, can be downloaded in Keras and Tensorﬂow, and can be later used for ﬁnetuning on other tasks.\\n\\nLearning outcomes:\\n\\nUnderstand how convolution, pooling, and ﬂattening operations are performed. • Perform an image classiﬁcation task using Convolutional Neural Networks. • Familiarize with notable Convolution Neural Network Architectures. • Understand Transfer Learning and Finetuning. • Perform an image classiﬁcation task through ﬁnetuning a Convolutional Neural Network\\n\\npreviously trained on a separate task.\\n\\n• Exposure to various applications of Convolutional Neural Networks.\\n\\nA fully connected neural network consists of a series of fully connected layers that connect every neuron in one layer to every neuron in the other layer. The main problem with fully connected neural networks is that the number of weights required is very large for certain types of data. For example, an image of 224x224x3 would require 150528 weights in just the ﬁrst hidden layer and will grow quickly for even bigger images. You can imagine how computationally intensive things would become once the images reach dimensions as large as 8K resolution images (7680×4320), training such a network would require a lot of time and resources.\\n\\nHowever, for image data, repeating patterns can occur in different places. Hence we can train many smaller detectors, capable of sliding across an image, to take advantage of the repeating patterns. This would reduce the number of weights required to be trained.\\n\\nA Convolutional Neural Network is a neural network with some convolutional layers (and some other layers). A convolutional layer has a number of ﬁlters that does the convolutional operation.\\n\\n© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022 T. T. Teoh, Z. Rong, Artiﬁcial Intelligence with Python, Machine Learning: Foundations, Methodologies, and Applications, https://doi.org/10.1007/978-981-16-8615-3_16\\n\\n262\\n\\n16 Convolutional Neural Networks\\n\\nThey can be compressed to the same parameters.',\n",
              "  '16.2 Pooling\\n\\nNowadays, a CNN always exploits extensive weight-sharing to reduce the degrees of the freedom of models. A pooling layer helps reduce computation time and gradually build up spatial and conﬁgural invariance. For image understanding, pooling layer helps extract more semantic meaning. The max pooling layer simply returns the maximum value over the values that the kernel operation is applied on. The example below illustrates the outputs of a max pooling and average pooling operation, respectively, given a kernel of size 2 and stride 2.\\n\\nmax pooling x fs [12] 2 [0 37| 4 25 average pooling\\n\\n16.3 Flattening\\n\\nAdding a Fully Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolu- tional layer. The Fully Connected layer is learning a possibly non-linear function in that space.\\n\\n16.4 Exercise\\n\\n265\\n\\nBy ﬂattening the image into a column vector, we have converted our input image into a suitable form for our Multi-Level Perceptron. The ﬂattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them using the Softmax Classiﬁcation technique.\\n\\nReLU Activation Fn. Volume-28x28x3  Output Volume 14x14x3  Output Volume 588x1  Output Volume 20x1  Output Nodes 5x1  Class 1  Class 2  Class 3  Class 4  Convolution layer Stride 1  Class 5  Max Pool layer Stride 2  Soft-max Layer  Flatten layer  Fully connected Layer ReLU Activation Fn.  Soft-Max Activation Fn \\n\\nInuput Volume 32x32x1\\n\\n16.4 Exercise\\n\\nWe will build a small CNN using Convolution layers, Max Pooling layers, and Dropout layers in order to predict the type of fruit in a picture.\\n\\nThe dataset we will use is the fruits 360 dataset. You can obtain the dataset from this link: https://www.kaggle.com/moltean/fruits\\n\\nimport numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd. read_csv) import os from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, Flatten from tensorflow.keras.layers import Conv2D, MaxPooling2D from tensorflow.keras import optimizers import numpy as np import pandas as pd\\n\\n(continues on next page)\\n\\n266\\n\\n16 Convolutional Neural Networks\\n\\n(continued from previous page)\\n\\nfrom tensorflow.keras.preprocessing.image import,, ~ImageDataGenerator import matplotlib.pyplot as plt import matplotlib.image as mpimg import pathlib\\n\\ntrain_root -=pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Training =) test_root = pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Test\")\\n\\ntrain_root -=pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Training =) test_root = pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Test\")\\n\\nbatch_size = 10 ',\n",
              "  '15 Image Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 239 Load the Dependencies . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 239 15.1 Load Image from urls . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 240 15.2 Image Analysis .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 243 15.3 Image Histogram .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 244 15.4 Contour .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 247 15.5 Grayscale Transformation . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 249 15.6 Histogram Equalization.. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 250 15.7 Fourier Transformation . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 252 15.8 High pass Filtering in FFT . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 253 15.9 15.10 Pattern Recognition .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 254 15.11 Sample Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 258\\n\\n16 Convolutional Neural Networks . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 261 The Convolution Operation.. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 262 Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 264 Flattening .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 264 Exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 265 CNN Architectures.. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.1 VGG16.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.2 Inception Net . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.3 ResNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 271 Finetuning .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 272 Other Tasks That Use CNNs . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 273 16.7.1 Object Detection . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 274 16.7.2 Semantic Segmentation . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 274',\n",
              "  'Model: \"sequential_1\"\\n\\n(continues on next page)\\n\\n16.7 Other Tasks That Use CNNs\\n\\n(continued from previous page)\\n\\nLayer (type) resnet50 (Model) (None, Param # 23587712 o global_average_pooling2d_1 (| (None, o dense 1 (Dense) (None, Total params: 23,856,131 Trainable params: 268,419 Non-trainable params: 23,587,712 268419\\n\\nLayer (type) resnet50 (Model) (None, Param # 23587712 o global_average_pooling2d_1 (| (None, o dense 1 (Dense) (None, Total params: 23,856,131 Trainable params: 268,419 Non-trainable params: 23,587,712 268419\\n\\nOutput Shape\\n\\nParam #\\n\\n(None, 4, 4, 2048)\\n\\n0\\n\\n(None, 131)\\n\\n268419\\n\\nmodel.fit(train_data, epochs=1) \\n\\n6770/6770 [= loss: 0.1312 - accuracy: 0.9728 - 388s 57ms/step -\\n\\n<tensorflow.python.keras.callbacks.History at 0x15420d63490> \\n\\n<tensorflow.python.keras.callbacks.History at 0x15420d63490>\\n\\nscore = model.evaluate(train_data) print(score) score = model.evaluate(test_data) print(score) \\n\\n6770/6770 [== loss: 0.0214 - accuracy: 0.9927 [0.021364932879805565, 0.9926578998565674 2269/2269 [== loss: 0.3093 - accuracy: 0.9347 [0.3093399107456207, 0.9346791505813599] - 387s 57ms/step - - 132s 58ms/step -\\n\\n16.7 Other Tasks That Use CNNs\\n\\nCNNs are used in many other tasks apart from Image classiﬁcation.\\n\\n273\\n\\n274\\n\\n16 Convolutional Neural Networks\\n\\n16.7.1 Object Detection\\n\\nClassiﬁcation tasks only tell us what is in the image and not where the object is. Object detection is the task of localizing objects within an image. CNNs, such as ResNets, are usually used as the feature extractor for object detection networks.\\n\\nGroundtruth: tv or monitor tv or monitor (2) tv or monitor (3) person remote control remote control (2)\\n\\n16.7.2 Semantic Segmentation\\n\\nUsing Fully Convolutional Nets, we can generate output maps which tell us which pixel belongs to which classes. This task is called Semantic Segmentation.\\n\\n16.7 Other Tasks That Use CNNs\\n\\n275\\n\\nChapter 17 Chatbot, Speech, and NLP\\n\\n®) ces\\n\\nAbstract Chatbots are programs that are capable of conversing with people trained for their speciﬁc tasks, such as providing parents with information about a school. This chapter will provide the skills required to create a basic chatbot that can converse through speech. Speech to text tools will be used to convert speech data into text data. An encoder-decoder architecture model will be trained using Long- Short Term Memory units for a question and answer task for conversation.',\n",
              "  '19.4.1 Intermediate Layers for Style and Content\\n\\nSo why do these intermediate outputs within our pretrained image classiﬁcation network allow us to deﬁne style and content representations?\\n\\nAt a high level, in order for a network to perform image classiﬁcation (which this network has been trained to do), it must understand the image. This requires taking the raw image as input pixels and building an internal representation that converts the raw image pixels into a complex understanding of the features present within the image.\\n\\nThis is also a reason why convolutional neural networks are able to generalize well: they are able to capture the invariances and deﬁning features within classes (e.g. cats vs. dogs) that are agnostic to background noise and other nuisances. Thus, somewhere between where the raw image is fed into the model and the output classiﬁcation label, the model serves as a complex feature extractor. By accessing intermediate layers of the model, you are able to describe the content and style of input images.\\n\\n19.5 Build the Model\\n\\n309\\n\\n19.5 Build the Model\\n\\nThe networks in tf.keras.applications are designed so you can easily extract the intermediate layer values using the Keras functional API.\\n\\nTo deﬁne a model using the functional API, specify the inputs and outputs: model = Model(inputs, outputs) This following function builds a VGG19 model that returns a list of intermediate\\n\\nlayer outputs:\\n\\ndef vgg_layers(layer_names) : \"\"\" Creates a vgg model that returns a list of intermediate,, soutput values.\"\"\" # Load our model. Load pretrained VGG, trained on imagenet,, data vgg = tf.keras.applications.VGG19(include_top=False, weights= —\\'imagenet\\') vgg.trainable = False outputs = [vgg.get_layer(name) .output for name in layer_ names] model = tf.keras.Model([vgg.input], outputs) return model\\n\\ndef vgg_layers(layer_names):\\n\\nAnd to create the model:\\n\\nstyle_extractor = vgg_layers(style_layers) style_outputs = style_extractor(style_image*255)  #Look at the statistics of each layer\\'s output for name, output in zip(style_layers, style_outputs):  print(name) print(\" print(\" print(\" print(\" print()  shape: \", output.numpy().shape) min: \", output.numpy().min()) max: \", output.numpy().max()) mean: \", output.numpy().mean()) \\n\\nblock1_conv1  shape: min: max: mean:  (1, 336, 512, 64)  0.0 835.5255  33.97525  block2_conv1  shape: min: max: mean:  (1, 168, 256, 128)  0.0 4625.8867  199.82687 \\n\\n(continues on next page)\\n\\n310\\n\\n310\\n\\n19 Neural Style Transfer\\n\\n(continued from previous page)\\n\\nblock3_conv1  shape: min: max: mean:  (1, 84, 128, 256)  0.0 8789.24  230.78099  block4_conv1  shape: min: max: mean:  (1, 42, 64, 512)  0.0 21566.133  791.24005  block5_conv1  (1, 21, 32, 512)  shape: min: max: mean:  0.0 3189.2532  59.179478 ',\n",
              "  'batch_size = 10\\n\\nfrom skimage import io image = io.imread(\"D:/Programming Stuff/Teoh\\'s Slides//book-ai- potato (docs) /fruits-360_dataset/fruits-360/Training/Apple,, “Braeburn/101_100.jpg\") print (image. shape) print (image) io. imshow (image)\\n\\n(100, 100, 3) [[[253 255 250] [255 255 251] [255 254 255] ... [255 255 255] [255 255 255] [255 255 255]]  [[251 255 252] [253 255 252] [255 254 255] ... [255 255 255] [255 255 255] [255 255 255]]  [[249 255 253] [251 255 254] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]  ... \\n\\n(100, 100, 3) [[[253 255 250] [255 255 251] [255 254 255] ... [255 255 255] [255 255 255] [255 255 255]]\\n\\n[[251 255 252] [253 255 252] [255 254 255] ... [255 255 255] [255 255 255] [255 255 255]]\\n\\n[[249 255 253] [251 255 254] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]\\n\\n...\\n\\n(continues on next page)\\n\\n16.4 Exercise\\n\\n(continued from previous page)\\n\\n[[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]  [[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]  [[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]] \\n\\n[[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]\\n\\n[[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]\\n\\n[[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]]\\n\\n<matplotlib.image.AxesImage at 0x1543232f070> \\n\\n<matplotlib.image.AxesImage at 0x1543232f070>\\n\\n0\\n\\nGenerator = ImageDataGenerator () train_data = Generator.flow_from_directory(train_root, (100,,, 100), batch_size=batch_size) test_data = Generator.flow_from_directory(test_root, (100,,, 100), batch_size=batch_size)\\n\\n267\\n\\n268\\n\\n16 Convolutional Neural Networks\\n\\nFound 67692 images belonging to 131 classes. Found 22688 images belonging to 131 classes. \\n\\nnum_classes = len([i for i in os.listdir(train_root)]) print(num_classes) \\n\\n131 \\n\\n131\\n\\nmodel = Sequential () model.add(Conv2D(16, (5, 5), input_shape=(100, 100, 3), sactivation=\\'relu\\')) model .add(MaxPooling2D(pool_size=(2, 2), strides=2)) model .add (Dropout (0.05) ) model .add(Conv2D(32, (5, 5), activation=\\'relu\\')) model .add(MaxPooling2D(pool_size=(2, 2), strides=2)) model .add (Dropout (0.05) ) model.add(Conv2D(64, (5, 5),activation=\\'relu\\')) model .add(MaxPooling2D(pool_size=(2, 2), strides=2)) model .add (Dropout (0.05) ) model .add(Conv2D(128, (5, 5), activation=\\'relu\\')) model .add(MaxPooling2D(pool_size=(2, 2), strides=2)) model .add (Dropout (0.05) ) model .add (Flatten () ) model.add(Dense(1024, activation=\\'relu\\')) model .add (Dropout (0.05) ) model.add(Dense (256, activation=\\'relu\\')) model .add (Dropout (0.05) ) model.add(Dense(num_classes, activation=\"softmax\") )\\n\\nmodel .compile(loss=keras.losses.categorical_crossentropy,,, —optimizer=optimizers.Adam(), metrics=[\\'accuracy\\']) model.fit(train_data, batch_size = batch_size, epochs=2)\\n\\nEpoch 1/2 6770/6770 [== - 160s 24ms/step - loss: 1. - accuracy: Epoch 2/2 6770/6770 - 129s 19ms/step - loss: 0.5038 - accuracy:\\n\\n16.5 CNN Architectures\\n\\n<tensorflow.python.keras.callbacks.History at 0x154323500a0> \\n\\nscore = model.evaluate(train_data) print(score) score = model.evaluate(test_data) print(score) \\n\\n6770/6770 [== - 105s 15ms/step -,, loss: 0.2151 - accuracy: [0.21505890786647797, 0.93660 2269/2269 [ ] - 34s 15ms/step -,, loss: 0.8411 - accuracy: [0.8410834670066833, 0.8113980889320374]']}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input= prompt_func(docs_by_type, query)\n",
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To3f9R25HL9n",
        "outputId": "0d23b65b-a5cf-4ea3-8028-0b8f0befa77d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HumanMessage(content=[{'type': 'text', 'text': 'Answer the question based only on the following context, which can include text, tables and the below image(s) if available:\\n    Question: What is CNN and How to use? explain\\n\\n    Text and tables:\\n    Chapter 16 Convolutional Neural Networks\\n\\n® a\\n\\nAbstract Convolutional Neural Networks are neural networks with convolution layers which perform operations similar to image processing ﬁlters. Convolutional Neural Networks are applied in a variety of tasks related to images such as image classiﬁcation, object detection, and semantic segmentation. Popular Network architectures include ResNet, GoogleNet, and VGG. These networks are often trained on very large datasets, can be downloaded in Keras and Tensorﬂow, and can be later used for ﬁnetuning on other tasks.\\n\\nLearning outcomes:\\n\\nUnderstand how convolution, pooling, and ﬂattening operations are performed. • Perform an image classiﬁcation task using Convolutional Neural Networks. • Familiarize with notable Convolution Neural Network Architectures. • Understand Transfer Learning and Finetuning. • Perform an image classiﬁcation task through ﬁnetuning a Convolutional Neural Network\\n\\npreviously trained on a separate task.\\n\\n• Exposure to various applications of Convolutional Neural Networks.\\n\\nA fully connected neural network consists of a series of fully connected layers that connect every neuron in one layer to every neuron in the other layer. The main problem with fully connected neural networks is that the number of weights required is very large for certain types of data. For example, an image of 224x224x3 would require 150528 weights in just the ﬁrst hidden layer and will grow quickly for even bigger images. You can imagine how computationally intensive things would become once the images reach dimensions as large as 8K resolution images (7680×4320), training such a network would require a lot of time and resources.\\n\\nHowever, for image data, repeating patterns can occur in different places. Hence we can train many smaller detectors, capable of sliding across an image, to take advantage of the repeating patterns. This would reduce the number of weights required to be trained.\\n\\nA Convolutional Neural Network is a neural network with some convolutional layers (and some other layers). A convolutional layer has a number of ﬁlters that does the convolutional operation.\\n\\n© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022 T. T. Teoh, Z. Rong, Artiﬁcial Intelligence with Python, Machine Learning: Foundations, Methodologies, and Applications, https://doi.org/10.1007/978-981-16-8615-3_16\\n\\n262\\n\\n16 Convolutional Neural Networks\\n\\nThey can be compressed to the same parameters.\\n16.2 Pooling\\n\\nNowadays, a CNN always exploits extensive weight-sharing to reduce the degrees of the freedom of models. A pooling layer helps reduce computation time and gradually build up spatial and conﬁgural invariance. For image understanding, pooling layer helps extract more semantic meaning. The max pooling layer simply returns the maximum value over the values that the kernel operation is applied on. The example below illustrates the outputs of a max pooling and average pooling operation, respectively, given a kernel of size 2 and stride 2.\\n\\nmax pooling x fs [12] 2 [0 37| 4 25 average pooling\\n\\n16.3 Flattening\\n\\nAdding a Fully Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolu- tional layer. The Fully Connected layer is learning a possibly non-linear function in that space.\\n\\n16.4 Exercise\\n\\n265\\n\\nBy ﬂattening the image into a column vector, we have converted our input image into a suitable form for our Multi-Level Perceptron. The ﬂattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them using the Softmax Classiﬁcation technique.\\n\\nReLU Activation Fn. Volume-28x28x3  Output Volume 14x14x3  Output Volume 588x1  Output Volume 20x1  Output Nodes 5x1  Class 1  Class 2  Class 3  Class 4  Convolution layer Stride 1  Class 5  Max Pool layer Stride 2  Soft-max Layer  Flatten layer  Fully connected Layer ReLU Activation Fn.  Soft-Max Activation Fn \\n\\nInuput Volume 32x32x1\\n\\n16.4 Exercise\\n\\nWe will build a small CNN using Convolution layers, Max Pooling layers, and Dropout layers in order to predict the type of fruit in a picture.\\n\\nThe dataset we will use is the fruits 360 dataset. You can obtain the dataset from this link: https://www.kaggle.com/moltean/fruits\\n\\nimport numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd. read_csv) import os from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, Flatten from tensorflow.keras.layers import Conv2D, MaxPooling2D from tensorflow.keras import optimizers import numpy as np import pandas as pd\\n\\n(continues on next page)\\n\\n266\\n\\n16 Convolutional Neural Networks\\n\\n(continued from previous page)\\n\\nfrom tensorflow.keras.preprocessing.image import,, ~ImageDataGenerator import matplotlib.pyplot as plt import matplotlib.image as mpimg import pathlib\\n\\ntrain_root -=pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Training =) test_root = pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Test\")\\n\\ntrain_root -=pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Training =) test_root = pathlib.Path(\"D:/Programming Stuff/Teoh\\'s Slides/ <book-ai-potato (docs) /fruits-360_dataset/fruits-360/Test\")\\n\\nbatch_size = 10 \\n15 Image Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 239 Load the Dependencies . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 239 15.1 Load Image from urls . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 240 15.2 Image Analysis .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 243 15.3 Image Histogram .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 244 15.4 Contour .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 247 15.5 Grayscale Transformation . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 249 15.6 Histogram Equalization.. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 250 15.7 Fourier Transformation . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 252 15.8 High pass Filtering in FFT . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 253 15.9 15.10 Pattern Recognition .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 254 15.11 Sample Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 258\\n\\n16 Convolutional Neural Networks . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 261 The Convolution Operation.. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 262 Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 264 Flattening .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 264 Exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 265 CNN Architectures.. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.1 VGG16.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.2 Inception Net . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 269 16.5.3 ResNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 271 Finetuning .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 272 Other Tasks That Use CNNs . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 273 16.7.1 Object Detection . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 274 16.7.2 Semantic Segmentation . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 274\\nModel: \"sequential_1\"\\n\\n(continues on next page)\\n\\n16.7 Other Tasks That Use CNNs\\n\\n(continued from previous page)\\n\\nLayer (type) resnet50 (Model) (None, Param # 23587712 o global_average_pooling2d_1 (| (None, o dense 1 (Dense) (None, Total params: 23,856,131 Trainable params: 268,419 Non-trainable params: 23,587,712 268419\\n\\nLayer (type) resnet50 (Model) (None, Param # 23587712 o global_average_pooling2d_1 (| (None, o dense 1 (Dense) (None, Total params: 23,856,131 Trainable params: 268,419 Non-trainable params: 23,587,712 268419\\n\\nOutput Shape\\n\\nParam #\\n\\n(None, 4, 4, 2048)\\n\\n0\\n\\n(None, 131)\\n\\n268419\\n\\nmodel.fit(train_data, epochs=1) \\n\\n6770/6770 [= loss: 0.1312 - accuracy: 0.9728 - 388s 57ms/step -\\n\\n<tensorflow.python.keras.callbacks.History at 0x15420d63490> \\n\\n<tensorflow.python.keras.callbacks.History at 0x15420d63490>\\n\\nscore = model.evaluate(train_data) print(score) score = model.evaluate(test_data) print(score) \\n\\n6770/6770 [== loss: 0.0214 - accuracy: 0.9927 [0.021364932879805565, 0.9926578998565674 2269/2269 [== loss: 0.3093 - accuracy: 0.9347 [0.3093399107456207, 0.9346791505813599] - 387s 57ms/step - - 132s 58ms/step -\\n\\n16.7 Other Tasks That Use CNNs\\n\\nCNNs are used in many other tasks apart from Image classiﬁcation.\\n\\n273\\n\\n274\\n\\n16 Convolutional Neural Networks\\n\\n16.7.1 Object Detection\\n\\nClassiﬁcation tasks only tell us what is in the image and not where the object is. Object detection is the task of localizing objects within an image. CNNs, such as ResNets, are usually used as the feature extractor for object detection networks.\\n\\nGroundtruth: tv or monitor tv or monitor (2) tv or monitor (3) person remote control remote control (2)\\n\\n16.7.2 Semantic Segmentation\\n\\nUsing Fully Convolutional Nets, we can generate output maps which tell us which pixel belongs to which classes. This task is called Semantic Segmentation.\\n\\n16.7 Other Tasks That Use CNNs\\n\\n275\\n\\nChapter 17 Chatbot, Speech, and NLP\\n\\n®) ces\\n\\nAbstract Chatbots are programs that are capable of conversing with people trained for their speciﬁc tasks, such as providing parents with information about a school. This chapter will provide the skills required to create a basic chatbot that can converse through speech. Speech to text tools will be used to convert speech data into text data. An encoder-decoder architecture model will be trained using Long- Short Term Memory units for a question and answer task for conversation.\\n19.4.1 Intermediate Layers for Style and Content\\n\\nSo why do these intermediate outputs within our pretrained image classiﬁcation network allow us to deﬁne style and content representations?\\n\\nAt a high level, in order for a network to perform image classiﬁcation (which this network has been trained to do), it must understand the image. This requires taking the raw image as input pixels and building an internal representation that converts the raw image pixels into a complex understanding of the features present within the image.\\n\\nThis is also a reason why convolutional neural networks are able to generalize well: they are able to capture the invariances and deﬁning features within classes (e.g. cats vs. dogs) that are agnostic to background noise and other nuisances. Thus, somewhere between where the raw image is fed into the model and the output classiﬁcation label, the model serves as a complex feature extractor. By accessing intermediate layers of the model, you are able to describe the content and style of input images.\\n\\n19.5 Build the Model\\n\\n309\\n\\n19.5 Build the Model\\n\\nThe networks in tf.keras.applications are designed so you can easily extract the intermediate layer values using the Keras functional API.\\n\\nTo deﬁne a model using the functional API, specify the inputs and outputs: model = Model(inputs, outputs) This following function builds a VGG19 model that returns a list of intermediate\\n\\nlayer outputs:\\n\\ndef vgg_layers(layer_names) : \"\"\" Creates a vgg model that returns a list of intermediate,, soutput values.\"\"\" # Load our model. Load pretrained VGG, trained on imagenet,, data vgg = tf.keras.applications.VGG19(include_top=False, weights= —\\'imagenet\\') vgg.trainable = False outputs = [vgg.get_layer(name) .output for name in layer_ names] model = tf.keras.Model([vgg.input], outputs) return model\\n\\ndef vgg_layers(layer_names):\\n\\nAnd to create the model:\\n\\nstyle_extractor = vgg_layers(style_layers) style_outputs = style_extractor(style_image*255)  #Look at the statistics of each layer\\'s output for name, output in zip(style_layers, style_outputs):  print(name) print(\" print(\" print(\" print(\" print()  shape: \", output.numpy().shape) min: \", output.numpy().min()) max: \", output.numpy().max()) mean: \", output.numpy().mean()) \\n\\nblock1_conv1  shape: min: max: mean:  (1, 336, 512, 64)  0.0 835.5255  33.97525  block2_conv1  shape: min: max: mean:  (1, 168, 256, 128)  0.0 4625.8867  199.82687 \\n\\n(continues on next page)\\n\\n310\\n\\n310\\n\\n19 Neural Style Transfer\\n\\n(continued from previous page)\\n\\nblock3_conv1  shape: min: max: mean:  (1, 84, 128, 256)  0.0 8789.24  230.78099  block4_conv1  shape: min: max: mean:  (1, 42, 64, 512)  0.0 21566.133  791.24005  block5_conv1  (1, 21, 32, 512)  shape: min: max: mean:  0.0 3189.2532  59.179478 \\nbatch_size = 10\\n\\nfrom skimage import io image = io.imread(\"D:/Programming Stuff/Teoh\\'s Slides//book-ai- potato (docs) /fruits-360_dataset/fruits-360/Training/Apple,, “Braeburn/101_100.jpg\") print (image. shape) print (image) io. imshow (image)\\n\\n(100, 100, 3) [[[253 255 250] [255 255 251] [255 254 255] ... [255 255 255] [255 255 255] [255 255 255]]  [[251 255 252] [253 255 252] [255 254 255] ... [255 255 255] [255 255 255] [255 255 255]]  [[249 255 253] [251 255 254] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]  ... \\n\\n(100, 100, 3) [[[253 255 250] [255 255 251] [255 254 255] ... [255 255 255] [255 255 255] [255 255 255]]\\n\\n[[251 255 252] [253 255 252] [255 254 255] ... [255 255 255] [255 255 255] [255 255 255]]\\n\\n[[249 255 253] [251 255 254] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]\\n\\n...\\n\\n(continues on next page)\\n\\n16.4 Exercise\\n\\n(continued from previous page)\\n\\n[[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]  [[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]  [[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]] \\n\\n[[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]\\n\\n[[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]\\n\\n[[255 255 255] [255 255 255] [255 255 255] ... [255 255 255] [255 255 255] [255 255 255]]]\\n\\n<matplotlib.image.AxesImage at 0x1543232f070> \\n\\n<matplotlib.image.AxesImage at 0x1543232f070>\\n\\n0\\n\\nGenerator = ImageDataGenerator () train_data = Generator.flow_from_directory(train_root, (100,,, 100), batch_size=batch_size) test_data = Generator.flow_from_directory(test_root, (100,,, 100), batch_size=batch_size)\\n\\n267\\n\\n268\\n\\n16 Convolutional Neural Networks\\n\\nFound 67692 images belonging to 131 classes. Found 22688 images belonging to 131 classes. \\n\\nnum_classes = len([i for i in os.listdir(train_root)]) print(num_classes) \\n\\n131 \\n\\n131\\n\\nmodel = Sequential () model.add(Conv2D(16, (5, 5), input_shape=(100, 100, 3), sactivation=\\'relu\\')) model .add(MaxPooling2D(pool_size=(2, 2), strides=2)) model .add (Dropout (0.05) ) model .add(Conv2D(32, (5, 5), activation=\\'relu\\')) model .add(MaxPooling2D(pool_size=(2, 2), strides=2)) model .add (Dropout (0.05) ) model.add(Conv2D(64, (5, 5),activation=\\'relu\\')) model .add(MaxPooling2D(pool_size=(2, 2), strides=2)) model .add (Dropout (0.05) ) model .add(Conv2D(128, (5, 5), activation=\\'relu\\')) model .add(MaxPooling2D(pool_size=(2, 2), strides=2)) model .add (Dropout (0.05) ) model .add (Flatten () ) model.add(Dense(1024, activation=\\'relu\\')) model .add (Dropout (0.05) ) model.add(Dense (256, activation=\\'relu\\')) model .add (Dropout (0.05) ) model.add(Dense(num_classes, activation=\"softmax\") )\\n\\nmodel .compile(loss=keras.losses.categorical_crossentropy,,, —optimizer=optimizers.Adam(), metrics=[\\'accuracy\\']) model.fit(train_data, batch_size = batch_size, epochs=2)\\n\\nEpoch 1/2 6770/6770 [== - 160s 24ms/step - loss: 1. - accuracy: Epoch 2/2 6770/6770 - 129s 19ms/step - loss: 0.5038 - accuracy:\\n\\n16.5 CNN Architectures\\n\\n<tensorflow.python.keras.callbacks.History at 0x154323500a0> \\n\\nscore = model.evaluate(train_data) print(score) score = model.evaluate(test_data) print(score) \\n\\n6770/6770 [== - 105s 15ms/step -,, loss: 0.2151 - accuracy: [0.21505890786647797, 0.93660 2269/2269 [ ] - 34s 15ms/step -,, loss: 0.8411 - accuracy: [0.8410834670066833, 0.8113980889320374]\\n\\n    '}])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(temperature=0.0,\n",
        "                   model=\"gpt-4o-mini\",\n",
        "                   max_tokens=1024,\n",
        "                   top_p=1.0) # vision models can work on both text and images\n",
        "\n",
        "output=model.invoke([input])\n",
        "output.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "cEzNAkIlIXso",
        "outputId": "68c3350e-cdb6-45a0-ec3b-33eed1e22c01"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"**What is CNN and How to use it?**\\n\\n**Convolutional Neural Networks (CNNs)** are a type of neural network specifically designed for processing structured grid data, such as images. They consist of multiple layers, including convolutional layers, pooling layers, and fully connected layers, which work together to extract features from input images and perform tasks like image classification, object detection, and semantic segmentation.\\n\\n### Key Components of CNNs:\\n1. **Convolutional Layers**: These layers apply convolution operations using filters (kernels) to the input image, allowing the network to learn spatial hierarchies of features. Each filter detects specific patterns, such as edges or textures.\\n\\n2. **Pooling Layers**: Pooling layers reduce the spatial dimensions of the feature maps, which helps decrease the computational load and control overfitting. Max pooling is a common technique that selects the maximum value from a set of values in the feature map.\\n\\n3. **Flattening**: After the convolutional and pooling layers, the multi-dimensional output is flattened into a one-dimensional vector to be fed into fully connected layers.\\n\\n4. **Fully Connected Layers**: These layers connect every neuron from the previous layer to every neuron in the current layer, allowing the network to learn complex combinations of features.\\n\\n### How to Use CNNs:\\n1. **Data Preparation**: Gather and preprocess your dataset. For example, you can use the Fruits 360 dataset for fruit classification tasks. Load images and split them into training and testing sets.\\n\\n2. **Model Building**: Use a deep learning framework like TensorFlow or Keras to build your CNN model. Here’s a simple example of how to define a CNN model in Keras:\\n\\n   ```python\\n   from tensorflow.keras.models import Sequential\\n   from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\\n\\n   model = Sequential()\\n   model.add(Conv2D(16, (5, 5), activation='relu', input_shape=(100, 100, 3)))\\n   model.add(MaxPooling2D(pool_size=(2, 2)))\\n   model.add(Dropout(0.05))\\n   model.add(Conv2D(32, (5, 5), activation='relu'))\\n   model.add(MaxPooling2D(pool_size=(2, 2)))\\n   model.add(Dropout(0.05))\\n   model.add(Flatten())\\n   model.add(Dense(256, activation='relu'))\\n   model.add(Dense(num_classes, activation='softmax'))\\n   ```\\n\\n3. **Compile the Model**: Choose a loss function and optimizer, and compile the model.\\n\\n   ```python\\n   model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\n   ```\\n\\n4. **Train the Model**: Fit the model to your training data.\\n\\n   ```python\\n   model.fit(train_data, epochs=2, batch_size=10)\\n   ```\\n\\n5. **Evaluate the Model**: After training, evaluate the model's performance on the test dataset.\\n\\n   ```python\\n   score = model.evaluate(test_data)\\n   print(score)\\n   ```\\n\\n6. **Fine-tuning and Transfer Learning**: You can also use pre-trained models (like VGG, ResNet) and fine-tune them for your specific tasks, which can save time and improve performance.\\n\\nBy following these steps, you can effectively use CNNs for various image-related tasks.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(output.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        },
        "id": "gIQwvMo4jOgi",
        "outputId": "b4e1d984-9fb2-4291-a61d-299d56823626"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **What is CNN and How to use it?**\n> \n> **Convolutional Neural Networks (CNNs)** are a type of neural network specifically designed for processing structured grid data, such as images. They consist of multiple layers, including convolutional layers, pooling layers, and fully connected layers, which work together to extract features from input images and perform tasks like image classification, object detection, and semantic segmentation.\n> \n> ### Key Components of CNNs:\n> 1. **Convolutional Layers**: These layers apply convolution operations using filters (kernels) to the input image, allowing the network to learn spatial hierarchies of features. Each filter detects specific patterns, such as edges or textures.\n> \n> 2. **Pooling Layers**: Pooling layers reduce the spatial dimensions of the feature maps, which helps decrease the computational load and control overfitting. Max pooling is a common technique that selects the maximum value from a set of values in the feature map.\n> \n> 3. **Flattening**: After the convolutional and pooling layers, the multi-dimensional output is flattened into a one-dimensional vector to be fed into fully connected layers.\n> \n> 4. **Fully Connected Layers**: These layers connect every neuron from the previous layer to every neuron in the current layer, allowing the network to learn complex combinations of features.\n> \n> ### How to Use CNNs:\n> 1. **Data Preparation**: Gather and preprocess your dataset. For example, you can use the Fruits 360 dataset for fruit classification tasks. Load images and split them into training and testing sets.\n> \n> 2. **Model Building**: Use a deep learning framework like TensorFlow or Keras to build your CNN model. Here’s a simple example of how to define a CNN model in Keras:\n> \n>    ```python\n>    from tensorflow.keras.models import Sequential\n>    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n> \n>    model = Sequential()\n>    model.add(Conv2D(16, (5, 5), activation='relu', input_shape=(100, 100, 3)))\n>    model.add(MaxPooling2D(pool_size=(2, 2)))\n>    model.add(Dropout(0.05))\n>    model.add(Conv2D(32, (5, 5), activation='relu'))\n>    model.add(MaxPooling2D(pool_size=(2, 2)))\n>    model.add(Dropout(0.05))\n>    model.add(Flatten())\n>    model.add(Dense(256, activation='relu'))\n>    model.add(Dense(num_classes, activation='softmax'))\n>    ```\n> \n> 3. **Compile the Model**: Choose a loss function and optimizer, and compile the model.\n> \n>    ```python\n>    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n>    ```\n> \n> 4. **Train the Model**: Fit the model to your training data.\n> \n>    ```python\n>    model.fit(train_data, epochs=2, batch_size=10)\n>    ```\n> \n> 5. **Evaluate the Model**: After training, evaluate the model's performance on the test dataset.\n> \n>    ```python\n>    score = model.evaluate(test_data)\n>    print(score)\n>    ```\n> \n> 6. **Fine-tuning and Transfer Learning**: You can also use pre-trained models (like VGG, ResNet) and fine-tune them for your specific tasks, which can save time and improve performance.\n> \n> By following these steps, you can effectively use CNNs for various image-related tasks."
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1k-hX3z76gyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking whether answer of model is related to the new_doc_search (retrival documents)"
      ],
      "metadata": {
        "id": "M5hNRK-yZWX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hallucination_grader(documents, generation):\n",
        "    ### Hallucination Grader\n",
        "\n",
        "    # Data model\n",
        "    class GradeHallucinations(BaseModel):\n",
        "        \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
        "\n",
        "        binary_score: str = Field(\n",
        "            description=\"Answer is grounded in the facts, 'yes' or 'no'\",\n",
        "            enum=[\"yes\", \"no\"]\n",
        "        )\n",
        "\n",
        "    # Prompt\n",
        "    system = \"\"\"You are a grader assessing whether an LLM generation is grounded in a set of retrieved facts. \\n\n",
        "        Give a binary score 'yes' or 'no'.\n",
        "\n",
        "        'no' means that the answer is grounded in the set of facts.\n",
        "\n",
        "        'yes' means that the answer is not grounded in the set of facts.\"\"\"\n",
        "\n",
        "    hallucination_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system),\n",
        "            (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # LLM with function call\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                    temperature=0,\n",
        "                    top_p=1.0)\n",
        "\n",
        "    combined_docs=\"\\n\".join([d.page_content for d in documents])\n",
        "\n",
        "\n",
        "    hallucination_grader = hallucination_prompt | llm.with_structured_output(GradeHallucinations)\n",
        "    hal_gr=hallucination_grader.invoke({\"documents\": combined_docs, \"generation\": generation})\n",
        "    return hal_gr\n",
        "\n"
      ],
      "metadata": {
        "id": "AA30TeyKaRo3"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hallucination_grader(new_docs, output.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIBPn2weS24U",
        "outputId": "459ca09c-810b-4bc4-bc29-3f507ed02ca2"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradeHallucinations(binary_score='no')"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking whether answer of model is related to the question"
      ],
      "metadata": {
        "id": "jtRo4EdZcXXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_grader(query, generation):\n",
        "\n",
        "    ### Answer Grader\n",
        "\n",
        "    # Data model\n",
        "    class GradeAnswer(BaseModel):\n",
        "        \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
        "\n",
        "        binary_score: str = Field(\n",
        "            description=\"Answer addresses the question, 'yes' or 'no'\",\n",
        "            enum=[\"yes\", \"no\"]\n",
        "        )\n",
        "\n",
        "\n",
        "    # Prompt\n",
        "    system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
        "        Give a binary score 'yes' or 'no'.\n",
        "        yes means that the answer resolves the question.\n",
        "        no means that the answer does not resolves the question. \"\"\"\n",
        "\n",
        "    answer_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system),\n",
        "            (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # LLM with function call\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                    temperature=0,\n",
        "                    top_p=1.0)\n",
        "\n",
        "    answer_grader = answer_prompt | llm.with_structured_output(GradeAnswer)\n",
        "    answer_gr=answer_grader.invoke({\"question\": query , \"generation\": generation})\n",
        "    return answer_gr"
      ],
      "metadata": {
        "id": "v02ivZWJaRce"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "X8Gyfw4_c0ZX",
        "outputId": "5bec49d0-a8ea-4810-9a09-0701c6e4e092"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is CNN and How to use? explain'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_grader(query, output.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaUk48f9cUZ2",
        "outputId": "e5e4dedc-d672-4eb3-ff56-5c478c25f147"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradeAnswer(binary_score='yes')"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rewrite Query (if the answer of model isn't related to the question)"
      ],
      "metadata": {
        "id": "3conKPrMEa1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rewrite_query(query):\n",
        "\n",
        "    ### Question Re-writer\n",
        "    from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "    # LLM\n",
        "    llm = ChatOpenAI(model=\"gpt-4o\",\n",
        "                    temperature=0,\n",
        "                    top_p=1.0)\n",
        "\n",
        "    # Prompt\n",
        "    system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\\n",
        "        for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "    re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "    return question_rewriter.invoke({\"question\": query})"
      ],
      "metadata": {
        "id": "YoXtDm3bi3KI"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"What is CNN and How to use? explain\"\n",
        "rewrite_query(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uVqZWqcMdUGn",
        "outputId": "4db59237-4df8-4f3b-c8d8-c7d61e2a3024"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is a Convolutional Neural Network (CNN) and how can it be utilized effectively?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_chatbot(query, model_name=\"gpt-4o-mini\", k=7):\n",
        "  \"\"\"Search for information about transformers and attention. For any questions about transformers and attention.\n",
        "    For any question about article of 'Attention Is All You Need', you must use this tool!\"\"\"\n",
        "  from langchain_openai import ChatOpenAI\n",
        "\n",
        "  print(\"RAG_CHATBOT\")\n",
        "  new_docs, _, doc_search =retrival_grader_and_get_relevant_retrival(query, k=k)\n",
        "  new_doc_search= [j for i in new_docs for j in doc_search if i.metadata[\"doc_id\"]==j.metadata[\"doc_id\"]]\n",
        "\n",
        "  docs_by_type = split_image_text_types(new_doc_search) # text ve image olarak ayrıştırılıyor\n",
        "  input= prompt_func(docs_by_type, query)\n",
        "\n",
        "  llm=ChatOpenAI(model_name=\"gpt-4o-mini\",\n",
        "                temperature=0,\n",
        "                top_p=1)\n",
        "\n",
        "  output=llm.invoke([input])\n",
        "\n",
        "  hallucination_gr=hallucination_grader(new_docs, output.content) #özetler üzerinden halisinasyon kontrolü yapılıyor.\n",
        "  a=0\n",
        "  while hallucination_gr.binary_score == \"yes\":\n",
        "    print(\"hallucination detected\")\n",
        "    output=llm.invoke([input])\n",
        "    hallucination_gr=hallucination_grader(new_docs, output.content)\n",
        "    if a==3:\n",
        "      print(\"hallucination problem could not be resolved\")\n",
        "      break\n",
        "    a+=1\n",
        "\n",
        "  answer_gr=answer_grader(query, output.content)\n",
        "  b=0\n",
        "  while answer_gr.binary_score == \"no\":\n",
        "    print(\"query is being rebuilt\")\n",
        "    rewritten_query = rewrite_query(query)\n",
        "    new_docs, _, doc_search= retrival_grader_and_get_relevant_retrival(rewritten_query, k=k)\n",
        "    new_doc_search= [j for i in new_docs for j in doc_search if i.metadata[\"doc_id\"]==j.metadata[\"doc_id\"]]\n",
        "\n",
        "    docs_by_type = split_image_text_types(new_doc_search) # text ve image olarak ayrıştırılıyor\n",
        "    input= prompt_func(docs_by_type, rewritten_query)\n",
        "    output=llm.invoke([input])\n",
        "\n",
        "    if b==3:\n",
        "      print(\"answer problem could not be resolved\")\n",
        "      break\n",
        "    b+=1\n",
        "\n",
        "    hallucination_gr=hallucination_grader(new_docs, output.content)\n",
        "    c=0\n",
        "    while hallucination_gr.binary_score == \"yes\":\n",
        "      print(\"hallucination detected\")\n",
        "      output=llm.invoke([input])\n",
        "      hallucination_gr = hallucination_grader(new_docs, output.content)\n",
        "      if c==3:\n",
        "        print(\"hallucination problem could not be resolved\")\n",
        "        break\n",
        "      c+=1\n",
        "\n",
        "    answer_gr=answer_grader(rewritten_query, output.content)\n",
        "    query = rewritten_query\n",
        "\n",
        "  return output.content"
      ],
      "metadata": {
        "id": "AsHqzLLhG_cn"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "our_query = \"What is CNN and How to use? explain\"\n",
        "rag_chatbot(our_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "ETKvf3IO7ZEA",
        "outputId": "246ee4df-5182-48c5-e22a-e3ef5c0194e4"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG_CHATBOT\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"**What is CNN and How to use it?**\\n\\n**Convolutional Neural Networks (CNNs)** are a class of deep learning models specifically designed for processing structured grid data, such as images. They consist of multiple layers, including convolutional layers, pooling layers, and fully connected layers, which work together to extract features from input images and perform tasks like image classification, object detection, and semantic segmentation.\\n\\n### Key Components of CNNs:\\n\\n1. **Convolutional Layers**: These layers apply convolution operations using filters (kernels) to the input image, allowing the network to learn spatial hierarchies of features. Each filter detects specific patterns, such as edges or textures.\\n\\n2. **Pooling Layers**: Pooling layers reduce the spatial dimensions of the feature maps, which helps decrease the computational load and control overfitting. Max pooling is a common technique that selects the maximum value from a set of values in the feature map.\\n\\n3. **Flattening**: After the convolutional and pooling layers, the multi-dimensional output is flattened into a one-dimensional vector to be fed into fully connected layers.\\n\\n4. **Fully Connected Layers**: These layers connect every neuron from the previous layer to every neuron in the current layer, allowing the model to learn complex combinations of features.\\n\\n5. **Activation Functions**: Functions like ReLU (Rectified Linear Unit) and Softmax are used to introduce non-linearity and to output probabilities for classification tasks.\\n\\n### How to Use CNNs:\\n\\n1. **Data Preparation**: Gather and preprocess your dataset. For image classification, you can use datasets like the Fruits 360 dataset. Images should be resized to a consistent shape (e.g., 100x100 pixels).\\n\\n2. **Model Building**: Use libraries like TensorFlow and Keras to build your CNN model. Here’s a simple example of how to create a CNN:\\n\\n   ```python\\n   from tensorflow.keras.models import Sequential\\n   from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\\n\\n   model = Sequential()\\n   model.add(Conv2D(16, (5, 5), activation='relu', input_shape=(100, 100, 3)))\\n   model.add(MaxPooling2D(pool_size=(2, 2)))\\n   model.add(Dropout(0.05))\\n   model.add(Conv2D(32, (5, 5), activation='relu'))\\n   model.add(MaxPooling2D(pool_size=(2, 2)))\\n   model.add(Dropout(0.05))\\n   model.add(Flatten())\\n   model.add(Dense(256, activation='relu'))\\n   model.add(Dense(num_classes, activation='softmax'))\\n   ```\\n\\n3. **Compile the Model**: Specify the loss function, optimizer, and metrics for evaluation.\\n\\n   ```python\\n   model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\n   ```\\n\\n4. **Train the Model**: Fit the model to your training data.\\n\\n   ```python\\n   model.fit(train_data, epochs=10, batch_size=32)\\n   ```\\n\\n5. **Evaluate the Model**: After training, evaluate the model's performance on test data.\\n\\n   ```python\\n   score = model.evaluate(test_data)\\n   print(score)\\n   ```\\n\\n6. **Fine-tuning and Transfer Learning**: You can also use pre-trained models (like VGG, ResNet) and fine-tune them for your specific tasks, which can save time and improve performance.\\n\\nBy following these steps, you can effectively utilize CNNs for various image-related tasks.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wkipedia"
      ],
      "metadata": {
        "id": "Ee6fsL2yOr2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbs5ZOMiPBme",
        "outputId": "5806375b-1dfe-413c-ba14-c62da460188d"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "wiki = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=2, doc_content_chars_max=1000))"
      ],
      "metadata": {
        "id": "IhsgN6muPCtR"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki.run(\"What is RNN models?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "XO7yUHloPFrA",
        "outputId": "cb76f5a1-f23c-442e-e9bf-b676d2363879"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Page: Recurrent neural network\\nSummary: Recurrent neural networks (RNNs) are a class of artificial neural networks for sequential data processing. Unlike feedforward neural networks, which process data in a single pass, RNNs process data across multiple time steps, making them well-adapted for modelling and processing text, speech, and time series.\\nThe fundamental building block of an RNN is the recurrent unit. This unit maintains a hidden state, essentially a form of memory, which is updated at each time step based on the current input and the previous hidden state. This feedback loop allows the network to learn from past inputs and incorporate that knowledge into its current processing.\\nEarly RNNs suffered from the vanishing gradient problem, limiting their ability to learn long-range dependencies. This was solved by the invention of Long Short-Term Memory (LSTM) networks in 1997, which became the standard architecture for RNN.\\nThey have been applied to tasks such as unsegmented, con'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def wiki_search(query):\n",
        "    \"\"\"\n",
        "    Web search based on the re-phrased question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with appended web results\n",
        "    \"\"\"\n",
        "    from langchain.schema import Document\n",
        "    from langchain_core.output_parsers import StrOutputParser\n",
        "    from langchain_community.tools import WikipediaQueryRun\n",
        "    from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "    print(\"WIKIPEDIA SEARCH\")\n",
        "\n",
        "    wiki = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=3, doc_content_chars_max=1000))\n",
        "\n",
        "\n",
        "    docs = wiki.invoke(query)\n",
        "    #wiki_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "    wiki_results = Document(page_content=docs)\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\n",
        "    If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
        "    Question: {question}\n",
        "    Context: {context}\n",
        "    Answer:\"\"\"\n",
        "    )\n",
        "\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                     temperature=0.0,\n",
        "                     top_p=1.0)\n",
        "\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    answer = chain.invoke({\"context\": wiki_results, \"question\": query})\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "87jZaY8PPTzx"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_search(\"what is RNN models\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "z1AGKkabS-4K",
        "outputId": "1a34a459-9ef4-49fd-a107-a7bdebaa6fd0"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WIKIPEDIA SEARCH\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Recurrent neural networks (RNNs) are a type of artificial neural network designed for processing sequential data. They maintain a hidden state that acts as memory, allowing them to learn from past inputs and incorporate that knowledge into current processing. RNNs are particularly useful for tasks involving text, speech, and time series data.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NKeIbxZQTPdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Router"
      ],
      "metadata": {
        "id": "vTagKdgPUYVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def router(query):\n",
        "\n",
        "    from typing import Literal\n",
        "\n",
        "    from langchain_core.prompts import ChatPromptTemplate\n",
        "    from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "    from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "    # Data model\n",
        "    class RouteQuery(BaseModel):\n",
        "        \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "        datasource: Literal[\"vectorstore\", \"wiki_search\", \"answer_yourself\"] = Field(\n",
        "            description=\"Given a user question choose to route it to wiki_search or  vectorstore or  answer_yourself.\"\n",
        "        )\n",
        "\n",
        "\n",
        "    # Prompt\n",
        "    system = \"\"\"You are an expert at routing a user question to a answer_yourself or wiki_search or vectorstore.\n",
        "\n",
        "      - answer_yourself: Use this for questions that a typical large language model (LLM) would be confident in answering. \\\n",
        "      These are generally factual questions about common knowledge topics.\n",
        "      - vectorstore: Use this for any questions specifically related to transformers and attention mechanisms as described \\\n",
        "      in the \"Attention is All You Need\" paper. This includes questions about the architecture, components, or functioning of transformers.\n",
        "      - wiki_search: Use this for all other questions that don't fit into the above categories, especially if they require \\\n",
        "      specialized knowledge or if an LLM might be uncertain about the answer.\n",
        "\n",
        "\n",
        "    After analyzing the question, make your decision about which of them to use.\"\"\"\n",
        "\n",
        "    route_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system),\n",
        "            (\"human\", \"{question}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # LLM with function call\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    question_router = route_prompt | llm.with_structured_output(RouteQuery)\n",
        "    return question_router.invoke({\"question\": query})"
      ],
      "metadata": {
        "id": "vOtsZ3g0_z0-"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(router(\"What is Llama 3.1 Models?\"))\n",
        "print(router(\"How to work transformers?\"))\n",
        "print(router(\"Who is the 5th president of USA?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWig3Wdl_z9F",
        "outputId": "423695ea-712b-4dbb-9563-b2b2577ed753"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasource='wiki_search'\n",
            "datasource='vectorstore'\n",
            "datasource='answer_yourself'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def final_chat_bot(query):\n",
        "  from langchain_core.output_parsers import StrOutputParser\n",
        "  from langchain_openai import ChatOpenAI\n",
        "  from langchain.prompts import PromptTemplate\n",
        "\n",
        "  if router(query).datasource==\"vectorstore\":\n",
        "    print(\"vectorstore\")\n",
        "    return rag_chatbot(query)\n",
        "  elif router(query).datasource==\"wiki_search\":\n",
        "    print(\"wiki_search\")\n",
        "    return wiki_search(query)\n",
        "  else:\n",
        "    print(\"answer_yourself\")\n",
        "    prompt_template_name = PromptTemplate.from_template(\n",
        "    \"Answer the user's {question} in the most accurate way possible.\"\n",
        "    )\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                     temperature=0.0,\n",
        "                     top_p=1.0)\n",
        "    chain = prompt_template_name | llm | StrOutputParser()\n",
        "\n",
        "    return chain.invoke({\"question\": query})"
      ],
      "metadata": {
        "id": "4ig6e93L_0El"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"What is self-attention?\"\n",
        "final_chat_bot(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "1_2Oe1k-_0Ll",
        "outputId": "0618910f-636a-4fb6-8838-840fce9e805e"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vectorstore\n",
            "RAG_CHATBOT\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Self-attention is a mechanism used in neural networks, particularly in natural language processing, that allows the model to weigh the importance of different words in a sequence when encoding a particular word. It enables the model to consider the relationships and dependencies between all words in a sentence, regardless of their position. This is achieved by computing attention scores that determine how much focus to place on other words when processing a specific word, allowing for a more nuanced understanding of context and meaning. Self-attention is a key component of architectures like the Transformer, which has become foundational in many state-of-the-art models.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"Who is the 5th president of America?\"\n",
        "final_chat_bot(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "l6z2OvlV_0S1",
        "outputId": "33319c25-d866-4171-8809-1db369ea58b7"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "answer_yourself\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The 5th president of the United States is James Monroe. He served from 1817 to 1825.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"What is Llama 3.1 Models?\"\n",
        "final_chat_bot(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "FgU9z9pcMk0D",
        "outputId": "39eb0046-c704-4c1b-99cf-52e69cfc7703"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wiki_search\n",
            "WIKIPEDIA SEARCH\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Llama 3.1 is the latest version of the Llama family of autoregressive large language models released by Meta AI in July 2024. These models are trained at various parameter sizes, ranging from 7 billion to 405 billion parameters, and are available under licenses that allow some commercial use. Llama models have evolved from being solely foundation models to also include instruction fine-tuned versions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"What is the difference between CNN, deep learning and RNN?\"\n",
        "final_chat_bot(query)"
      ],
      "metadata": {
        "id": "jG0W5WAoUwsT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "39b0628c-3b11-4f33-c575-de9cbac0a0ec"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wiki_search\n",
            "WIKIPEDIA SEARCH\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'CNN (Convolutional Neural Network) and RNN (Recurrent Neural Network) are both types of deep learning architectures, which is a subset of machine learning that uses neural networks with multiple layers. CNNs are primarily used for processing grid-like data such as images, while RNNs are designed for sequential data like time series or natural language. In summary, deep learning encompasses both CNNs and RNNs, which serve different purposes in data processing.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  CONCLUSION"
      ],
      "metadata": {
        "id": "2AuM656MOl2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project sets out to create a sophisticated Multimodal Agentic RAG (Retrieval-Augmented Generation) chatbot designed to leverage content from the book \"Artificial Intelligence with Python\" for accurate and contextually relevant question-and-answer interactions. By integrating a robust retriever and model alignment process, the chatbot ensures that responses are grounded in the book's content, maintaining high fidelity to the source material. The layered approach to answer generation—comprising retriever and model alignment, answer and content alignment, and question and answer alignment—enhances the reliability and relevance of the chatbot's responses.\n",
        "\n",
        "Additionally, the integration of a Wikipedia tool allows the chatbot to extend its capability beyond the book's scope, providing comprehensive answers to a broader range of queries. This dual-source approach ensures that the chatbot is not only a specialized tool for the book's content but also a versatile resource for general knowledge, making it a powerful and well-rounded solution for users seeking information.\n",
        "\n",
        "Overall, this project demonstrates a meticulous approach to building a responsive, accurate, and contextually aware chatbot, capable of navigating and utilizing both specialized and general knowledge sources to meet diverse user needs."
      ],
      "metadata": {
        "id": "FE_pxiMMPA5E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7V59IUH7FDSh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}